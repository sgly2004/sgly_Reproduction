"""
CLoRAè®­ç»ƒä¸»è„šæœ¬ - é›†æˆæ­£äº¤æ­£åˆ™åŒ–çš„LoRAå¾®è°ƒ
"""

import torch
import torch.nn.functional as F
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os
import argparse
import json
import sys
from datetime import datetime

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from logger_utils import setup_logger, redirect_print_to_logger, restore_print

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_processor import get_train_data, get_eval_datasets, get_in_domain_eval_dataset
from model_setup import setup_lora_model
from clora_loss import compute_orthogonal_loss, generate_regularization_matrices
from utils.gradient_processing import get_projected_gradients


class CLoRATrainer(Trainer):
    """
    è‡ªå®šä¹‰CLoRAè®­ç»ƒå™¨ï¼Œé›†æˆæ­£äº¤æ­£åˆ™åŒ–æŸå¤±
    
    ä¿®å¤äº†æŸå¤±è®¡ç®—é€»è¾‘ï¼Œæ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•å’ŒéªŒè¯åŠŸèƒ½
    """
    
    def __init__(self, regularization_matrices=None, orthogonal_weight=0.1, use_clora=True, 
                 debug_mode=False, logger=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.regularization_matrices = regularization_matrices
        self.orthogonal_weight = orthogonal_weight
        self.use_clora = use_clora
        self.debug_mode = debug_mode
        self.logger = logger
        
        # éªŒè¯æ­£åˆ™åŒ–çŸ©é˜µ
        if self.use_clora and self.regularization_matrices is not None:
            self._validate_regularization_matrices()
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.loss_stats = {
            "total_steps": 0,
            "classification_losses": [],
            "orthogonal_losses": [],
            "total_losses": [],
            "layer_contributions": {}
        }
        
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        é‡å†™æŸå¤±è®¡ç®—ï¼Œæ·»åŠ æ­£äº¤æ­£åˆ™åŒ–æŸå¤±
        
        ä¿®å¤äº†è®¾å¤‡å…¼å®¹æ€§ã€æ•°å€¼ç¨³å®šæ€§å’ŒæŸå¤±ç´¯åŠ é€»è¾‘
        
        Args:
            model: æ¨¡å‹
            inputs: è¾“å…¥æ•°æ®
            return_outputs: æ˜¯å¦è¿”å›è¾“å‡º
        
        Returns:
            æŸå¤±å€¼ï¼ˆå¦‚æœreturn_outputs=Trueï¼Œåˆ™è¿”å›(loss, outputs)ï¼‰
        """
        
        # è·å–è®¾å¤‡ä¿¡æ¯
        device = next(model.parameters()).device
        
        # è·å–æ ‡ç­¾
        labels = inputs.get("labels")
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        
        # è®¡ç®—æ ‡å‡†åˆ†ç±»æŸå¤±
        if labels is not None:
            logits = outputs.get("logits")
            if logits is None:
                raise ValueError("Model outputs do not contain 'logits'")
            classification_loss = F.cross_entropy(logits, labels)
        else:
            classification_loss = outputs.loss
            if classification_loss is None:
                raise ValueError("Model outputs do not contain loss and no labels provided")
        
        # ç¡®ä¿åˆ†ç±»æŸå¤±åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        classification_loss = classification_loss.to(device)
        
        # è®¡ç®—æ€»æŸå¤±
        if self.use_clora and self.regularization_matrices is not None:
            # CLoRAæ¨¡å¼ï¼šæ·»åŠ æ­£äº¤æ­£åˆ™åŒ–æŸå¤±
            orthogonal_loss, layer_details = self._compute_orthogonal_regularization(model)
            
            # ç¡®ä¿æ­£äº¤æŸå¤±åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šå¹¶ä¸”éœ€è¦æ¢¯åº¦
            if not isinstance(orthogonal_loss, torch.Tensor):
                orthogonal_loss = torch.tensor(orthogonal_loss, device=device, requires_grad=True)
            else:
                orthogonal_loss = orthogonal_loss.to(device)
                if not orthogonal_loss.requires_grad:
                    orthogonal_loss.requires_grad_(True)
            
            # è®¡ç®—æ€»æŸå¤±
            total_loss = classification_loss + self.orthogonal_weight * orthogonal_loss
            
            # è°ƒè¯•ä¿¡æ¯
            if self.debug_mode:
                self._log_debug_info(classification_loss, orthogonal_loss, total_loss, layer_details)
            
            # è®°å½•å„éƒ¨åˆ†æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
            self._log_training_metrics({
                "classification_loss": classification_loss.item(),
                "orthogonal_loss": orthogonal_loss.item(),
                "orthogonal_weight": self.orthogonal_weight,
                "weighted_orthogonal_loss": (self.orthogonal_weight * orthogonal_loss).item(),
                "total_loss": total_loss.item(),
                "mode": "CLoRA",
                "num_lora_layers": len(layer_details) if layer_details else 0
            })
            
        else:
            # æ ‡å‡†LoRAæ¨¡å¼ï¼šåªä½¿ç”¨åˆ†ç±»æŸå¤±
            total_loss = classification_loss
            orthogonal_loss = torch.tensor(0.0, device=device)
            
            # è®°å½•æŸå¤±ï¼ˆç”¨äºç›‘æ§ï¼‰
            self._log_training_metrics({
                "classification_loss": classification_loss.item(),
                "orthogonal_loss": 0.0,
                "total_loss": total_loss.item(),
                "mode": "LoRA"
            })
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self._update_loss_stats(classification_loss, orthogonal_loss, total_loss)
        
        # éªŒè¯æŸå¤±çš„åˆç†æ€§
        if self.debug_mode:
            self._validate_loss_values(classification_loss, orthogonal_loss, total_loss)
        
        return (total_loss, outputs) if return_outputs else total_loss
    
    def training_step(self, model, inputs, num_items_in_batch=None):
        """
        è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤ï¼Œé›†æˆPCAæ¢¯åº¦æ›´æ–°æœºåˆ¶
        
        å¦‚æœå¯ç”¨äº†PCAæ¢¯åº¦æ›´æ–°ï¼Œå°†åœ¨åå‘ä¼ æ’­åå¯¹LoRAå‚æ•°çš„æ¢¯åº¦è¿›è¡ŒPCAæŠ•å½±å¤„ç†
        
        Args:
            model: è®­ç»ƒä¸­çš„æ¨¡å‹
            inputs: è¾“å…¥æ•°æ®
            num_items_in_batch: æ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°é‡
        """
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        
        # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
        inputs = self._prepare_inputs(inputs)
        
        with self.compute_loss_context_manager():
            loss = self.compute_loss(model, inputs)
        
        # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
        if self.args.gradient_accumulation_steps > 1:
            loss = loss / self.args.gradient_accumulation_steps
        
        self.accelerator.backward(loss)
        
        # PCAæ¢¯åº¦æ›´æ–°æœºåˆ¶
        if hasattr(self.args, 'use_pca_grad') and self.args.use_pca_grad:
            self._apply_pca_gradient_update(model)
        
        return loss.detach()
    
    def _apply_pca_gradient_update(self, model):
        """
        åº”ç”¨PCAæ¢¯åº¦æ›´æ–°æœºåˆ¶
        
        Args:
            model: è®­ç»ƒä¸­çš„æ¨¡å‹
        """
        try:
            # è·å–å½“å‰è®¾å¤‡
            device = next(model.parameters()).device
            
            # è·å–PCAæŠ•å½±åçš„æ¢¯åº¦
            n_components = getattr(self.args, 'pca_components', 1)
            projected_gradients = get_projected_gradients(
                model=model,
                n_components=n_components,
                device=device
            )
            
            if self.debug_mode and projected_gradients:
                print(f"PCAæ¢¯åº¦æ›´æ–°: å¤„ç†äº† {len(projected_gradients)} ä¸ªLoRAå‚æ•°")
            
            # å°†æŠ•å½±åçš„æ¢¯åº¦åº”ç”¨å›æ¨¡å‹å‚æ•°
            self._apply_projected_gradients_to_model(model, projected_gradients)
            
        except Exception as e:
            if self.debug_mode:
                print(f"PCAæ¢¯åº¦æ›´æ–°å¤±è´¥: {e}")
                import traceback
                print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # å¦‚æœPCAå¤„ç†å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹æ¢¯åº¦
            pass
    
    def _apply_projected_gradients_to_model(self, model, projected_gradients):
        """
        å°†æŠ•å½±åçš„æ¢¯åº¦åº”ç”¨å›æ¨¡å‹çš„LoRAå‚æ•°
        
        Args:
            model: è®­ç»ƒä¸­çš„æ¨¡å‹
            projected_gradients: get_projected_gradientsçš„è¾“å‡ºç»“æœ
        """
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                # å¤„ç†lora_Aå‚æ•°
                if 'default' in module.lora_A:
                    lora_a_key = f"{name}.lora_A"
                    if lora_a_key in projected_gradients:
                        projected_grad = projected_gradients[lora_a_key]['projected']
                        if projected_grad is not None:
                            # å°†æŠ•å½±åçš„æ¢¯åº¦èµ‹å€¼ç»™å‚æ•°çš„.gradå±æ€§
                            module.lora_A['default'].weight.grad = projected_grad.clone()
                
                # å¤„ç†lora_Bå‚æ•°
                if 'default' in module.lora_B:
                    lora_b_key = f"{name}.lora_B"
                    if lora_b_key in projected_gradients:
                        projected_grad = projected_gradients[lora_b_key]['projected']
                        if projected_grad is not None:
                            # å°†æŠ•å½±åçš„æ¢¯åº¦èµ‹å€¼ç»™å‚æ•°çš„.gradå±æ€§
                            module.lora_B['default'].weight.grad = projected_grad.clone()
    
    def _compute_orthogonal_regularization(self, model):
        """
        è®¡ç®—æ‰€æœ‰LoRAå±‚çš„æ­£äº¤æ­£åˆ™åŒ–æŸå¤±
        
        ä¿®å¤äº†è®¾å¤‡å…¼å®¹æ€§ã€æ•°å€¼ç¨³å®šæ€§å’Œç´¯åŠ é€»è¾‘
        
        Args:
            model: PEFTæ¨¡å‹
        
        Returns:
            tuple: (æ€»çš„æ­£äº¤æ­£åˆ™åŒ–æŸå¤±, å±‚çº§è¯¦ç»†ä¿¡æ¯)
        """
        device = next(model.parameters()).device
        total_orthogonal_loss = torch.tensor(0.0, device=device, requires_grad=True)
        layer_details = []
        num_lora_layers = 0
        
        # éå†æ‰€æœ‰LoRAå±‚
        for name, module in model.named_modules():
            # æŸ¥æ‰¾LoRAçš„Aå’ŒBçŸ©é˜µ
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                # è·å–Aå’ŒBçŸ©é˜µçš„æƒé‡
                for adapter_name in module.lora_A.keys():
                    try:
                        lora_a = module.lora_A[adapter_name].weight  # (r, input_dim)
                        lora_b = module.lora_B[adapter_name].weight  # (output_dim, r)
                        
                        # ç¡®ä¿çŸ©é˜µåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                        lora_a = lora_a.to(device)
                        lora_b = lora_b.to(device)
                        
                        # è½¬ç½®ä»¥åŒ¹é…æˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ ¼å¼
                        lora_a_transposed = lora_a.T  # (input_dim, r)
                        lora_b_transposed = lora_b.T  # (r, output_dim)
                        
                        # è®¡ç®—å½“å‰å±‚çš„æ­£äº¤æŸå¤±
                        if self.regularization_matrices is not None:
                            layer_orthogonal_loss = compute_orthogonal_loss(
                                lora_a_transposed, 
                                lora_b_transposed, 
                                self.regularization_matrices
                            )
                            
                            # ç¡®ä¿æŸå¤±å€¼æ˜¯å¼ é‡å¹¶åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                            if not isinstance(layer_orthogonal_loss, torch.Tensor):
                                layer_orthogonal_loss = torch.tensor(
                                    layer_orthogonal_loss, device=device, requires_grad=True
                                )
                            else:
                                layer_orthogonal_loss = layer_orthogonal_loss.to(device)
                                if not layer_orthogonal_loss.requires_grad:
                                    layer_orthogonal_loss.requires_grad_(True)
                            
                            # ç´¯åŠ æŸå¤±
                            total_orthogonal_loss = total_orthogonal_loss + layer_orthogonal_loss
                            num_lora_layers += 1
                            
                            # è®°å½•å±‚çº§è¯¦ç»†ä¿¡æ¯
                            layer_info = {
                                "layer_name": f"{name}.{adapter_name}",
                                "lora_a_shape": list(lora_a.shape),
                                "lora_b_shape": list(lora_b.shape),
                                "loss_value": layer_orthogonal_loss.item(),
                                "lora_a_norm": torch.norm(lora_a).item(),
                                "lora_b_norm": torch.norm(lora_b).item()
                            }
                            layer_details.append(layer_info)
                            
                            if self.debug_mode:
                                print(f"Layer {layer_info['layer_name']}: "
                                      f"Loss={layer_orthogonal_loss.item():.6f}, "
                                      f"A_norm={layer_info['lora_a_norm']:.6f}, "
                                      f"B_norm={layer_info['lora_b_norm']:.6f}")
                        
                    except Exception as e:
                        print(f"Error processing LoRA layer {name}.{adapter_name}: {e}")
                        continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°LoRAå±‚ï¼Œè¿”å›é›¶æŸå¤±
        if num_lora_layers == 0:
            if self.debug_mode:
                print("Warning: No LoRA layers found for orthogonal regularization")
            return torch.tensor(0.0, device=device, requires_grad=True), []
        
        # å¹³å‡åŒ–æŸå¤±
        averaged_loss = total_orthogonal_loss / num_lora_layers
        
        if self.debug_mode:
            print(f"Orthogonal loss summary: {num_lora_layers} layers, "
                  f"total={total_orthogonal_loss.item():.6f}, "
                  f"averaged={averaged_loss.item():.6f}")
        
        return averaged_loss, layer_details
    
    def _validate_regularization_matrices(self):
        """éªŒè¯æ­£åˆ™åŒ–çŸ©é˜µçš„æœ‰æ•ˆæ€§"""
        if self.regularization_matrices is None:
            raise ValueError("Regularization matrices cannot be None when use_clora=True")
        
        required_keys = ['P_A', 'P_B']
        for key in required_keys:
            if key not in self.regularization_matrices:
                raise ValueError(f"Missing regularization matrix: {key}")
            
            matrix = self.regularization_matrices[key]
            if not isinstance(matrix, torch.Tensor):
                raise ValueError(f"Regularization matrix {key} must be a torch.Tensor")
            
            if matrix.dim() != 2:
                raise ValueError(f"Regularization matrix {key} must be 2D")
            
            if matrix.shape[0] != matrix.shape[1]:
                raise ValueError(f"Regularization matrix {key} must be square")
        
        if self.debug_mode:
            print("Regularization matrices validation passed")
            for key, matrix in self.regularization_matrices.items():
                print(f"  {key}: shape={matrix.shape}, dtype={matrix.dtype}")
    
    def _log_training_metrics(self, metrics):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        if hasattr(self, 'log'):
            self.log(metrics)
    
    def _log_debug_info(self, classification_loss, orthogonal_loss, total_loss, layer_details):
        """è®°å½•è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯"""
        debug_info = f"\n Debug Info [Step {self.loss_stats['total_steps']}]:"
        debug_info += f"\n  Classification Loss: {classification_loss.item():.6f}"
        debug_info += f"\n  Orthogonal Loss: {orthogonal_loss.item():.6f}"
        debug_info += f"\n  Orthogonal Weight: {self.orthogonal_weight}"
        debug_info += f"\n  Weighted Orthogonal: {(self.orthogonal_weight * orthogonal_loss).item():.6f}"
        debug_info += f"\n  Total Loss: {total_loss.item():.6f}"
        debug_info += f"\n  Number of LoRA layers: {len(layer_details)}"
        
        if layer_details and len(layer_details) <= 5:  # åªæ˜¾ç¤ºå‰5å±‚çš„è¯¦ç»†ä¿¡æ¯
            debug_info += f"\n  Layer contributions:"
            for detail in layer_details:
                debug_info += f"\n    {detail['layer_name']}: {detail['loss_value']:.6f}"
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ—¥å¿—
        print(debug_info)
        if self.logger:
            self.logger.debug(debug_info)
    
    def _update_loss_stats(self, classification_loss, orthogonal_loss, total_loss):
        """æ›´æ–°æŸå¤±ç»Ÿè®¡ä¿¡æ¯"""
        self.loss_stats["total_steps"] += 1
        self.loss_stats["classification_losses"].append(classification_loss.item())
        self.loss_stats["orthogonal_losses"].append(orthogonal_loss.item())
        self.loss_stats["total_losses"].append(total_loss.item())
        
        # ä¿æŒåˆ—è¡¨é•¿åº¦åœ¨åˆç†èŒƒå›´å†…
        max_history = 1000
        for key in ["classification_losses", "orthogonal_losses", "total_losses"]:
            if len(self.loss_stats[key]) > max_history:
                self.loss_stats[key] = self.loss_stats[key][-max_history:]
    
    def _validate_loss_values(self, classification_loss, orthogonal_loss, total_loss):
        """éªŒè¯æŸå¤±å€¼çš„åˆç†æ€§"""
        def is_valid_loss(loss_tensor, name):
            if torch.isnan(loss_tensor):
                raise ValueError(f"{name} contains NaN values")
            if torch.isinf(loss_tensor):
                raise ValueError(f"{name} contains infinite values")
            if loss_tensor.item() < 0:
                print(f"  Warning: {name} is negative: {loss_tensor.item()}")
            if loss_tensor.item() > 1000:
                print(f" Warning: {name} is very large: {loss_tensor.item()}")
        
        is_valid_loss(classification_loss, "Classification loss")
        is_valid_loss(orthogonal_loss, "Orthogonal loss")
        is_valid_loss(total_loss, "Total loss")
        
        # éªŒè¯æŸå¤±å…³ç³»
        expected_total = classification_loss + self.orthogonal_weight * orthogonal_loss
        if abs(total_loss.item() - expected_total.item()) > 1e-6:
            print(f" Warning: Loss calculation mismatch. "
                  f"Expected: {expected_total.item():.6f}, Got: {total_loss.item():.6f}")
    
    def get_loss_statistics(self):
        """è·å–æŸå¤±ç»Ÿè®¡ä¿¡æ¯"""
        if not self.loss_stats["total_losses"]:
            return {}
        
        import numpy as np
        return {
            "total_steps": self.loss_stats["total_steps"],
            "avg_classification_loss": np.mean(self.loss_stats["classification_losses"]),
            "avg_orthogonal_loss": np.mean(self.loss_stats["orthogonal_losses"]),
            "avg_total_loss": np.mean(self.loss_stats["total_losses"]),
            "recent_classification_loss": self.loss_stats["classification_losses"][-1] if self.loss_stats["classification_losses"] else 0,
            "recent_orthogonal_loss": self.loss_stats["orthogonal_losses"][-1] if self.loss_stats["orthogonal_losses"] else 0,
            "recent_total_loss": self.loss_stats["total_losses"][-1] if self.loss_stats["total_losses"] else 0
        }


def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    Args:
        eval_pred: é¢„æµ‹ç»“æœ
    
    Returns:
        dict: è¯„ä¼°æŒ‡æ ‡
    """
    predictions, labels = eval_pred
    predictions = predictions.argmax(axis=-1)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(labels, predictions)
    
    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='binary'
    )
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }


def setup_training_args(output_dir="./clora_results", **kwargs):
    """
    è®¾ç½®è®­ç»ƒå‚æ•°
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        **kwargs: å…¶ä»–è®­ç»ƒå‚æ•°
    
    Returns:
        TrainingArguments: è®­ç»ƒå‚æ•°å¯¹è±¡
    """
    
    default_args = {
        "output_dir": output_dir,
        "num_train_epochs": 3,
        "per_device_train_batch_size": 8,
        "per_device_eval_batch_size": 16,
        "learning_rate": 2e-4,
        "warmup_steps": 100,
        "logging_steps": 100,
        "eval_strategy": "epoch",
        "save_strategy": "epoch",
        "load_best_model_at_end": True,
        "metric_for_best_model": "f1",
        "greater_is_better": True,
        "save_total_limit": 3,
        "report_to": None,  # ç¦ç”¨wandbç­‰ç¬¬ä¸‰æ–¹æ—¥å¿—
        "dataloader_pin_memory": False,
    }
    
    # æ›´æ–°é»˜è®¤å‚æ•°
    default_args.update(kwargs)
    
    return TrainingArguments(**default_args)


def parse_arguments():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        argparse.Namespace: è§£æåçš„å‚æ•°
    """
    parser = argparse.ArgumentParser(description="CLoRA vs LoRA å¯¹æ¯”è®­ç»ƒè„šæœ¬")
    
    # è®­ç»ƒæ¨¡å¼å‚æ•°
    parser.add_argument(
        "--use_clora", 
        action="store_true",
        default=True,
        help="æ˜¯å¦ä½¿ç”¨CLoRAè®­ç»ƒï¼ˆé»˜è®¤: Trueï¼‰"
    )
    
    parser.add_argument(
        "--no_clora",
        action="store_true", 
        help="ä½¿ç”¨çº¯LoRAè®­ç»ƒï¼ˆç­‰ä»·äº --use_clora=Falseï¼‰"
    )
    
    # æ­£äº¤æŸå¤±æƒé‡å‚æ•°
    parser.add_argument(
        "--lambda_param",
        type=float,
        default=0.1,
        help="æ­£äº¤æ­£åˆ™åŒ–æŸå¤±æƒé‡ Î»ï¼ˆé»˜è®¤: 0.1ï¼‰"
    )
    
    # PCAæ¢¯åº¦æ›´æ–°æœºåˆ¶å‚æ•°
    parser.add_argument(
        "--use_pca_grad",
        action="store_true",
        help="å¯ç”¨å®éªŒæ€§çš„PCAæ¢¯åº¦æ›´æ–°æœºåˆ¶"
    )
    
    parser.add_argument(
        "--pca_components",
        type=int,
        default=1,
        help="PCAæ¢¯åº¦æŠ•å½±ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡ï¼ˆé»˜è®¤: 1ï¼‰"
    )
    
    # è¾“å‡ºç›®å½•å‚æ•°
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤è‡ªåŠ¨æ ¹æ®è®­ç»ƒæ¨¡å¼è®¾ç½®ï¼‰"
    )
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=3,
        help="è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 3ï¼‰"
    )
    
    parser.add_argument(
        "--batch_size",
        type=int,
        default=8,
        help="è®­ç»ƒæ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 8ï¼‰"
    )
    
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=2e-4,
        help="å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 2e-4ï¼‰"
    )
    
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=10000,
        help="æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°ï¼ˆé»˜è®¤: 10000ï¼‰"
    )
    
    parser.add_argument(
        "--max_eval_samples",
        type=int,
        default=1000,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆé»˜è®¤: 1000ï¼‰"
    )
    
    # è®¾å¤‡å‚æ•°
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        choices=["auto", "cpu", "cuda"],
        help="è®­ç»ƒè®¾å¤‡ï¼ˆé»˜è®¤: autoï¼‰"
    )
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument(
        "--debug_mode",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†çš„è°ƒè¯•è¾“å‡º"
    )
    
    args = parser.parse_args()
    
    # å¤„ç†äº’æ–¥å‚æ•°
    if args.no_clora:
        args.use_clora = False
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if args.output_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if args.use_clora:
            args.output_dir = f"./logs/results_clora_{timestamp}"
        else:
            args.output_dir = f"./logs/results_lora_{timestamp}"
    
    return args


def save_training_config(args, output_dir):
    """
    ä¿å­˜è®­ç»ƒé…ç½®åˆ°æ–‡ä»¶
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
        output_dir: è¾“å‡ºç›®å½•
    """
    config = {
        "training_mode": "CLoRA" if args.use_clora else "LoRA",
        "use_clora": args.use_clora,
        "lambda_param": args.lambda_param,
        "use_pca_grad": args.use_pca_grad,
        "pca_components": args.pca_components if args.use_pca_grad else None,
        "num_epochs": args.num_epochs,
        "batch_size": args.batch_size,
        "learning_rate": args.learning_rate,
        "max_train_samples": args.max_train_samples,
        "max_eval_samples": args.max_eval_samples,
        "timestamp": datetime.now().isoformat(),
        "output_dir": output_dir
    }
    
    os.makedirs(output_dir, exist_ok=True)
    config_file = os.path.join(output_dir, "training_config.json")
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {config_file}")


def main(args=None):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if args is None:
        args = parse_arguments()
    
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    logger_name = f"{'CLoRA' if args.use_clora else 'LoRA'}_training"
    log_dir = os.path.join(args.output_dir, "logs")
    logger = setup_logger(
        name=logger_name,
        log_dir=log_dir,
        console_output=True,
        command_args=sys.argv,
        script_name="train_clora.py"
    )
    
    # é‡å®šå‘printè¾“å‡ºåˆ°æ—¥å¿—ï¼ˆä¿æŒæ§åˆ¶å°è¾“å‡ºï¼‰
    original_stdout = redirect_print_to_logger(logger, also_print=False)
    
    try:
        # è®°å½•è®­ç»ƒé…ç½®
        config_info = {
            "è®­ç»ƒæ¨¡å¼": f"{'CLoRA (å¸¦æ­£äº¤æ­£åˆ™åŒ–)' if args.use_clora else 'LoRA (æ ‡å‡†)'}",
            "è¾“å‡ºç›®å½•": args.output_dir,
            "è®­ç»ƒè½®æ•°": args.num_epochs,
            "æ‰¹æ¬¡å¤§å°": args.batch_size,
            "å­¦ä¹ ç‡": args.learning_rate,
            "æœ€å¤§è®­ç»ƒæ ·æœ¬": args.max_train_samples,
            "æœ€å¤§è¯„ä¼°æ ·æœ¬": args.max_eval_samples,
            "è°ƒè¯•æ¨¡å¼": args.debug_mode
        }
        
        if args.use_clora:
            config_info["æ­£äº¤æŸå¤±æƒé‡ (Î»)"] = args.lambda_param
        
        if args.use_pca_grad:
            config_info["PCAæ¢¯åº¦æ›´æ–°"] = f"å¯ç”¨ (ä¸»æˆåˆ†æ•°é‡: {args.pca_components})"
        
        logger.log_section(f"{'CLoRA' if args.use_clora else 'LoRA'} è®­ç»ƒå¼€å§‹")
        logger.log_config(config_info, "è®­ç»ƒé…ç½®")
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        save_training_config(args, args.output_dir)
        logger.info(f"è®­ç»ƒé…ç½®å·²ä¿å­˜åˆ°: {args.output_dir}/training_config.json")
    
        # 1. å¤„ç†æ•°æ®é›†
        logger.log_step(1, "å¤„ç†æ•°æ®é›†")
        
        # è·å–è®­ç»ƒæ•°æ®ï¼ˆCommonsenseQAè®­ç»ƒé›†ï¼‰
        train_dataset, tokenizer = get_train_data(max_samples=args.max_train_samples)
        
        # è·å–éªŒè¯æ•°æ®é›†ï¼ˆCommonsenseQAéªŒè¯é›†ï¼‰
        val_dataset = get_in_domain_eval_dataset(tokenizer, max_samples=args.max_eval_samples)
        
        if val_dataset is None:
            raise ValueError("æ— æ³•åŠ è½½CommonsenseQAéªŒè¯æ•°æ®é›†ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ•°æ®é›†å¯ç”¨æ€§")
        
        # é¢„åŠ è½½åŸŸå¤–è¯„ä¼°æ•°æ®é›†ï¼ˆç”¨äºæœ€ç»ˆæµ‹è¯•ï¼‰
        out_of_domain_datasets = get_eval_datasets(tokenizer, max_samples_per_dataset=args.max_eval_samples)
        
        dataset_info = {
            "è®­ç»ƒé›†": f"CommonsenseQA train ({len(train_dataset)} æ ·æœ¬)",
            "éªŒè¯é›†": f"CommonsenseQA validation ({len(val_dataset)} æ ·æœ¬)",
            "åŸŸå¤–æµ‹è¯•æ•°æ®é›†": list(out_of_domain_datasets.keys())
        }
        logger.log_config(dataset_info, "æ•°æ®é›†ä¿¡æ¯")
    
        # 2. è®¾ç½®æ¨¡å‹
        logger.log_step(2, "è®¾ç½®LoRAæ¨¡å‹")
        model = setup_lora_model()
        logger.info("LoRAæ¨¡å‹è®¾ç½®å®Œæˆ")
        
        # 3. ç”Ÿæˆæ­£åˆ™åŒ–çŸ©é˜µï¼ˆä»…å½“ä½¿ç”¨CLoRAæ—¶ï¼‰
        regularization_matrices = None
        if args.use_clora:
            logger.log_step(3, "ç”Ÿæˆæ­£åˆ™åŒ–çŸ©é˜µ")
            lora_rank = 8  # ä¸model_setup.pyä¸­çš„rankä¿æŒä¸€è‡´
            regularization_matrices = generate_regularization_matrices(
                lora_rank, method='orthogonal'
            )
            matrix_info = {
                "P_Aå½¢çŠ¶": str(regularization_matrices['P_A'].shape),
                "P_Bå½¢çŠ¶": str(regularization_matrices['P_B'].shape),
                "ç”Ÿæˆæ–¹æ³•": "orthogonal",
                "LoRA rank": lora_rank
            }
            logger.log_config(matrix_info, "æ­£åˆ™åŒ–çŸ©é˜µä¿¡æ¯")
        else:
            logger.log_step(3, "è·³è¿‡æ­£åˆ™åŒ–çŸ©é˜µç”Ÿæˆï¼ˆæ ‡å‡†LoRAæ¨¡å¼ï¼‰")
        
        # 4. è®¾ç½®è®­ç»ƒå‚æ•°
        logger.log_step(4, "è®¾ç½®è®­ç»ƒå‚æ•°")
        training_args = setup_training_args(
            output_dir=args.output_dir,
            num_train_epochs=args.num_epochs,
            per_device_train_batch_size=args.batch_size,
            per_device_eval_batch_size=args.batch_size * 2,
            learning_rate=args.learning_rate,
        )
        training_args_info = {
            "è®­ç»ƒè½®æ•°": args.num_epochs,
            "è®­ç»ƒæ‰¹æ¬¡å¤§å°": args.batch_size,
            "è¯„ä¼°æ‰¹æ¬¡å¤§å°": args.batch_size * 2,
            "å­¦ä¹ ç‡": args.learning_rate,
            "è¾“å‡ºç›®å½•": args.output_dir
        }
        logger.log_config(training_args_info, "è®­ç»ƒå‚æ•°é…ç½®")
    
        # 5. åˆ›å»ºæ•°æ®æ•´ç†å™¨
        logger.log_step(5, "åˆ›å»ºæ•°æ®æ•´ç†å™¨")
        data_collator = DataCollatorWithPadding(
            tokenizer=tokenizer,
            padding=True,
            return_tensors="pt"
        )
        logger.info("æ•°æ®æ•´ç†å™¨åˆ›å»ºå®Œæˆ")
        
        # 6. åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå™¨
        trainer_name = "CLoRAè®­ç»ƒå™¨" if args.use_clora else "LoRAè®­ç»ƒå™¨"
        logger.log_step(6, f"åˆ›å»º{trainer_name}")
        
        # æ·»åŠ è°ƒè¯•æ¨¡å¼å‚æ•°
        debug_mode = getattr(args, 'debug_mode', False)
        
        trainer = CLoRATrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
            compute_metrics=compute_metrics,
            regularization_matrices=regularization_matrices,
            orthogonal_weight=args.lambda_param,
            use_clora=args.use_clora,
            debug_mode=debug_mode,
            logger=logger
        )
        
        trainer_info = {
            "è®­ç»ƒå™¨ç±»å‹": trainer_name,
            "æ­£äº¤æŸå¤±æƒé‡": args.lambda_param if args.use_clora else "N/A",
            "è°ƒè¯•æ¨¡å¼": debug_mode,
            "è®­ç»ƒæ•°æ®é›†å¤§å°": len(train_dataset),
            "éªŒè¯æ•°æ®é›†å¤§å°": len(val_dataset)
        }
        logger.log_config(trainer_info, "è®­ç»ƒå™¨é…ç½®")
    
        # 7. å¼€å§‹è®­ç»ƒ
        logger.log_section("å¼€å§‹è®­ç»ƒé˜¶æ®µ")
        logger.info("æ­£åœ¨å¯åŠ¨è®­ç»ƒè¿‡ç¨‹...")
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train()
        logger.info("è®­ç»ƒè¿‡ç¨‹å®Œæˆ")
        
        # 8. è¯„ä¼°æ¨¡å‹
        logger.log_section("æ¨¡å‹è¯„ä¼°é˜¶æ®µ")
        
        # 8.1 åŸŸå†…éªŒè¯è¯„ä¼°ï¼ˆCommonsenseQA validationï¼‰
        logger.log_step("8.1", "åŸŸå†…éªŒè¯è¯„ä¼°ï¼ˆCommonsenseQAï¼‰")
        in_domain_results = trainer.evaluate()
        
        in_domain_metrics = {}
        for key, value in in_domain_results.items():
            if isinstance(value, float):
                in_domain_metrics[key] = f"{value:.4f}"
            else:
                in_domain_metrics[key] = str(value)
        
        logger.log_results(in_domain_metrics, "åŸŸå†…éªŒè¯ç»“æœï¼ˆCommonsenseQAï¼‰")
        
        # 8.2 åŸŸå¤–æµ‹è¯•è¯„ä¼°ï¼ˆBoolQ, ARC, HellaSwagç­‰ï¼‰
        logger.log_step("8.2", "åŸŸå¤–æµ‹è¯•è¯„ä¼°")
        out_of_domain_results = {}
        
        for dataset_name, dataset in out_of_domain_datasets.items():
            logger.info(f"æ­£åœ¨è¯„ä¼° {dataset_name.upper()} æ•°æ®é›†...")
            
            # ä¸´æ—¶æ›´æ¢è¯„ä¼°æ•°æ®é›†
            trainer.eval_dataset = dataset
            ood_results = trainer.evaluate()
            
            # æ ¼å¼åŒ–ç»“æœ
            ood_metrics = {}
            for key, value in ood_results.items():
                if isinstance(value, float):
                    ood_metrics[key] = f"{value:.4f}"
                else:
                    ood_metrics[key] = str(value)
            
            out_of_domain_results[dataset_name] = ood_metrics
            logger.log_results(ood_metrics, f"åŸŸå¤–æµ‹è¯•ç»“æœï¼ˆ{dataset_name.upper()}ï¼‰")
        
        # æ¢å¤åŸå§‹éªŒè¯æ•°æ®é›†
        trainer.eval_dataset = val_dataset
        
        # æ±‡æ€»æ‰€æœ‰è¯„ä¼°ç»“æœ
        eval_results = {
            "in_domain": in_domain_results,
            "out_of_domain": out_of_domain_results
        }
    
        # è®°å½•è®­ç»ƒæŸå¤±ç»Ÿè®¡ä¿¡æ¯
        loss_stats = trainer.get_loss_statistics()
        if loss_stats:
            loss_metrics = {
                "æ€»è®­ç»ƒæ­¥æ•°": loss_stats['total_steps'],
                "å¹³å‡åˆ†ç±»æŸå¤±": f"{loss_stats['avg_classification_loss']:.6f}",
                "å¹³å‡æ€»æŸå¤±": f"{loss_stats['avg_total_loss']:.6f}",
                "æœ€ç»ˆåˆ†ç±»æŸå¤±": f"{loss_stats['recent_classification_loss']:.6f}",
                "æœ€ç»ˆæ€»æŸå¤±": f"{loss_stats['recent_total_loss']:.6f}"
            }
            
            if args.use_clora:
                loss_metrics.update({
                    "å¹³å‡æ­£äº¤æŸå¤±": f"{loss_stats['avg_orthogonal_loss']:.6f}",
                    "æœ€ç»ˆæ­£äº¤æŸå¤±": f"{loss_stats['recent_orthogonal_loss']:.6f}"
                })
            
            logger.log_metrics(loss_metrics, "è®­ç»ƒæŸå¤±ç»Ÿè®¡")
    
        # 9. ä¿å­˜æ¨¡å‹
        logger.log_step(7, "ä¿å­˜æ¨¡å‹")
        final_model_dir = os.path.join(args.output_dir, "final_model")
        model.save_pretrained(final_model_dir)
        tokenizer.save_pretrained(final_model_dir)
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_dir}")
    
        # ä¿å­˜è®­ç»ƒæ€»ç»“
        training_summary = {
            "training_mode": "CLoRA" if args.use_clora else "LoRA",
            "in_domain_eval_results": {key: value for key, value in in_domain_results.items() if not key.startswith('eval_')},
            "out_of_domain_eval_results": {
                dataset_name: {key: value for key, value in results.items() if not key.startswith('eval_')}
                for dataset_name, results in out_of_domain_results.items()
            },
            "total_training_time": "è®­ç»ƒå®Œæˆ",
            "model_path": final_model_dir,
            "config": {
                "use_clora": args.use_clora,
                "lambda_param": args.lambda_param if args.use_clora else None,
                "use_pca_grad": args.use_pca_grad,
                "pca_components": args.pca_components if args.use_pca_grad else None,
                "num_epochs": args.num_epochs,
                "batch_size": args.batch_size,
                "learning_rate": args.learning_rate
            }
        }
        
        summary_file = os.path.join(args.output_dir, "training_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(training_summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è®­ç»ƒæ€»ç»“å·²ä¿å­˜åˆ°: {summary_file}")
        
        # è®°å½•è®­ç»ƒå®Œæˆ
        logger.log_section(f"{'CLoRA' if args.use_clora else 'LoRA'} è®­ç»ƒå®Œæˆ", "ğŸ‰")
        
        completion_summary = {
            "è®­ç»ƒæ¨¡å¼": "CLoRA" if args.use_clora else "LoRA",
            "æ¨¡å‹è·¯å¾„": final_model_dir,
            "æ€»ç»“æ–‡ä»¶": summary_file,
            "æ—¥å¿—æ–‡ä»¶": logger.get_log_file_path()
        }
        logger.log_config(completion_summary, "è®­ç»ƒå®Œæˆæ€»ç»“")
    
        return {
            "model_path": final_model_dir,
            "eval_results": eval_results,
            "config": training_summary["config"]
        }
        
    except KeyboardInterrupt:
        logger.error("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        logger.log_exception(e, "è®­ç»ƒè¿‡ç¨‹ä¸­")
        raise
    finally:
        # æ¢å¤åŸå§‹stdoutå¹¶å…³é—­æ—¥å¿—å™¨
        restore_print(original_stdout)
        logger.close()


if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§
    torch.manual_seed(42)
    
    # è¿è¡Œä¸»è®­ç»ƒå‡½æ•°
    try:
        main()
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise