{
  "best_global_step": 305,
  "best_metric": 0.29455445544554454,
  "best_model_checkpoint": "./results_lora_20250705_235249/checkpoint-305",
  "epoch": 8.0,
  "eval_steps": 500,
  "global_step": 2440,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "classification_loss": 6.029106140136719,
      "epoch": 0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 0,
      "total_loss": 6.029106140136719
    },
    {
      "classification_loss": 5.26099967956543,
      "epoch": 0.003278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1,
      "total_loss": 5.26099967956543
    },
    {
      "classification_loss": 6.786002159118652,
      "epoch": 0.006557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2,
      "total_loss": 6.786002159118652
    },
    {
      "classification_loss": 6.763433456420898,
      "epoch": 0.009836065573770493,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 3,
      "total_loss": 6.763433456420898
    },
    {
      "classification_loss": 7.246137619018555,
      "epoch": 0.013114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 4,
      "total_loss": 7.246137619018555
    },
    {
      "classification_loss": 5.6747283935546875,
      "epoch": 0.01639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 5,
      "total_loss": 5.6747283935546875
    },
    {
      "classification_loss": 7.14031457901001,
      "epoch": 0.019672131147540985,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 6,
      "total_loss": 7.14031457901001
    },
    {
      "classification_loss": 7.131854057312012,
      "epoch": 0.022950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 7,
      "total_loss": 7.131854057312012
    },
    {
      "classification_loss": 6.111852645874023,
      "epoch": 0.02622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 8,
      "total_loss": 6.111852645874023
    },
    {
      "classification_loss": 6.751564025878906,
      "epoch": 0.029508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 9,
      "total_loss": 6.751564025878906
    },
    {
      "classification_loss": 4.768473148345947,
      "epoch": 0.03278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 10,
      "total_loss": 4.768473148345947
    },
    {
      "classification_loss": 6.933128833770752,
      "epoch": 0.036065573770491806,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 11,
      "total_loss": 6.933128833770752
    },
    {
      "classification_loss": 6.745428085327148,
      "epoch": 0.03934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 12,
      "total_loss": 6.745428085327148
    },
    {
      "classification_loss": 7.054797172546387,
      "epoch": 0.04262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 13,
      "total_loss": 7.054797172546387
    },
    {
      "classification_loss": 6.76793909072876,
      "epoch": 0.04590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 14,
      "total_loss": 6.76793909072876
    },
    {
      "classification_loss": 5.073094844818115,
      "epoch": 0.04918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 15,
      "total_loss": 5.073094844818115
    },
    {
      "classification_loss": 6.4377055168151855,
      "epoch": 0.05245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 16,
      "total_loss": 6.4377055168151855
    },
    {
      "classification_loss": 5.5165863037109375,
      "epoch": 0.05573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 17,
      "total_loss": 5.5165863037109375
    },
    {
      "classification_loss": 7.268939018249512,
      "epoch": 0.05901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 18,
      "total_loss": 7.268939018249512
    },
    {
      "classification_loss": 6.805829048156738,
      "epoch": 0.06229508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 19,
      "total_loss": 6.805829048156738
    },
    {
      "classification_loss": 6.703608512878418,
      "epoch": 0.06557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 20,
      "total_loss": 6.703608512878418
    },
    {
      "classification_loss": 5.311041831970215,
      "epoch": 0.06885245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 21,
      "total_loss": 5.311041831970215
    },
    {
      "classification_loss": 5.257755279541016,
      "epoch": 0.07213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 22,
      "total_loss": 5.257755279541016
    },
    {
      "classification_loss": 6.257777214050293,
      "epoch": 0.07540983606557378,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 23,
      "total_loss": 6.257777214050293
    },
    {
      "classification_loss": 5.339997291564941,
      "epoch": 0.07868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 24,
      "total_loss": 5.339997291564941
    },
    {
      "classification_loss": 5.6603007316589355,
      "epoch": 0.08196721311475409,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 25,
      "total_loss": 5.6603007316589355
    },
    {
      "classification_loss": 5.220198631286621,
      "epoch": 0.08524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 26,
      "total_loss": 5.220198631286621
    },
    {
      "classification_loss": 5.460502624511719,
      "epoch": 0.08852459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 27,
      "total_loss": 5.460502624511719
    },
    {
      "classification_loss": 4.58080530166626,
      "epoch": 0.09180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 28,
      "total_loss": 4.58080530166626
    },
    {
      "classification_loss": 6.558686256408691,
      "epoch": 0.09508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 29,
      "total_loss": 6.558686256408691
    },
    {
      "classification_loss": 4.91140079498291,
      "epoch": 0.09836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 30,
      "total_loss": 4.91140079498291
    },
    {
      "classification_loss": 5.6349334716796875,
      "epoch": 0.10163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 31,
      "total_loss": 5.6349334716796875
    },
    {
      "classification_loss": 5.403424263000488,
      "epoch": 0.10491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 32,
      "total_loss": 5.403424263000488
    },
    {
      "classification_loss": 3.603884220123291,
      "epoch": 0.10819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 33,
      "total_loss": 3.603884220123291
    },
    {
      "classification_loss": 4.430056095123291,
      "epoch": 0.11147540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 34,
      "total_loss": 4.430056095123291
    },
    {
      "classification_loss": 5.044696807861328,
      "epoch": 0.11475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 35,
      "total_loss": 5.044696807861328
    },
    {
      "classification_loss": 4.773556709289551,
      "epoch": 0.1180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 36,
      "total_loss": 4.773556709289551
    },
    {
      "classification_loss": 5.51384973526001,
      "epoch": 0.12131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 37,
      "total_loss": 5.51384973526001
    },
    {
      "classification_loss": 4.824620723724365,
      "epoch": 0.12459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 38,
      "total_loss": 4.824620723724365
    },
    {
      "classification_loss": 4.803764343261719,
      "epoch": 0.12786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 39,
      "total_loss": 4.803764343261719
    },
    {
      "classification_loss": 3.645808696746826,
      "epoch": 0.13114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 40,
      "total_loss": 3.645808696746826
    },
    {
      "classification_loss": 4.754886627197266,
      "epoch": 0.13442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 41,
      "total_loss": 4.754886627197266
    },
    {
      "classification_loss": 3.2896270751953125,
      "epoch": 0.1377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 42,
      "total_loss": 3.2896270751953125
    },
    {
      "classification_loss": 3.987785816192627,
      "epoch": 0.14098360655737704,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 43,
      "total_loss": 3.987785816192627
    },
    {
      "classification_loss": 3.4023499488830566,
      "epoch": 0.14426229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 44,
      "total_loss": 3.4023499488830566
    },
    {
      "classification_loss": 2.4281792640686035,
      "epoch": 0.14754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 45,
      "total_loss": 2.4281792640686035
    },
    {
      "classification_loss": 2.922004222869873,
      "epoch": 0.15081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 46,
      "total_loss": 2.922004222869873
    },
    {
      "classification_loss": 2.2227625846862793,
      "epoch": 0.1540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 47,
      "total_loss": 2.2227625846862793
    },
    {
      "classification_loss": 2.1517064571380615,
      "epoch": 0.15737704918032788,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 48,
      "total_loss": 2.1517064571380615
    },
    {
      "classification_loss": 1.749600887298584,
      "epoch": 0.16065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 49,
      "total_loss": 1.749600887298584
    },
    {
      "classification_loss": 1.7081162929534912,
      "epoch": 0.16393442622950818,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 50,
      "total_loss": 1.7081162929534912
    },
    {
      "classification_loss": 1.1040520668029785,
      "epoch": 0.16721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 51,
      "total_loss": 1.1040520668029785
    },
    {
      "classification_loss": 1.0640112161636353,
      "epoch": 0.17049180327868851,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 52,
      "total_loss": 1.0640112161636353
    },
    {
      "classification_loss": 1.0751070976257324,
      "epoch": 0.1737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 53,
      "total_loss": 1.0751070976257324
    },
    {
      "classification_loss": 1.1094213724136353,
      "epoch": 0.17704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 54,
      "total_loss": 1.1094213724136353
    },
    {
      "classification_loss": 1.2306082248687744,
      "epoch": 0.18032786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 55,
      "total_loss": 1.2306082248687744
    },
    {
      "classification_loss": 1.1208399534225464,
      "epoch": 0.18360655737704917,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 56,
      "total_loss": 1.1208399534225464
    },
    {
      "classification_loss": 1.0001277923583984,
      "epoch": 0.18688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 57,
      "total_loss": 1.0001277923583984
    },
    {
      "classification_loss": 0.9024312496185303,
      "epoch": 0.1901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 58,
      "total_loss": 0.9024312496185303
    },
    {
      "classification_loss": 0.8169190883636475,
      "epoch": 0.19344262295081968,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 59,
      "total_loss": 0.8169190883636475
    },
    {
      "classification_loss": 0.809917151927948,
      "epoch": 0.19672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 60,
      "total_loss": 0.809917151927948
    },
    {
      "classification_loss": 1.0662477016448975,
      "epoch": 0.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 61,
      "total_loss": 1.0662477016448975
    },
    {
      "classification_loss": 0.9991847276687622,
      "epoch": 0.20327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 62,
      "total_loss": 0.9991847276687622
    },
    {
      "classification_loss": 1.049583077430725,
      "epoch": 0.20655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 63,
      "total_loss": 1.049583077430725
    },
    {
      "classification_loss": 1.0871797800064087,
      "epoch": 0.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 64,
      "total_loss": 1.0871797800064087
    },
    {
      "classification_loss": 0.9566490650177002,
      "epoch": 0.21311475409836064,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 65,
      "total_loss": 0.9566490650177002
    },
    {
      "classification_loss": 1.1002774238586426,
      "epoch": 0.21639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 66,
      "total_loss": 1.1002774238586426
    },
    {
      "classification_loss": 0.9420120120048523,
      "epoch": 0.21967213114754097,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 67,
      "total_loss": 0.9420120120048523
    },
    {
      "classification_loss": 0.9195044636726379,
      "epoch": 0.22295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 68,
      "total_loss": 0.9195044636726379
    },
    {
      "classification_loss": 0.941886842250824,
      "epoch": 0.2262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 69,
      "total_loss": 0.941886842250824
    },
    {
      "classification_loss": 0.9129692316055298,
      "epoch": 0.22950819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 70,
      "total_loss": 0.9129692316055298
    },
    {
      "classification_loss": 0.966028094291687,
      "epoch": 0.23278688524590163,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 71,
      "total_loss": 0.966028094291687
    },
    {
      "classification_loss": 0.7267212867736816,
      "epoch": 0.2360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 72,
      "total_loss": 0.7267212867736816
    },
    {
      "classification_loss": 0.8605775237083435,
      "epoch": 0.23934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 73,
      "total_loss": 0.8605775237083435
    },
    {
      "classification_loss": 0.8440930247306824,
      "epoch": 0.24262295081967214,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 74,
      "total_loss": 0.8440930247306824
    },
    {
      "classification_loss": 0.9578810334205627,
      "epoch": 0.2459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 75,
      "total_loss": 0.9578810334205627
    },
    {
      "classification_loss": 0.739755392074585,
      "epoch": 0.24918032786885247,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 76,
      "total_loss": 0.739755392074585
    },
    {
      "classification_loss": 0.6734002828598022,
      "epoch": 0.25245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 77,
      "total_loss": 0.6734002828598022
    },
    {
      "classification_loss": 0.8013323545455933,
      "epoch": 0.25573770491803277,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 78,
      "total_loss": 0.8013323545455933
    },
    {
      "classification_loss": 0.8135108351707458,
      "epoch": 0.25901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 79,
      "total_loss": 0.8135108351707458
    },
    {
      "classification_loss": 0.7676095962524414,
      "epoch": 0.26229508196721313,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 80,
      "total_loss": 0.7676095962524414
    },
    {
      "classification_loss": 0.8518051505088806,
      "epoch": 0.26557377049180325,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 81,
      "total_loss": 0.8518051505088806
    },
    {
      "classification_loss": 0.7586124539375305,
      "epoch": 0.26885245901639343,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 82,
      "total_loss": 0.7586124539375305
    },
    {
      "classification_loss": 0.8029384613037109,
      "epoch": 0.2721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 83,
      "total_loss": 0.8029384613037109
    },
    {
      "classification_loss": 0.7209784388542175,
      "epoch": 0.2754098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 84,
      "total_loss": 0.7209784388542175
    },
    {
      "classification_loss": 0.8773118257522583,
      "epoch": 0.2786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 85,
      "total_loss": 0.8773118257522583
    },
    {
      "classification_loss": 0.901491641998291,
      "epoch": 0.2819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 86,
      "total_loss": 0.901491641998291
    },
    {
      "classification_loss": 0.8136327266693115,
      "epoch": 0.28524590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 87,
      "total_loss": 0.8136327266693115
    },
    {
      "classification_loss": 0.8451808094978333,
      "epoch": 0.28852459016393445,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 88,
      "total_loss": 0.8451808094978333
    },
    {
      "classification_loss": 0.8602125644683838,
      "epoch": 0.29180327868852457,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 89,
      "total_loss": 0.8602125644683838
    },
    {
      "classification_loss": 0.7479987740516663,
      "epoch": 0.29508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 90,
      "total_loss": 0.7479987740516663
    },
    {
      "classification_loss": 0.6634376645088196,
      "epoch": 0.2983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 91,
      "total_loss": 0.6634376645088196
    },
    {
      "classification_loss": 0.8941195011138916,
      "epoch": 0.3016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 92,
      "total_loss": 0.8941195011138916
    },
    {
      "classification_loss": 0.817435622215271,
      "epoch": 0.30491803278688523,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 93,
      "total_loss": 0.817435622215271
    },
    {
      "classification_loss": 0.8090084195137024,
      "epoch": 0.3081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 94,
      "total_loss": 0.8090084195137024
    },
    {
      "classification_loss": 0.6875966191291809,
      "epoch": 0.3114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 95,
      "total_loss": 0.6875966191291809
    },
    {
      "classification_loss": 0.7169122099876404,
      "epoch": 0.31475409836065577,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 96,
      "total_loss": 0.7169122099876404
    },
    {
      "classification_loss": 0.7003886699676514,
      "epoch": 0.3180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 97,
      "total_loss": 0.7003886699676514
    },
    {
      "classification_loss": 0.7689865827560425,
      "epoch": 0.32131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 98,
      "total_loss": 0.7689865827560425
    },
    {
      "classification_loss": 0.6941261291503906,
      "epoch": 0.32459016393442625,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 99,
      "total_loss": 0.6941261291503906
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 5.701595306396484,
      "learning_rate": 0.00019800000000000002,
      "loss": 3.0912,
      "step": 100
    },
    {
      "classification_loss": 0.742901086807251,
      "epoch": 0.32786885245901637,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 100,
      "total_loss": 0.742901086807251
    },
    {
      "classification_loss": 0.6920511722564697,
      "epoch": 0.33114754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 101,
      "total_loss": 0.6920511722564697
    },
    {
      "classification_loss": 0.7078407406806946,
      "epoch": 0.3344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 102,
      "total_loss": 0.7078407406806946
    },
    {
      "classification_loss": 0.752394437789917,
      "epoch": 0.3377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 103,
      "total_loss": 0.752394437789917
    },
    {
      "classification_loss": 0.7132714986801147,
      "epoch": 0.34098360655737703,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 104,
      "total_loss": 0.7132714986801147
    },
    {
      "classification_loss": 0.7066264152526855,
      "epoch": 0.3442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 105,
      "total_loss": 0.7066264152526855
    },
    {
      "classification_loss": 0.7448021769523621,
      "epoch": 0.3475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 106,
      "total_loss": 0.7448021769523621
    },
    {
      "classification_loss": 0.692280113697052,
      "epoch": 0.35081967213114756,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 107,
      "total_loss": 0.692280113697052
    },
    {
      "classification_loss": 0.6864626407623291,
      "epoch": 0.3540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 108,
      "total_loss": 0.6864626407623291
    },
    {
      "classification_loss": 0.7038480043411255,
      "epoch": 0.35737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 109,
      "total_loss": 0.7038480043411255
    },
    {
      "classification_loss": 0.7720316648483276,
      "epoch": 0.36065573770491804,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 110,
      "total_loss": 0.7720316648483276
    },
    {
      "classification_loss": 0.7428768873214722,
      "epoch": 0.3639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 111,
      "total_loss": 0.7428768873214722
    },
    {
      "classification_loss": 0.7480695247650146,
      "epoch": 0.36721311475409835,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 112,
      "total_loss": 0.7480695247650146
    },
    {
      "classification_loss": 0.7385047674179077,
      "epoch": 0.3704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 113,
      "total_loss": 0.7385047674179077
    },
    {
      "classification_loss": 0.7424154877662659,
      "epoch": 0.3737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 114,
      "total_loss": 0.7424154877662659
    },
    {
      "classification_loss": 0.7708160877227783,
      "epoch": 0.3770491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 115,
      "total_loss": 0.7708160877227783
    },
    {
      "classification_loss": 0.7045778632164001,
      "epoch": 0.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 116,
      "total_loss": 0.7045778632164001
    },
    {
      "classification_loss": 0.6869282126426697,
      "epoch": 0.3836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 117,
      "total_loss": 0.6869282126426697
    },
    {
      "classification_loss": 0.686357855796814,
      "epoch": 0.38688524590163936,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 118,
      "total_loss": 0.686357855796814
    },
    {
      "classification_loss": 0.7250933051109314,
      "epoch": 0.3901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 119,
      "total_loss": 0.7250933051109314
    },
    {
      "classification_loss": 0.7211783528327942,
      "epoch": 0.39344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 120,
      "total_loss": 0.7211783528327942
    },
    {
      "classification_loss": 0.7074207663536072,
      "epoch": 0.39672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 121,
      "total_loss": 0.7074207663536072
    },
    {
      "classification_loss": 0.7038979530334473,
      "epoch": 0.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 122,
      "total_loss": 0.7038979530334473
    },
    {
      "classification_loss": 0.713259756565094,
      "epoch": 0.40327868852459015,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 123,
      "total_loss": 0.713259756565094
    },
    {
      "classification_loss": 0.7022611498832703,
      "epoch": 0.4065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 124,
      "total_loss": 0.7022611498832703
    },
    {
      "classification_loss": 0.7241578698158264,
      "epoch": 0.4098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 125,
      "total_loss": 0.7241578698158264
    },
    {
      "classification_loss": 0.6973316669464111,
      "epoch": 0.4131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 126,
      "total_loss": 0.6973316669464111
    },
    {
      "classification_loss": 0.7200682759284973,
      "epoch": 0.4163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 127,
      "total_loss": 0.7200682759284973
    },
    {
      "classification_loss": 0.7193293571472168,
      "epoch": 0.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 128,
      "total_loss": 0.7193293571472168
    },
    {
      "classification_loss": 0.7121246457099915,
      "epoch": 0.42295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 129,
      "total_loss": 0.7121246457099915
    },
    {
      "classification_loss": 0.7147690653800964,
      "epoch": 0.4262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 130,
      "total_loss": 0.7147690653800964
    },
    {
      "classification_loss": 0.7148407697677612,
      "epoch": 0.42950819672131146,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 131,
      "total_loss": 0.7148407697677612
    },
    {
      "classification_loss": 0.6894114017486572,
      "epoch": 0.43278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 132,
      "total_loss": 0.6894114017486572
    },
    {
      "classification_loss": 0.7117214202880859,
      "epoch": 0.4360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 133,
      "total_loss": 0.7117214202880859
    },
    {
      "classification_loss": 0.7498599290847778,
      "epoch": 0.43934426229508194,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 134,
      "total_loss": 0.7498599290847778
    },
    {
      "classification_loss": 0.7207503318786621,
      "epoch": 0.4426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 135,
      "total_loss": 0.7207503318786621
    },
    {
      "classification_loss": 0.7144953608512878,
      "epoch": 0.4459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 136,
      "total_loss": 0.7144953608512878
    },
    {
      "classification_loss": 0.7184303402900696,
      "epoch": 0.4491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 137,
      "total_loss": 0.7184303402900696
    },
    {
      "classification_loss": 0.7242590188980103,
      "epoch": 0.4524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 138,
      "total_loss": 0.7242590188980103
    },
    {
      "classification_loss": 0.7010200619697571,
      "epoch": 0.4557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 139,
      "total_loss": 0.7010200619697571
    },
    {
      "classification_loss": 0.6684954166412354,
      "epoch": 0.45901639344262296,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 140,
      "total_loss": 0.6684954166412354
    },
    {
      "classification_loss": 0.6995996236801147,
      "epoch": 0.46229508196721314,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 141,
      "total_loss": 0.6995996236801147
    },
    {
      "classification_loss": 0.7771286964416504,
      "epoch": 0.46557377049180326,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 142,
      "total_loss": 0.7771286964416504
    },
    {
      "classification_loss": 0.7298861145973206,
      "epoch": 0.46885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 143,
      "total_loss": 0.7298861145973206
    },
    {
      "classification_loss": 0.7282312512397766,
      "epoch": 0.4721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 144,
      "total_loss": 0.7282312512397766
    },
    {
      "classification_loss": 0.6427392959594727,
      "epoch": 0.47540983606557374,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 145,
      "total_loss": 0.6427392959594727
    },
    {
      "classification_loss": 0.7246519923210144,
      "epoch": 0.4786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 146,
      "total_loss": 0.7246519923210144
    },
    {
      "classification_loss": 0.730600893497467,
      "epoch": 0.4819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 147,
      "total_loss": 0.730600893497467
    },
    {
      "classification_loss": 0.7111387252807617,
      "epoch": 0.4852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 148,
      "total_loss": 0.7111387252807617
    },
    {
      "classification_loss": 0.7102357745170593,
      "epoch": 0.4885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 149,
      "total_loss": 0.7102357745170593
    },
    {
      "classification_loss": 0.7123368978500366,
      "epoch": 0.4918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 150,
      "total_loss": 0.7123368978500366
    },
    {
      "classification_loss": 0.7341248393058777,
      "epoch": 0.49508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 151,
      "total_loss": 0.7341248393058777
    },
    {
      "classification_loss": 0.7222155928611755,
      "epoch": 0.49836065573770494,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 152,
      "total_loss": 0.7222155928611755
    },
    {
      "classification_loss": 0.7102056741714478,
      "epoch": 0.5016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 153,
      "total_loss": 0.7102056741714478
    },
    {
      "classification_loss": 0.7005139589309692,
      "epoch": 0.5049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 154,
      "total_loss": 0.7005139589309692
    },
    {
      "classification_loss": 0.6699138283729553,
      "epoch": 0.5081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 155,
      "total_loss": 0.6699138283729553
    },
    {
      "classification_loss": 0.7051196098327637,
      "epoch": 0.5114754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 156,
      "total_loss": 0.7051196098327637
    },
    {
      "classification_loss": 0.7138460278511047,
      "epoch": 0.5147540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 157,
      "total_loss": 0.7138460278511047
    },
    {
      "classification_loss": 0.7164344191551208,
      "epoch": 0.5180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 158,
      "total_loss": 0.7164344191551208
    },
    {
      "classification_loss": 0.7110159397125244,
      "epoch": 0.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 159,
      "total_loss": 0.7110159397125244
    },
    {
      "classification_loss": 0.6978866457939148,
      "epoch": 0.5245901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 160,
      "total_loss": 0.6978866457939148
    },
    {
      "classification_loss": 0.7022634744644165,
      "epoch": 0.5278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 161,
      "total_loss": 0.7022634744644165
    },
    {
      "classification_loss": 0.7068108916282654,
      "epoch": 0.5311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 162,
      "total_loss": 0.7068108916282654
    },
    {
      "classification_loss": 0.701731264591217,
      "epoch": 0.5344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 163,
      "total_loss": 0.701731264591217
    },
    {
      "classification_loss": 0.7147204279899597,
      "epoch": 0.5377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 164,
      "total_loss": 0.7147204279899597
    },
    {
      "classification_loss": 0.6866638660430908,
      "epoch": 0.5409836065573771,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 165,
      "total_loss": 0.6866638660430908
    },
    {
      "classification_loss": 0.7390316724777222,
      "epoch": 0.5442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 166,
      "total_loss": 0.7390316724777222
    },
    {
      "classification_loss": 0.7071415185928345,
      "epoch": 0.5475409836065573,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 167,
      "total_loss": 0.7071415185928345
    },
    {
      "classification_loss": 0.689320981502533,
      "epoch": 0.5508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 168,
      "total_loss": 0.689320981502533
    },
    {
      "classification_loss": 0.7058994174003601,
      "epoch": 0.5540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 169,
      "total_loss": 0.7058994174003601
    },
    {
      "classification_loss": 0.7203948497772217,
      "epoch": 0.5573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 170,
      "total_loss": 0.7203948497772217
    },
    {
      "classification_loss": 0.7007912397384644,
      "epoch": 0.5606557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 171,
      "total_loss": 0.7007912397384644
    },
    {
      "classification_loss": 0.6909976005554199,
      "epoch": 0.5639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 172,
      "total_loss": 0.6909976005554199
    },
    {
      "classification_loss": 0.7286224961280823,
      "epoch": 0.5672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 173,
      "total_loss": 0.7286224961280823
    },
    {
      "classification_loss": 0.7029265761375427,
      "epoch": 0.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 174,
      "total_loss": 0.7029265761375427
    },
    {
      "classification_loss": 0.7289374470710754,
      "epoch": 0.5737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 175,
      "total_loss": 0.7289374470710754
    },
    {
      "classification_loss": 0.7278526425361633,
      "epoch": 0.5770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 176,
      "total_loss": 0.7278526425361633
    },
    {
      "classification_loss": 0.7072170376777649,
      "epoch": 0.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 177,
      "total_loss": 0.7072170376777649
    },
    {
      "classification_loss": 0.6945974826812744,
      "epoch": 0.5836065573770491,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 178,
      "total_loss": 0.6945974826812744
    },
    {
      "classification_loss": 0.6839261054992676,
      "epoch": 0.5868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 179,
      "total_loss": 0.6839261054992676
    },
    {
      "classification_loss": 0.6654853224754333,
      "epoch": 0.5901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 180,
      "total_loss": 0.6654853224754333
    },
    {
      "classification_loss": 0.6980960369110107,
      "epoch": 0.5934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 181,
      "total_loss": 0.6980960369110107
    },
    {
      "classification_loss": 0.7297942638397217,
      "epoch": 0.5967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 182,
      "total_loss": 0.7297942638397217
    },
    {
      "classification_loss": 0.6894410252571106,
      "epoch": 0.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 183,
      "total_loss": 0.6894410252571106
    },
    {
      "classification_loss": 0.7140787243843079,
      "epoch": 0.6032786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 184,
      "total_loss": 0.7140787243843079
    },
    {
      "classification_loss": 0.6974874138832092,
      "epoch": 0.6065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 185,
      "total_loss": 0.6974874138832092
    },
    {
      "classification_loss": 0.7107126712799072,
      "epoch": 0.6098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 186,
      "total_loss": 0.7107126712799072
    },
    {
      "classification_loss": 0.7369199991226196,
      "epoch": 0.6131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 187,
      "total_loss": 0.7369199991226196
    },
    {
      "classification_loss": 0.705725908279419,
      "epoch": 0.6163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 188,
      "total_loss": 0.705725908279419
    },
    {
      "classification_loss": 0.7055432200431824,
      "epoch": 0.6196721311475409,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 189,
      "total_loss": 0.7055432200431824
    },
    {
      "classification_loss": 0.6962285041809082,
      "epoch": 0.6229508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 190,
      "total_loss": 0.6962285041809082
    },
    {
      "classification_loss": 0.7342478036880493,
      "epoch": 0.6262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 191,
      "total_loss": 0.7342478036880493
    },
    {
      "classification_loss": 0.7222051620483398,
      "epoch": 0.6295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 192,
      "total_loss": 0.7222051620483398
    },
    {
      "classification_loss": 0.6642899513244629,
      "epoch": 0.6327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 193,
      "total_loss": 0.6642899513244629
    },
    {
      "classification_loss": 0.7100484371185303,
      "epoch": 0.6360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 194,
      "total_loss": 0.7100484371185303
    },
    {
      "classification_loss": 0.713492214679718,
      "epoch": 0.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 195,
      "total_loss": 0.713492214679718
    },
    {
      "classification_loss": 0.6334360837936401,
      "epoch": 0.6426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 196,
      "total_loss": 0.6334360837936401
    },
    {
      "classification_loss": 0.6974565386772156,
      "epoch": 0.6459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 197,
      "total_loss": 0.6974565386772156
    },
    {
      "classification_loss": 0.702074408531189,
      "epoch": 0.6491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 198,
      "total_loss": 0.702074408531189
    },
    {
      "classification_loss": 0.7526955008506775,
      "epoch": 0.6524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 199,
      "total_loss": 0.7526955008506775
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 4.703481674194336,
      "learning_rate": 0.0001967,
      "loss": 0.7118,
      "step": 200
    },
    {
      "classification_loss": 0.7006101608276367,
      "epoch": 0.6557377049180327,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 200,
      "total_loss": 0.7006101608276367
    },
    {
      "classification_loss": 0.7057560086250305,
      "epoch": 0.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 201,
      "total_loss": 0.7057560086250305
    },
    {
      "classification_loss": 0.6795766353607178,
      "epoch": 0.6622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 202,
      "total_loss": 0.6795766353607178
    },
    {
      "classification_loss": 0.6476075649261475,
      "epoch": 0.6655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 203,
      "total_loss": 0.6476075649261475
    },
    {
      "classification_loss": 0.7112053632736206,
      "epoch": 0.6688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 204,
      "total_loss": 0.7112053632736206
    },
    {
      "classification_loss": 0.704059362411499,
      "epoch": 0.6721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 205,
      "total_loss": 0.704059362411499
    },
    {
      "classification_loss": 0.7086321115493774,
      "epoch": 0.6754098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 206,
      "total_loss": 0.7086321115493774
    },
    {
      "classification_loss": 0.6958596110343933,
      "epoch": 0.6786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 207,
      "total_loss": 0.6958596110343933
    },
    {
      "classification_loss": 0.6778581142425537,
      "epoch": 0.6819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 208,
      "total_loss": 0.6778581142425537
    },
    {
      "classification_loss": 0.6778616309165955,
      "epoch": 0.6852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 209,
      "total_loss": 0.6778616309165955
    },
    {
      "classification_loss": 0.7022836804389954,
      "epoch": 0.6885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 210,
      "total_loss": 0.7022836804389954
    },
    {
      "classification_loss": 0.7133873105049133,
      "epoch": 0.6918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 211,
      "total_loss": 0.7133873105049133
    },
    {
      "classification_loss": 0.6909018158912659,
      "epoch": 0.6950819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 212,
      "total_loss": 0.6909018158912659
    },
    {
      "classification_loss": 0.6824254393577576,
      "epoch": 0.6983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 213,
      "total_loss": 0.6824254393577576
    },
    {
      "classification_loss": 0.7163437604904175,
      "epoch": 0.7016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 214,
      "total_loss": 0.7163437604904175
    },
    {
      "classification_loss": 0.6583008766174316,
      "epoch": 0.7049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 215,
      "total_loss": 0.6583008766174316
    },
    {
      "classification_loss": 0.6642976403236389,
      "epoch": 0.7081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 216,
      "total_loss": 0.6642976403236389
    },
    {
      "classification_loss": 0.6475402116775513,
      "epoch": 0.7114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 217,
      "total_loss": 0.6475402116775513
    },
    {
      "classification_loss": 0.6955196261405945,
      "epoch": 0.7147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 218,
      "total_loss": 0.6955196261405945
    },
    {
      "classification_loss": 0.7278201580047607,
      "epoch": 0.7180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 219,
      "total_loss": 0.7278201580047607
    },
    {
      "classification_loss": 0.7335761189460754,
      "epoch": 0.7213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 220,
      "total_loss": 0.7335761189460754
    },
    {
      "classification_loss": 0.6683060526847839,
      "epoch": 0.7245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 221,
      "total_loss": 0.6683060526847839
    },
    {
      "classification_loss": 0.7194944024085999,
      "epoch": 0.7278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 222,
      "total_loss": 0.7194944024085999
    },
    {
      "classification_loss": 0.718978762626648,
      "epoch": 0.7311475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 223,
      "total_loss": 0.718978762626648
    },
    {
      "classification_loss": 0.7054257988929749,
      "epoch": 0.7344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 224,
      "total_loss": 0.7054257988929749
    },
    {
      "classification_loss": 0.718989372253418,
      "epoch": 0.7377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 225,
      "total_loss": 0.718989372253418
    },
    {
      "classification_loss": 0.7282262444496155,
      "epoch": 0.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 226,
      "total_loss": 0.7282262444496155
    },
    {
      "classification_loss": 0.6980904340744019,
      "epoch": 0.7442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 227,
      "total_loss": 0.6980904340744019
    },
    {
      "classification_loss": 0.6713348031044006,
      "epoch": 0.7475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 228,
      "total_loss": 0.6713348031044006
    },
    {
      "classification_loss": 0.7151282429695129,
      "epoch": 0.7508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 229,
      "total_loss": 0.7151282429695129
    },
    {
      "classification_loss": 0.6882845163345337,
      "epoch": 0.7540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 230,
      "total_loss": 0.6882845163345337
    },
    {
      "classification_loss": 0.7072422504425049,
      "epoch": 0.7573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 231,
      "total_loss": 0.7072422504425049
    },
    {
      "classification_loss": 0.6815532445907593,
      "epoch": 0.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 232,
      "total_loss": 0.6815532445907593
    },
    {
      "classification_loss": 0.6569985151290894,
      "epoch": 0.7639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 233,
      "total_loss": 0.6569985151290894
    },
    {
      "classification_loss": 0.6934366822242737,
      "epoch": 0.7672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 234,
      "total_loss": 0.6934366822242737
    },
    {
      "classification_loss": 0.6896246671676636,
      "epoch": 0.7704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 235,
      "total_loss": 0.6896246671676636
    },
    {
      "classification_loss": 0.726188063621521,
      "epoch": 0.7737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 236,
      "total_loss": 0.726188063621521
    },
    {
      "classification_loss": 0.6986669301986694,
      "epoch": 0.7770491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 237,
      "total_loss": 0.6986669301986694
    },
    {
      "classification_loss": 0.7299679517745972,
      "epoch": 0.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 238,
      "total_loss": 0.7299679517745972
    },
    {
      "classification_loss": 0.681686520576477,
      "epoch": 0.7836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 239,
      "total_loss": 0.681686520576477
    },
    {
      "classification_loss": 0.6903578042984009,
      "epoch": 0.7868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 240,
      "total_loss": 0.6903578042984009
    },
    {
      "classification_loss": 0.7175540328025818,
      "epoch": 0.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 241,
      "total_loss": 0.7175540328025818
    },
    {
      "classification_loss": 0.6984310746192932,
      "epoch": 0.7934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 242,
      "total_loss": 0.6984310746192932
    },
    {
      "classification_loss": 0.6987614631652832,
      "epoch": 0.7967213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 243,
      "total_loss": 0.6987614631652832
    },
    {
      "classification_loss": 0.6763342618942261,
      "epoch": 0.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 244,
      "total_loss": 0.6763342618942261
    },
    {
      "classification_loss": 0.7228949069976807,
      "epoch": 0.8032786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 245,
      "total_loss": 0.7228949069976807
    },
    {
      "classification_loss": 0.6791761517524719,
      "epoch": 0.8065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 246,
      "total_loss": 0.6791761517524719
    },
    {
      "classification_loss": 0.6982665061950684,
      "epoch": 0.8098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 247,
      "total_loss": 0.6982665061950684
    },
    {
      "classification_loss": 0.7332811951637268,
      "epoch": 0.8131147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 248,
      "total_loss": 0.7332811951637268
    },
    {
      "classification_loss": 0.6729944348335266,
      "epoch": 0.8163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 249,
      "total_loss": 0.6729944348335266
    },
    {
      "classification_loss": 0.6996914148330688,
      "epoch": 0.819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 250,
      "total_loss": 0.6996914148330688
    },
    {
      "classification_loss": 0.7072135806083679,
      "epoch": 0.8229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 251,
      "total_loss": 0.7072135806083679
    },
    {
      "classification_loss": 0.6905090808868408,
      "epoch": 0.8262295081967214,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 252,
      "total_loss": 0.6905090808868408
    },
    {
      "classification_loss": 0.671786904335022,
      "epoch": 0.8295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 253,
      "total_loss": 0.671786904335022
    },
    {
      "classification_loss": 0.6960476636886597,
      "epoch": 0.8327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 254,
      "total_loss": 0.6960476636886597
    },
    {
      "classification_loss": 0.6613081693649292,
      "epoch": 0.8360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 255,
      "total_loss": 0.6613081693649292
    },
    {
      "classification_loss": 0.6890307664871216,
      "epoch": 0.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 256,
      "total_loss": 0.6890307664871216
    },
    {
      "classification_loss": 0.6780089139938354,
      "epoch": 0.8426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 257,
      "total_loss": 0.6780089139938354
    },
    {
      "classification_loss": 0.6876348257064819,
      "epoch": 0.8459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 258,
      "total_loss": 0.6876348257064819
    },
    {
      "classification_loss": 0.7140586972236633,
      "epoch": 0.8491803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 259,
      "total_loss": 0.7140586972236633
    },
    {
      "classification_loss": 0.69904625415802,
      "epoch": 0.8524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 260,
      "total_loss": 0.69904625415802
    },
    {
      "classification_loss": 0.6914536952972412,
      "epoch": 0.8557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 261,
      "total_loss": 0.6914536952972412
    },
    {
      "classification_loss": 0.6884915828704834,
      "epoch": 0.8590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 262,
      "total_loss": 0.6884915828704834
    },
    {
      "classification_loss": 0.686316967010498,
      "epoch": 0.8622950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 263,
      "total_loss": 0.686316967010498
    },
    {
      "classification_loss": 0.6936506628990173,
      "epoch": 0.8655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 264,
      "total_loss": 0.6936506628990173
    },
    {
      "classification_loss": 0.7467581629753113,
      "epoch": 0.8688524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 265,
      "total_loss": 0.7467581629753113
    },
    {
      "classification_loss": 0.706075131893158,
      "epoch": 0.8721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 266,
      "total_loss": 0.706075131893158
    },
    {
      "classification_loss": 0.680850625038147,
      "epoch": 0.8754098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 267,
      "total_loss": 0.680850625038147
    },
    {
      "classification_loss": 0.6794779896736145,
      "epoch": 0.8786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 268,
      "total_loss": 0.6794779896736145
    },
    {
      "classification_loss": 0.6976137757301331,
      "epoch": 0.8819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 269,
      "total_loss": 0.6976137757301331
    },
    {
      "classification_loss": 0.6684204339981079,
      "epoch": 0.8852459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 270,
      "total_loss": 0.6684204339981079
    },
    {
      "classification_loss": 0.7025395035743713,
      "epoch": 0.8885245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 271,
      "total_loss": 0.7025395035743713
    },
    {
      "classification_loss": 0.6849387884140015,
      "epoch": 0.8918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 272,
      "total_loss": 0.6849387884140015
    },
    {
      "classification_loss": 0.6784437894821167,
      "epoch": 0.8950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 273,
      "total_loss": 0.6784437894821167
    },
    {
      "classification_loss": 0.6674915552139282,
      "epoch": 0.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 274,
      "total_loss": 0.6674915552139282
    },
    {
      "classification_loss": 0.6754313707351685,
      "epoch": 0.9016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 275,
      "total_loss": 0.6754313707351685
    },
    {
      "classification_loss": 0.6888018846511841,
      "epoch": 0.9049180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 276,
      "total_loss": 0.6888018846511841
    },
    {
      "classification_loss": 0.7052072882652283,
      "epoch": 0.9081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 277,
      "total_loss": 0.7052072882652283
    },
    {
      "classification_loss": 0.6901190876960754,
      "epoch": 0.9114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 278,
      "total_loss": 0.6901190876960754
    },
    {
      "classification_loss": 0.7169199585914612,
      "epoch": 0.9147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 279,
      "total_loss": 0.7169199585914612
    },
    {
      "classification_loss": 0.6962001919746399,
      "epoch": 0.9180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 280,
      "total_loss": 0.6962001919746399
    },
    {
      "classification_loss": 0.6727451086044312,
      "epoch": 0.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 281,
      "total_loss": 0.6727451086044312
    },
    {
      "classification_loss": 0.6463252305984497,
      "epoch": 0.9245901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 282,
      "total_loss": 0.6463252305984497
    },
    {
      "classification_loss": 0.6885053515434265,
      "epoch": 0.9278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 283,
      "total_loss": 0.6885053515434265
    },
    {
      "classification_loss": 0.695947527885437,
      "epoch": 0.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 284,
      "total_loss": 0.695947527885437
    },
    {
      "classification_loss": 0.6800493001937866,
      "epoch": 0.9344262295081968,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 285,
      "total_loss": 0.6800493001937866
    },
    {
      "classification_loss": 0.7095627784729004,
      "epoch": 0.9377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 286,
      "total_loss": 0.7095627784729004
    },
    {
      "classification_loss": 0.6669729948043823,
      "epoch": 0.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 287,
      "total_loss": 0.6669729948043823
    },
    {
      "classification_loss": 0.6740326285362244,
      "epoch": 0.9442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 288,
      "total_loss": 0.6740326285362244
    },
    {
      "classification_loss": 0.685367226600647,
      "epoch": 0.9475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 289,
      "total_loss": 0.685367226600647
    },
    {
      "classification_loss": 0.6946611404418945,
      "epoch": 0.9508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 290,
      "total_loss": 0.6946611404418945
    },
    {
      "classification_loss": 0.6969387531280518,
      "epoch": 0.9540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 291,
      "total_loss": 0.6969387531280518
    },
    {
      "classification_loss": 0.6880154609680176,
      "epoch": 0.9573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 292,
      "total_loss": 0.6880154609680176
    },
    {
      "classification_loss": 0.6588075757026672,
      "epoch": 0.9606557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 293,
      "total_loss": 0.6588075757026672
    },
    {
      "classification_loss": 0.7109318375587463,
      "epoch": 0.9639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 294,
      "total_loss": 0.7109318375587463
    },
    {
      "classification_loss": 0.7191862463951111,
      "epoch": 0.9672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 295,
      "total_loss": 0.7191862463951111
    },
    {
      "classification_loss": 0.691542387008667,
      "epoch": 0.9704918032786886,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 296,
      "total_loss": 0.691542387008667
    },
    {
      "classification_loss": 0.6856371760368347,
      "epoch": 0.9737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 297,
      "total_loss": 0.6856371760368347
    },
    {
      "classification_loss": 0.6834441423416138,
      "epoch": 0.9770491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 298,
      "total_loss": 0.6834441423416138
    },
    {
      "classification_loss": 0.6589464545249939,
      "epoch": 0.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 299,
      "total_loss": 0.6589464545249939
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 0.7335357069969177,
      "learning_rate": 0.0001933666666666667,
      "loss": 0.6927,
      "step": 300
    },
    {
      "classification_loss": 0.7044894695281982,
      "epoch": 0.9836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 300,
      "total_loss": 0.7044894695281982
    },
    {
      "classification_loss": 0.6442395448684692,
      "epoch": 0.9868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 301,
      "total_loss": 0.6442395448684692
    },
    {
      "classification_loss": 0.6622635722160339,
      "epoch": 0.9901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 302,
      "total_loss": 0.6622635722160339
    },
    {
      "classification_loss": 0.6949057579040527,
      "epoch": 0.9934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 303,
      "total_loss": 0.6949057579040527
    },
    {
      "classification_loss": 0.6714540719985962,
      "epoch": 0.9967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 304,
      "total_loss": 0.6714540719985962
    },
    {
      "classification_loss": 0.7220368385314941,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.7220368385314941
    },
    {
      "classification_loss": 0.7181529998779297,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.7181529998779297
    },
    {
      "classification_loss": 0.7117705941200256,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.7117705941200256
    },
    {
      "classification_loss": 0.716465175151825,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.716465175151825
    },
    {
      "classification_loss": 0.706150233745575,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.706150233745575
    },
    {
      "classification_loss": 0.6986932158470154,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.6986932158470154
    },
    {
      "classification_loss": 0.7241065502166748,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.7241065502166748
    },
    {
      "classification_loss": 0.7195279598236084,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.7195279598236084
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.43,
      "eval_f1": 0.29455445544554454,
      "eval_loss": 0.7144950032234192,
      "eval_precision": 0.6197916666666666,
      "eval_recall": 0.19318181818181818,
      "eval_runtime": 5.999,
      "eval_samples_per_second": 166.695,
      "eval_steps_per_second": 1.334,
      "step": 305
    },
    {
      "classification_loss": 0.6848212480545044,
      "epoch": 1.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 305,
      "total_loss": 0.6848212480545044
    },
    {
      "classification_loss": 0.6966485381126404,
      "epoch": 1.0032786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 306,
      "total_loss": 0.6966485381126404
    },
    {
      "classification_loss": 0.6982526779174805,
      "epoch": 1.0065573770491802,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 307,
      "total_loss": 0.6982526779174805
    },
    {
      "classification_loss": 0.6771653890609741,
      "epoch": 1.0098360655737706,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 308,
      "total_loss": 0.6771653890609741
    },
    {
      "classification_loss": 0.7104071974754333,
      "epoch": 1.0131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 309,
      "total_loss": 0.7104071974754333
    },
    {
      "classification_loss": 0.6861061453819275,
      "epoch": 1.0163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 310,
      "total_loss": 0.6861061453819275
    },
    {
      "classification_loss": 0.6968622207641602,
      "epoch": 1.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 311,
      "total_loss": 0.6968622207641602
    },
    {
      "classification_loss": 0.6723414063453674,
      "epoch": 1.022950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 312,
      "total_loss": 0.6723414063453674
    },
    {
      "classification_loss": 0.6801667809486389,
      "epoch": 1.0262295081967212,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 313,
      "total_loss": 0.6801667809486389
    },
    {
      "classification_loss": 0.6736375093460083,
      "epoch": 1.0295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 314,
      "total_loss": 0.6736375093460083
    },
    {
      "classification_loss": 0.6726878881454468,
      "epoch": 1.0327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 315,
      "total_loss": 0.6726878881454468
    },
    {
      "classification_loss": 0.6938344240188599,
      "epoch": 1.0360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 316,
      "total_loss": 0.6938344240188599
    },
    {
      "classification_loss": 0.6581801772117615,
      "epoch": 1.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 317,
      "total_loss": 0.6581801772117615
    },
    {
      "classification_loss": 0.6751754879951477,
      "epoch": 1.042622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 318,
      "total_loss": 0.6751754879951477
    },
    {
      "classification_loss": 0.7201510071754456,
      "epoch": 1.0459016393442624,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 319,
      "total_loss": 0.7201510071754456
    },
    {
      "classification_loss": 0.6931667923927307,
      "epoch": 1.0491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 320,
      "total_loss": 0.6931667923927307
    },
    {
      "classification_loss": 0.6565389633178711,
      "epoch": 1.0524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 321,
      "total_loss": 0.6565389633178711
    },
    {
      "classification_loss": 0.6852599382400513,
      "epoch": 1.0557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 322,
      "total_loss": 0.6852599382400513
    },
    {
      "classification_loss": 0.6914754509925842,
      "epoch": 1.0590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 323,
      "total_loss": 0.6914754509925842
    },
    {
      "classification_loss": 0.6961082220077515,
      "epoch": 1.0622950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 324,
      "total_loss": 0.6961082220077515
    },
    {
      "classification_loss": 0.68470698595047,
      "epoch": 1.0655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 325,
      "total_loss": 0.68470698595047
    },
    {
      "classification_loss": 0.6738974452018738,
      "epoch": 1.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 326,
      "total_loss": 0.6738974452018738
    },
    {
      "classification_loss": 0.6581007242202759,
      "epoch": 1.0721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 327,
      "total_loss": 0.6581007242202759
    },
    {
      "classification_loss": 0.6673474311828613,
      "epoch": 1.0754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 328,
      "total_loss": 0.6673474311828613
    },
    {
      "classification_loss": 0.6496099233627319,
      "epoch": 1.0786885245901638,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 329,
      "total_loss": 0.6496099233627319
    },
    {
      "classification_loss": 0.6547068953514099,
      "epoch": 1.0819672131147542,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 330,
      "total_loss": 0.6547068953514099
    },
    {
      "classification_loss": 0.7050886750221252,
      "epoch": 1.0852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 331,
      "total_loss": 0.7050886750221252
    },
    {
      "classification_loss": 0.6974179148674011,
      "epoch": 1.0885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 332,
      "total_loss": 0.6974179148674011
    },
    {
      "classification_loss": 0.6830812692642212,
      "epoch": 1.0918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 333,
      "total_loss": 0.6830812692642212
    },
    {
      "classification_loss": 0.7118186950683594,
      "epoch": 1.0950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 334,
      "total_loss": 0.7118186950683594
    },
    {
      "classification_loss": 0.7309463620185852,
      "epoch": 1.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 335,
      "total_loss": 0.7309463620185852
    },
    {
      "classification_loss": 0.7098247408866882,
      "epoch": 1.1016393442622952,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 336,
      "total_loss": 0.7098247408866882
    },
    {
      "classification_loss": 0.6780053973197937,
      "epoch": 1.1049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 337,
      "total_loss": 0.6780053973197937
    },
    {
      "classification_loss": 0.6910977363586426,
      "epoch": 1.1081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 338,
      "total_loss": 0.6910977363586426
    },
    {
      "classification_loss": 0.6865598559379578,
      "epoch": 1.1114754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 339,
      "total_loss": 0.6865598559379578
    },
    {
      "classification_loss": 0.6198625564575195,
      "epoch": 1.1147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 340,
      "total_loss": 0.6198625564575195
    },
    {
      "classification_loss": 0.7204165458679199,
      "epoch": 1.118032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 341,
      "total_loss": 0.7204165458679199
    },
    {
      "classification_loss": 0.6816821694374084,
      "epoch": 1.1213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 342,
      "total_loss": 0.6816821694374084
    },
    {
      "classification_loss": 0.6702285408973694,
      "epoch": 1.1245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 343,
      "total_loss": 0.6702285408973694
    },
    {
      "classification_loss": 0.6479620933532715,
      "epoch": 1.1278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 344,
      "total_loss": 0.6479620933532715
    },
    {
      "classification_loss": 0.6663740277290344,
      "epoch": 1.1311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 345,
      "total_loss": 0.6663740277290344
    },
    {
      "classification_loss": 0.6634979844093323,
      "epoch": 1.1344262295081968,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 346,
      "total_loss": 0.6634979844093323
    },
    {
      "classification_loss": 0.6820067763328552,
      "epoch": 1.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 347,
      "total_loss": 0.6820067763328552
    },
    {
      "classification_loss": 0.685373842716217,
      "epoch": 1.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 348,
      "total_loss": 0.685373842716217
    },
    {
      "classification_loss": 0.6951534748077393,
      "epoch": 1.1442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 349,
      "total_loss": 0.6951534748077393
    },
    {
      "classification_loss": 0.6503526568412781,
      "epoch": 1.1475409836065573,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 350,
      "total_loss": 0.6503526568412781
    },
    {
      "classification_loss": 0.6928096413612366,
      "epoch": 1.1508196721311474,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 351,
      "total_loss": 0.6928096413612366
    },
    {
      "classification_loss": 0.6768551468849182,
      "epoch": 1.1540983606557378,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 352,
      "total_loss": 0.6768551468849182
    },
    {
      "classification_loss": 0.6428161859512329,
      "epoch": 1.157377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 353,
      "total_loss": 0.6428161859512329
    },
    {
      "classification_loss": 0.6681473851203918,
      "epoch": 1.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 354,
      "total_loss": 0.6681473851203918
    },
    {
      "classification_loss": 0.6482967734336853,
      "epoch": 1.1639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 355,
      "total_loss": 0.6482967734336853
    },
    {
      "classification_loss": 0.6705722808837891,
      "epoch": 1.1672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 356,
      "total_loss": 0.6705722808837891
    },
    {
      "classification_loss": 0.6407349705696106,
      "epoch": 1.1704918032786886,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 357,
      "total_loss": 0.6407349705696106
    },
    {
      "classification_loss": 0.7089855670928955,
      "epoch": 1.1737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 358,
      "total_loss": 0.7089855670928955
    },
    {
      "classification_loss": 0.6488339304924011,
      "epoch": 1.1770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 359,
      "total_loss": 0.6488339304924011
    },
    {
      "classification_loss": 0.6799866557121277,
      "epoch": 1.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 360,
      "total_loss": 0.6799866557121277
    },
    {
      "classification_loss": 0.7046429514884949,
      "epoch": 1.1836065573770491,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 361,
      "total_loss": 0.7046429514884949
    },
    {
      "classification_loss": 0.6905555129051208,
      "epoch": 1.1868852459016392,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 362,
      "total_loss": 0.6905555129051208
    },
    {
      "classification_loss": 0.6759887337684631,
      "epoch": 1.1901639344262296,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 363,
      "total_loss": 0.6759887337684631
    },
    {
      "classification_loss": 0.632932722568512,
      "epoch": 1.1934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 364,
      "total_loss": 0.632932722568512
    },
    {
      "classification_loss": 0.7288864850997925,
      "epoch": 1.1967213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 365,
      "total_loss": 0.7288864850997925
    },
    {
      "classification_loss": 0.7018261551856995,
      "epoch": 1.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 366,
      "total_loss": 0.7018261551856995
    },
    {
      "classification_loss": 0.6441947817802429,
      "epoch": 1.20327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 367,
      "total_loss": 0.6441947817802429
    },
    {
      "classification_loss": 0.7334960103034973,
      "epoch": 1.2065573770491804,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 368,
      "total_loss": 0.7334960103034973
    },
    {
      "classification_loss": 0.6877341866493225,
      "epoch": 1.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 369,
      "total_loss": 0.6877341866493225
    },
    {
      "classification_loss": 0.6665780544281006,
      "epoch": 1.2131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 370,
      "total_loss": 0.6665780544281006
    },
    {
      "classification_loss": 0.6543006300926208,
      "epoch": 1.2163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 371,
      "total_loss": 0.6543006300926208
    },
    {
      "classification_loss": 0.7008847594261169,
      "epoch": 1.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 372,
      "total_loss": 0.7008847594261169
    },
    {
      "classification_loss": 0.7062990069389343,
      "epoch": 1.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 373,
      "total_loss": 0.7062990069389343
    },
    {
      "classification_loss": 0.6467128396034241,
      "epoch": 1.2262295081967214,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 374,
      "total_loss": 0.6467128396034241
    },
    {
      "classification_loss": 0.6916633248329163,
      "epoch": 1.2295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 375,
      "total_loss": 0.6916633248329163
    },
    {
      "classification_loss": 0.6728904843330383,
      "epoch": 1.2327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 376,
      "total_loss": 0.6728904843330383
    },
    {
      "classification_loss": 0.6907960176467896,
      "epoch": 1.2360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 377,
      "total_loss": 0.6907960176467896
    },
    {
      "classification_loss": 0.6886368989944458,
      "epoch": 1.2393442622950819,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 378,
      "total_loss": 0.6886368989944458
    },
    {
      "classification_loss": 0.6569947004318237,
      "epoch": 1.2426229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 379,
      "total_loss": 0.6569947004318237
    },
    {
      "classification_loss": 0.6647143363952637,
      "epoch": 1.2459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 380,
      "total_loss": 0.6647143363952637
    },
    {
      "classification_loss": 0.6997794508934021,
      "epoch": 1.2491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 381,
      "total_loss": 0.6997794508934021
    },
    {
      "classification_loss": 0.6335008144378662,
      "epoch": 1.2524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 382,
      "total_loss": 0.6335008144378662
    },
    {
      "classification_loss": 0.6584832072257996,
      "epoch": 1.2557377049180327,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 383,
      "total_loss": 0.6584832072257996
    },
    {
      "classification_loss": 0.7369388341903687,
      "epoch": 1.2590163934426228,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 384,
      "total_loss": 0.7369388341903687
    },
    {
      "classification_loss": 0.6482481956481934,
      "epoch": 1.2622950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 385,
      "total_loss": 0.6482481956481934
    },
    {
      "classification_loss": 0.7202877402305603,
      "epoch": 1.2655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 386,
      "total_loss": 0.7202877402305603
    },
    {
      "classification_loss": 0.6177520751953125,
      "epoch": 1.2688524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 387,
      "total_loss": 0.6177520751953125
    },
    {
      "classification_loss": 0.6579982042312622,
      "epoch": 1.2721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 388,
      "total_loss": 0.6579982042312622
    },
    {
      "classification_loss": 0.6664286255836487,
      "epoch": 1.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 389,
      "total_loss": 0.6664286255836487
    },
    {
      "classification_loss": 0.7351288795471191,
      "epoch": 1.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 390,
      "total_loss": 0.7351288795471191
    },
    {
      "classification_loss": 0.674067497253418,
      "epoch": 1.2819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 391,
      "total_loss": 0.674067497253418
    },
    {
      "classification_loss": 0.6532196998596191,
      "epoch": 1.2852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 392,
      "total_loss": 0.6532196998596191
    },
    {
      "classification_loss": 0.6973771452903748,
      "epoch": 1.2885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 393,
      "total_loss": 0.6973771452903748
    },
    {
      "classification_loss": 0.6737073063850403,
      "epoch": 1.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 394,
      "total_loss": 0.6737073063850403
    },
    {
      "classification_loss": 0.6934196352958679,
      "epoch": 1.2950819672131146,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 395,
      "total_loss": 0.6934196352958679
    },
    {
      "classification_loss": 0.7097926139831543,
      "epoch": 1.298360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 396,
      "total_loss": 0.7097926139831543
    },
    {
      "classification_loss": 0.6944233179092407,
      "epoch": 1.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 397,
      "total_loss": 0.6944233179092407
    },
    {
      "classification_loss": 0.7158463597297668,
      "epoch": 1.3049180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 398,
      "total_loss": 0.7158463597297668
    },
    {
      "classification_loss": 0.7102077603340149,
      "epoch": 1.3081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 399,
      "total_loss": 0.7102077603340149
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 4.7904839515686035,
      "learning_rate": 0.00019003333333333336,
      "loss": 0.6808,
      "step": 400
    },
    {
      "classification_loss": 0.6769688725471497,
      "epoch": 1.3114754098360657,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 400,
      "total_loss": 0.6769688725471497
    },
    {
      "classification_loss": 0.7169974446296692,
      "epoch": 1.3147540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 401,
      "total_loss": 0.7169974446296692
    },
    {
      "classification_loss": 0.6981260180473328,
      "epoch": 1.318032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 402,
      "total_loss": 0.6981260180473328
    },
    {
      "classification_loss": 0.6863323450088501,
      "epoch": 1.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 403,
      "total_loss": 0.6863323450088501
    },
    {
      "classification_loss": 0.6627207398414612,
      "epoch": 1.3245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 404,
      "total_loss": 0.6627207398414612
    },
    {
      "classification_loss": 0.7193465232849121,
      "epoch": 1.3278688524590163,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 405,
      "total_loss": 0.7193465232849121
    },
    {
      "classification_loss": 0.6781594157218933,
      "epoch": 1.3311475409836064,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 406,
      "total_loss": 0.6781594157218933
    },
    {
      "classification_loss": 0.6865848898887634,
      "epoch": 1.3344262295081968,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 407,
      "total_loss": 0.6865848898887634
    },
    {
      "classification_loss": 0.656986653804779,
      "epoch": 1.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 408,
      "total_loss": 0.656986653804779
    },
    {
      "classification_loss": 0.6695076823234558,
      "epoch": 1.340983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 409,
      "total_loss": 0.6695076823234558
    },
    {
      "classification_loss": 0.6722299456596375,
      "epoch": 1.3442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 410,
      "total_loss": 0.6722299456596375
    },
    {
      "classification_loss": 0.6868859529495239,
      "epoch": 1.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 411,
      "total_loss": 0.6868859529495239
    },
    {
      "classification_loss": 0.6813832521438599,
      "epoch": 1.3508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 412,
      "total_loss": 0.6813832521438599
    },
    {
      "classification_loss": 0.6951927542686462,
      "epoch": 1.3540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 413,
      "total_loss": 0.6951927542686462
    },
    {
      "classification_loss": 0.6991255879402161,
      "epoch": 1.3573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 414,
      "total_loss": 0.6991255879402161
    },
    {
      "classification_loss": 0.6326900124549866,
      "epoch": 1.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 415,
      "total_loss": 0.6326900124549866
    },
    {
      "classification_loss": 0.6811577081680298,
      "epoch": 1.3639344262295081,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 416,
      "total_loss": 0.6811577081680298
    },
    {
      "classification_loss": 0.63130784034729,
      "epoch": 1.3672131147540982,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 417,
      "total_loss": 0.63130784034729
    },
    {
      "classification_loss": 0.6839938163757324,
      "epoch": 1.3704918032786886,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 418,
      "total_loss": 0.6839938163757324
    },
    {
      "classification_loss": 0.6608614325523376,
      "epoch": 1.3737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 419,
      "total_loss": 0.6608614325523376
    },
    {
      "classification_loss": 0.6734241247177124,
      "epoch": 1.3770491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 420,
      "total_loss": 0.6734241247177124
    },
    {
      "classification_loss": 0.6762297749519348,
      "epoch": 1.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 421,
      "total_loss": 0.6762297749519348
    },
    {
      "classification_loss": 0.633268415927887,
      "epoch": 1.3836065573770493,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 422,
      "total_loss": 0.633268415927887
    },
    {
      "classification_loss": 0.6914992928504944,
      "epoch": 1.3868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 423,
      "total_loss": 0.6914992928504944
    },
    {
      "classification_loss": 0.6836352348327637,
      "epoch": 1.3901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 424,
      "total_loss": 0.6836352348327637
    },
    {
      "classification_loss": 0.6781685948371887,
      "epoch": 1.3934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 425,
      "total_loss": 0.6781685948371887
    },
    {
      "classification_loss": 0.6553655862808228,
      "epoch": 1.3967213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 426,
      "total_loss": 0.6553655862808228
    },
    {
      "classification_loss": 0.6339852809906006,
      "epoch": 1.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 427,
      "total_loss": 0.6339852809906006
    },
    {
      "classification_loss": 0.6871461868286133,
      "epoch": 1.40327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 428,
      "total_loss": 0.6871461868286133
    },
    {
      "classification_loss": 0.6575843691825867,
      "epoch": 1.4065573770491804,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 429,
      "total_loss": 0.6575843691825867
    },
    {
      "classification_loss": 0.6591646075248718,
      "epoch": 1.4098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 430,
      "total_loss": 0.6591646075248718
    },
    {
      "classification_loss": 0.6860789656639099,
      "epoch": 1.4131147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 431,
      "total_loss": 0.6860789656639099
    },
    {
      "classification_loss": 0.6611535549163818,
      "epoch": 1.4163934426229507,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 432,
      "total_loss": 0.6611535549163818
    },
    {
      "classification_loss": 0.685343325138092,
      "epoch": 1.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 433,
      "total_loss": 0.685343325138092
    },
    {
      "classification_loss": 0.657705545425415,
      "epoch": 1.4229508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 434,
      "total_loss": 0.657705545425415
    },
    {
      "classification_loss": 0.7025576233863831,
      "epoch": 1.4262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 435,
      "total_loss": 0.7025576233863831
    },
    {
      "classification_loss": 0.6381174921989441,
      "epoch": 1.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 436,
      "total_loss": 0.6381174921989441
    },
    {
      "classification_loss": 0.6740797758102417,
      "epoch": 1.4327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 437,
      "total_loss": 0.6740797758102417
    },
    {
      "classification_loss": 0.7165680527687073,
      "epoch": 1.4360655737704917,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 438,
      "total_loss": 0.7165680527687073
    },
    {
      "classification_loss": 0.6642221212387085,
      "epoch": 1.4393442622950818,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 439,
      "total_loss": 0.6642221212387085
    },
    {
      "classification_loss": 0.6284206509590149,
      "epoch": 1.4426229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 440,
      "total_loss": 0.6284206509590149
    },
    {
      "classification_loss": 0.7223174571990967,
      "epoch": 1.4459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 441,
      "total_loss": 0.7223174571990967
    },
    {
      "classification_loss": 0.6528487205505371,
      "epoch": 1.4491803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 442,
      "total_loss": 0.6528487205505371
    },
    {
      "classification_loss": 0.6593498587608337,
      "epoch": 1.4524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 443,
      "total_loss": 0.6593498587608337
    },
    {
      "classification_loss": 0.7194620966911316,
      "epoch": 1.455737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 444,
      "total_loss": 0.7194620966911316
    },
    {
      "classification_loss": 0.6397531032562256,
      "epoch": 1.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 445,
      "total_loss": 0.6397531032562256
    },
    {
      "classification_loss": 0.6968640685081482,
      "epoch": 1.4622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 446,
      "total_loss": 0.6968640685081482
    },
    {
      "classification_loss": 0.6712772250175476,
      "epoch": 1.4655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 447,
      "total_loss": 0.6712772250175476
    },
    {
      "classification_loss": 0.6631370782852173,
      "epoch": 1.4688524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 448,
      "total_loss": 0.6631370782852173
    },
    {
      "classification_loss": 0.7208870053291321,
      "epoch": 1.4721311475409835,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 449,
      "total_loss": 0.7208870053291321
    },
    {
      "classification_loss": 0.6590541005134583,
      "epoch": 1.4754098360655736,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 450,
      "total_loss": 0.6590541005134583
    },
    {
      "classification_loss": 0.6917712688446045,
      "epoch": 1.478688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 451,
      "total_loss": 0.6917712688446045
    },
    {
      "classification_loss": 0.6612716317176819,
      "epoch": 1.481967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 452,
      "total_loss": 0.6612716317176819
    },
    {
      "classification_loss": 0.6466686725616455,
      "epoch": 1.4852459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 453,
      "total_loss": 0.6466686725616455
    },
    {
      "classification_loss": 0.7037209868431091,
      "epoch": 1.4885245901639343,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 454,
      "total_loss": 0.7037209868431091
    },
    {
      "classification_loss": 0.6972444653511047,
      "epoch": 1.4918032786885247,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 455,
      "total_loss": 0.6972444653511047
    },
    {
      "classification_loss": 0.6700567603111267,
      "epoch": 1.4950819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 456,
      "total_loss": 0.6700567603111267
    },
    {
      "classification_loss": 0.6633701324462891,
      "epoch": 1.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 457,
      "total_loss": 0.6633701324462891
    },
    {
      "classification_loss": 0.6491206884384155,
      "epoch": 1.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 458,
      "total_loss": 0.6491206884384155
    },
    {
      "classification_loss": 0.6789048910140991,
      "epoch": 1.5049180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 459,
      "total_loss": 0.6789048910140991
    },
    {
      "classification_loss": 0.6670783758163452,
      "epoch": 1.5081967213114753,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 460,
      "total_loss": 0.6670783758163452
    },
    {
      "classification_loss": 0.6931024193763733,
      "epoch": 1.5114754098360654,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 461,
      "total_loss": 0.6931024193763733
    },
    {
      "classification_loss": 0.6490392088890076,
      "epoch": 1.5147540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 462,
      "total_loss": 0.6490392088890076
    },
    {
      "classification_loss": 0.6664279699325562,
      "epoch": 1.518032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 463,
      "total_loss": 0.6664279699325562
    },
    {
      "classification_loss": 0.6806185245513916,
      "epoch": 1.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 464,
      "total_loss": 0.6806185245513916
    },
    {
      "classification_loss": 0.664716362953186,
      "epoch": 1.5245901639344264,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 465,
      "total_loss": 0.664716362953186
    },
    {
      "classification_loss": 0.698867678642273,
      "epoch": 1.5278688524590165,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 466,
      "total_loss": 0.698867678642273
    },
    {
      "classification_loss": 0.6802564859390259,
      "epoch": 1.5311475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 467,
      "total_loss": 0.6802564859390259
    },
    {
      "classification_loss": 0.6575250029563904,
      "epoch": 1.5344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 468,
      "total_loss": 0.6575250029563904
    },
    {
      "classification_loss": 0.6499452590942383,
      "epoch": 1.5377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 469,
      "total_loss": 0.6499452590942383
    },
    {
      "classification_loss": 0.6493350863456726,
      "epoch": 1.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 470,
      "total_loss": 0.6493350863456726
    },
    {
      "classification_loss": 0.6850734353065491,
      "epoch": 1.544262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 471,
      "total_loss": 0.6850734353065491
    },
    {
      "classification_loss": 0.685588538646698,
      "epoch": 1.5475409836065572,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 472,
      "total_loss": 0.685588538646698
    },
    {
      "classification_loss": 0.6741653680801392,
      "epoch": 1.5508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 473,
      "total_loss": 0.6741653680801392
    },
    {
      "classification_loss": 0.6693263649940491,
      "epoch": 1.5540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 474,
      "total_loss": 0.6693263649940491
    },
    {
      "classification_loss": 0.6361807584762573,
      "epoch": 1.5573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 475,
      "total_loss": 0.6361807584762573
    },
    {
      "classification_loss": 0.6861531734466553,
      "epoch": 1.5606557377049182,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 476,
      "total_loss": 0.6861531734466553
    },
    {
      "classification_loss": 0.6727224588394165,
      "epoch": 1.5639344262295083,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 477,
      "total_loss": 0.6727224588394165
    },
    {
      "classification_loss": 0.6791217923164368,
      "epoch": 1.5672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 478,
      "total_loss": 0.6791217923164368
    },
    {
      "classification_loss": 0.6783818006515503,
      "epoch": 1.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 479,
      "total_loss": 0.6783818006515503
    },
    {
      "classification_loss": 0.6727819442749023,
      "epoch": 1.5737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 480,
      "total_loss": 0.6727819442749023
    },
    {
      "classification_loss": 0.6507512927055359,
      "epoch": 1.5770491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 481,
      "total_loss": 0.6507512927055359
    },
    {
      "classification_loss": 0.6598182320594788,
      "epoch": 1.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 482,
      "total_loss": 0.6598182320594788
    },
    {
      "classification_loss": 0.7080186605453491,
      "epoch": 1.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 483,
      "total_loss": 0.7080186605453491
    },
    {
      "classification_loss": 0.6588483452796936,
      "epoch": 1.5868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 484,
      "total_loss": 0.6588483452796936
    },
    {
      "classification_loss": 0.6870830059051514,
      "epoch": 1.5901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 485,
      "total_loss": 0.6870830059051514
    },
    {
      "classification_loss": 0.6846591234207153,
      "epoch": 1.5934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 486,
      "total_loss": 0.6846591234207153
    },
    {
      "classification_loss": 0.6748862862586975,
      "epoch": 1.59672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 487,
      "total_loss": 0.6748862862586975
    },
    {
      "classification_loss": 0.7161657810211182,
      "epoch": 1.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 488,
      "total_loss": 0.7161657810211182
    },
    {
      "classification_loss": 0.6010173559188843,
      "epoch": 1.6032786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 489,
      "total_loss": 0.6010173559188843
    },
    {
      "classification_loss": 0.7074069380760193,
      "epoch": 1.6065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 490,
      "total_loss": 0.7074069380760193
    },
    {
      "classification_loss": 0.6102820634841919,
      "epoch": 1.6098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 491,
      "total_loss": 0.6102820634841919
    },
    {
      "classification_loss": 0.6559180021286011,
      "epoch": 1.6131147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 492,
      "total_loss": 0.6559180021286011
    },
    {
      "classification_loss": 0.6920071244239807,
      "epoch": 1.6163934426229507,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 493,
      "total_loss": 0.6920071244239807
    },
    {
      "classification_loss": 0.7153034210205078,
      "epoch": 1.6196721311475408,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 494,
      "total_loss": 0.7153034210205078
    },
    {
      "classification_loss": 0.6373955011367798,
      "epoch": 1.6229508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 495,
      "total_loss": 0.6373955011367798
    },
    {
      "classification_loss": 0.7230664491653442,
      "epoch": 1.6262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 496,
      "total_loss": 0.7230664491653442
    },
    {
      "classification_loss": 0.6953338384628296,
      "epoch": 1.6295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 497,
      "total_loss": 0.6953338384628296
    },
    {
      "classification_loss": 0.7308003902435303,
      "epoch": 1.6327868852459018,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 498,
      "total_loss": 0.7308003902435303
    },
    {
      "classification_loss": 0.6706781983375549,
      "epoch": 1.6360655737704919,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 499,
      "total_loss": 0.6706781983375549
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 4.780510902404785,
      "learning_rate": 0.0001867,
      "loss": 0.6746,
      "step": 500
    },
    {
      "classification_loss": 0.6798039078712463,
      "epoch": 1.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 500,
      "total_loss": 0.6798039078712463
    },
    {
      "classification_loss": 0.6738409996032715,
      "epoch": 1.6426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 501,
      "total_loss": 0.6738409996032715
    },
    {
      "classification_loss": 0.6516898274421692,
      "epoch": 1.6459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 502,
      "total_loss": 0.6516898274421692
    },
    {
      "classification_loss": 0.6703016757965088,
      "epoch": 1.6491803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 503,
      "total_loss": 0.6703016757965088
    },
    {
      "classification_loss": 0.6703382134437561,
      "epoch": 1.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 504,
      "total_loss": 0.6703382134437561
    },
    {
      "classification_loss": 0.663578987121582,
      "epoch": 1.6557377049180326,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 505,
      "total_loss": 0.663578987121582
    },
    {
      "classification_loss": 0.6078974008560181,
      "epoch": 1.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 506,
      "total_loss": 0.6078974008560181
    },
    {
      "classification_loss": 0.7326939702033997,
      "epoch": 1.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 507,
      "total_loss": 0.7326939702033997
    },
    {
      "classification_loss": 0.662992537021637,
      "epoch": 1.6655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 508,
      "total_loss": 0.662992537021637
    },
    {
      "classification_loss": 0.681625247001648,
      "epoch": 1.6688524590163936,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 509,
      "total_loss": 0.681625247001648
    },
    {
      "classification_loss": 0.7295600771903992,
      "epoch": 1.6721311475409837,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 510,
      "total_loss": 0.7295600771903992
    },
    {
      "classification_loss": 0.6505188345909119,
      "epoch": 1.6754098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 511,
      "total_loss": 0.6505188345909119
    },
    {
      "classification_loss": 0.6835789084434509,
      "epoch": 1.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 512,
      "total_loss": 0.6835789084434509
    },
    {
      "classification_loss": 0.6999694108963013,
      "epoch": 1.681967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 513,
      "total_loss": 0.6999694108963013
    },
    {
      "classification_loss": 0.702979326248169,
      "epoch": 1.6852459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 514,
      "total_loss": 0.702979326248169
    },
    {
      "classification_loss": 0.6814655065536499,
      "epoch": 1.6885245901639343,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 515,
      "total_loss": 0.6814655065536499
    },
    {
      "classification_loss": 0.6421952247619629,
      "epoch": 1.6918032786885244,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 516,
      "total_loss": 0.6421952247619629
    },
    {
      "classification_loss": 0.6581999659538269,
      "epoch": 1.6950819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 517,
      "total_loss": 0.6581999659538269
    },
    {
      "classification_loss": 0.6725009679794312,
      "epoch": 1.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 518,
      "total_loss": 0.6725009679794312
    },
    {
      "classification_loss": 0.6322433948516846,
      "epoch": 1.7016393442622952,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 519,
      "total_loss": 0.6322433948516846
    },
    {
      "classification_loss": 0.6700465679168701,
      "epoch": 1.7049180327868854,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 520,
      "total_loss": 0.6700465679168701
    },
    {
      "classification_loss": 0.6905934810638428,
      "epoch": 1.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 521,
      "total_loss": 0.6905934810638428
    },
    {
      "classification_loss": 0.7540953159332275,
      "epoch": 1.7114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 522,
      "total_loss": 0.7540953159332275
    },
    {
      "classification_loss": 0.6420707106590271,
      "epoch": 1.7147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 523,
      "total_loss": 0.6420707106590271
    },
    {
      "classification_loss": 0.5714407563209534,
      "epoch": 1.7180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 524,
      "total_loss": 0.5714407563209534
    },
    {
      "classification_loss": 0.7148767709732056,
      "epoch": 1.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 525,
      "total_loss": 0.7148767709732056
    },
    {
      "classification_loss": 0.6644479632377625,
      "epoch": 1.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 526,
      "total_loss": 0.6644479632377625
    },
    {
      "classification_loss": 0.7138468623161316,
      "epoch": 1.7278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 527,
      "total_loss": 0.7138468623161316
    },
    {
      "classification_loss": 0.6345535516738892,
      "epoch": 1.7311475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 528,
      "total_loss": 0.6345535516738892
    },
    {
      "classification_loss": 0.6038643717765808,
      "epoch": 1.7344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 529,
      "total_loss": 0.6038643717765808
    },
    {
      "classification_loss": 0.6814008951187134,
      "epoch": 1.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 530,
      "total_loss": 0.6814008951187134
    },
    {
      "classification_loss": 0.6362941265106201,
      "epoch": 1.7409836065573772,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 531,
      "total_loss": 0.6362941265106201
    },
    {
      "classification_loss": 0.7082619071006775,
      "epoch": 1.7442622950819673,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 532,
      "total_loss": 0.7082619071006775
    },
    {
      "classification_loss": 0.6938539743423462,
      "epoch": 1.7475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 533,
      "total_loss": 0.6938539743423462
    },
    {
      "classification_loss": 0.6531975865364075,
      "epoch": 1.7508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 534,
      "total_loss": 0.6531975865364075
    },
    {
      "classification_loss": 0.6834739446640015,
      "epoch": 1.7540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 535,
      "total_loss": 0.6834739446640015
    },
    {
      "classification_loss": 0.6975390315055847,
      "epoch": 1.7573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 536,
      "total_loss": 0.6975390315055847
    },
    {
      "classification_loss": 0.6900414228439331,
      "epoch": 1.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 537,
      "total_loss": 0.6900414228439331
    },
    {
      "classification_loss": 0.6336277723312378,
      "epoch": 1.7639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 538,
      "total_loss": 0.6336277723312378
    },
    {
      "classification_loss": 0.7405732274055481,
      "epoch": 1.7672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 539,
      "total_loss": 0.7405732274055481
    },
    {
      "classification_loss": 0.6851773262023926,
      "epoch": 1.7704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 540,
      "total_loss": 0.6851773262023926
    },
    {
      "classification_loss": 0.7003178596496582,
      "epoch": 1.7737704918032788,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 541,
      "total_loss": 0.7003178596496582
    },
    {
      "classification_loss": 0.6997108459472656,
      "epoch": 1.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 542,
      "total_loss": 0.6997108459472656
    },
    {
      "classification_loss": 0.6415879726409912,
      "epoch": 1.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 543,
      "total_loss": 0.6415879726409912
    },
    {
      "classification_loss": 0.6660112738609314,
      "epoch": 1.7836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 544,
      "total_loss": 0.6660112738609314
    },
    {
      "classification_loss": 0.6419232487678528,
      "epoch": 1.7868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 545,
      "total_loss": 0.6419232487678528
    },
    {
      "classification_loss": 0.6708616614341736,
      "epoch": 1.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 546,
      "total_loss": 0.6708616614341736
    },
    {
      "classification_loss": 0.6144054532051086,
      "epoch": 1.7934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 547,
      "total_loss": 0.6144054532051086
    },
    {
      "classification_loss": 0.6643661856651306,
      "epoch": 1.7967213114754097,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 548,
      "total_loss": 0.6643661856651306
    },
    {
      "classification_loss": 0.6591456532478333,
      "epoch": 1.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 549,
      "total_loss": 0.6591456532478333
    },
    {
      "classification_loss": 0.6356680393218994,
      "epoch": 1.8032786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 550,
      "total_loss": 0.6356680393218994
    },
    {
      "classification_loss": 0.6776261329650879,
      "epoch": 1.8065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 551,
      "total_loss": 0.6776261329650879
    },
    {
      "classification_loss": 0.665436863899231,
      "epoch": 1.8098360655737706,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 552,
      "total_loss": 0.665436863899231
    },
    {
      "classification_loss": 0.6696509718894958,
      "epoch": 1.8131147540983608,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 553,
      "total_loss": 0.6696509718894958
    },
    {
      "classification_loss": 0.7067769765853882,
      "epoch": 1.8163934426229509,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 554,
      "total_loss": 0.7067769765853882
    },
    {
      "classification_loss": 0.6365448236465454,
      "epoch": 1.819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 555,
      "total_loss": 0.6365448236465454
    },
    {
      "classification_loss": 0.670056939125061,
      "epoch": 1.8229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 556,
      "total_loss": 0.670056939125061
    },
    {
      "classification_loss": 0.6689658761024475,
      "epoch": 1.8262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 557,
      "total_loss": 0.6689658761024475
    },
    {
      "classification_loss": 0.6404631733894348,
      "epoch": 1.8295081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 558,
      "total_loss": 0.6404631733894348
    },
    {
      "classification_loss": 0.6449511647224426,
      "epoch": 1.8327868852459015,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 559,
      "total_loss": 0.6449511647224426
    },
    {
      "classification_loss": 0.6766822338104248,
      "epoch": 1.8360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 560,
      "total_loss": 0.6766822338104248
    },
    {
      "classification_loss": 0.6835570931434631,
      "epoch": 1.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 561,
      "total_loss": 0.6835570931434631
    },
    {
      "classification_loss": 0.7257384657859802,
      "epoch": 1.842622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 562,
      "total_loss": 0.7257384657859802
    },
    {
      "classification_loss": 0.6392423510551453,
      "epoch": 1.8459016393442624,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 563,
      "total_loss": 0.6392423510551453
    },
    {
      "classification_loss": 0.632548987865448,
      "epoch": 1.8491803278688526,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 564,
      "total_loss": 0.632548987865448
    },
    {
      "classification_loss": 0.6389024257659912,
      "epoch": 1.8524590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 565,
      "total_loss": 0.6389024257659912
    },
    {
      "classification_loss": 0.6338834166526794,
      "epoch": 1.8557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 566,
      "total_loss": 0.6338834166526794
    },
    {
      "classification_loss": 0.6967529058456421,
      "epoch": 1.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 567,
      "total_loss": 0.6967529058456421
    },
    {
      "classification_loss": 0.6266933679580688,
      "epoch": 1.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 568,
      "total_loss": 0.6266933679580688
    },
    {
      "classification_loss": 0.6823803186416626,
      "epoch": 1.8655737704918032,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 569,
      "total_loss": 0.6823803186416626
    },
    {
      "classification_loss": 0.6547815203666687,
      "epoch": 1.8688524590163933,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 570,
      "total_loss": 0.6547815203666687
    },
    {
      "classification_loss": 0.7114624977111816,
      "epoch": 1.8721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 571,
      "total_loss": 0.7114624977111816
    },
    {
      "classification_loss": 0.6536729335784912,
      "epoch": 1.8754098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 572,
      "total_loss": 0.6536729335784912
    },
    {
      "classification_loss": 0.6964762210845947,
      "epoch": 1.8786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 573,
      "total_loss": 0.6964762210845947
    },
    {
      "classification_loss": 0.6321980357170105,
      "epoch": 1.8819672131147542,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 574,
      "total_loss": 0.6321980357170105
    },
    {
      "classification_loss": 0.6329523921012878,
      "epoch": 1.8852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 575,
      "total_loss": 0.6329523921012878
    },
    {
      "classification_loss": 0.6911779642105103,
      "epoch": 1.8885245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 576,
      "total_loss": 0.6911779642105103
    },
    {
      "classification_loss": 0.7042400240898132,
      "epoch": 1.8918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 577,
      "total_loss": 0.7042400240898132
    },
    {
      "classification_loss": 0.620142936706543,
      "epoch": 1.8950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 578,
      "total_loss": 0.620142936706543
    },
    {
      "classification_loss": 0.6199340224266052,
      "epoch": 1.8983606557377048,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 579,
      "total_loss": 0.6199340224266052
    },
    {
      "classification_loss": 0.6962715983390808,
      "epoch": 1.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 580,
      "total_loss": 0.6962715983390808
    },
    {
      "classification_loss": 0.6757364273071289,
      "epoch": 1.904918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 581,
      "total_loss": 0.6757364273071289
    },
    {
      "classification_loss": 0.6484354734420776,
      "epoch": 1.9081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 582,
      "total_loss": 0.6484354734420776
    },
    {
      "classification_loss": 0.6717698574066162,
      "epoch": 1.9114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 583,
      "total_loss": 0.6717698574066162
    },
    {
      "classification_loss": 0.6871111989021301,
      "epoch": 1.9147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 584,
      "total_loss": 0.6871111989021301
    },
    {
      "classification_loss": 0.6578736901283264,
      "epoch": 1.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 585,
      "total_loss": 0.6578736901283264
    },
    {
      "classification_loss": 0.6707973480224609,
      "epoch": 1.9213114754098362,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 586,
      "total_loss": 0.6707973480224609
    },
    {
      "classification_loss": 0.6894813179969788,
      "epoch": 1.9245901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 587,
      "total_loss": 0.6894813179969788
    },
    {
      "classification_loss": 0.6331295371055603,
      "epoch": 1.9278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 588,
      "total_loss": 0.6331295371055603
    },
    {
      "classification_loss": 0.6556650996208191,
      "epoch": 1.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 589,
      "total_loss": 0.6556650996208191
    },
    {
      "classification_loss": 0.6872742772102356,
      "epoch": 1.9344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 590,
      "total_loss": 0.6872742772102356
    },
    {
      "classification_loss": 0.6499640345573425,
      "epoch": 1.9377049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 591,
      "total_loss": 0.6499640345573425
    },
    {
      "classification_loss": 0.613507091999054,
      "epoch": 1.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 592,
      "total_loss": 0.613507091999054
    },
    {
      "classification_loss": 0.6725738048553467,
      "epoch": 1.9442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 593,
      "total_loss": 0.6725738048553467
    },
    {
      "classification_loss": 0.7270599603652954,
      "epoch": 1.9475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 594,
      "total_loss": 0.7270599603652954
    },
    {
      "classification_loss": 0.6765421032905579,
      "epoch": 1.9508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 595,
      "total_loss": 0.6765421032905579
    },
    {
      "classification_loss": 0.6274402737617493,
      "epoch": 1.9540983606557378,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 596,
      "total_loss": 0.6274402737617493
    },
    {
      "classification_loss": 0.6870592832565308,
      "epoch": 1.957377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 597,
      "total_loss": 0.6870592832565308
    },
    {
      "classification_loss": 0.671206533908844,
      "epoch": 1.960655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 598,
      "total_loss": 0.671206533908844
    },
    {
      "classification_loss": 0.6408132314682007,
      "epoch": 1.9639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 599,
      "total_loss": 0.6408132314682007
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 2.5866174697875977,
      "learning_rate": 0.00018336666666666666,
      "loss": 0.6678,
      "step": 600
    },
    {
      "classification_loss": 0.6467112898826599,
      "epoch": 1.9672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 600,
      "total_loss": 0.6467112898826599
    },
    {
      "classification_loss": 0.6963127255439758,
      "epoch": 1.9704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 601,
      "total_loss": 0.6963127255439758
    },
    {
      "classification_loss": 0.6747451424598694,
      "epoch": 1.9737704918032786,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 602,
      "total_loss": 0.6747451424598694
    },
    {
      "classification_loss": 0.6750808358192444,
      "epoch": 1.9770491803278687,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 603,
      "total_loss": 0.6750808358192444
    },
    {
      "classification_loss": 0.6371378302574158,
      "epoch": 1.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 604,
      "total_loss": 0.6371378302574158
    },
    {
      "classification_loss": 0.6340317726135254,
      "epoch": 1.9836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 605,
      "total_loss": 0.6340317726135254
    },
    {
      "classification_loss": 0.6922329664230347,
      "epoch": 1.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 606,
      "total_loss": 0.6922329664230347
    },
    {
      "classification_loss": 0.6333170533180237,
      "epoch": 1.9901639344262296,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 607,
      "total_loss": 0.6333170533180237
    },
    {
      "classification_loss": 0.6631103754043579,
      "epoch": 1.9934426229508198,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 608,
      "total_loss": 0.6631103754043579
    },
    {
      "classification_loss": 0.7463656067848206,
      "epoch": 1.9967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 609,
      "total_loss": 0.7463656067848206
    },
    {
      "classification_loss": 0.7399024963378906,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7399024963378906
    },
    {
      "classification_loss": 0.7347451448440552,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7347451448440552
    },
    {
      "classification_loss": 0.7235320210456848,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7235320210456848
    },
    {
      "classification_loss": 0.7326233386993408,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7326233386993408
    },
    {
      "classification_loss": 0.735866129398346,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.735866129398346
    },
    {
      "classification_loss": 0.7231451272964478,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7231451272964478
    },
    {
      "classification_loss": 0.737723708152771,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.737723708152771
    },
    {
      "classification_loss": 0.7568022608757019,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.7568022608757019
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.401,
      "eval_f1": 0.07987711213517665,
      "eval_loss": 0.7350322604179382,
      "eval_precision": 0.7428571428571429,
      "eval_recall": 0.04220779220779221,
      "eval_runtime": 6.029,
      "eval_samples_per_second": 165.866,
      "eval_steps_per_second": 1.327,
      "step": 610
    },
    {
      "classification_loss": 0.6520331501960754,
      "epoch": 2.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 610,
      "total_loss": 0.6520331501960754
    },
    {
      "classification_loss": 0.6423784494400024,
      "epoch": 2.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 611,
      "total_loss": 0.6423784494400024
    },
    {
      "classification_loss": 0.6053074598312378,
      "epoch": 2.0065573770491802,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 612,
      "total_loss": 0.6053074598312378
    },
    {
      "classification_loss": 0.5889111161231995,
      "epoch": 2.0098360655737704,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 613,
      "total_loss": 0.5889111161231995
    },
    {
      "classification_loss": 0.6307178735733032,
      "epoch": 2.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 614,
      "total_loss": 0.6307178735733032
    },
    {
      "classification_loss": 0.7059490084648132,
      "epoch": 2.0163934426229506,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 615,
      "total_loss": 0.7059490084648132
    },
    {
      "classification_loss": 0.6091319918632507,
      "epoch": 2.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 616,
      "total_loss": 0.6091319918632507
    },
    {
      "classification_loss": 0.6122727990150452,
      "epoch": 2.0229508196721313,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 617,
      "total_loss": 0.6122727990150452
    },
    {
      "classification_loss": 0.6368649005889893,
      "epoch": 2.0262295081967214,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 618,
      "total_loss": 0.6368649005889893
    },
    {
      "classification_loss": 0.664544403553009,
      "epoch": 2.0295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 619,
      "total_loss": 0.664544403553009
    },
    {
      "classification_loss": 0.6196258664131165,
      "epoch": 2.0327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 620,
      "total_loss": 0.6196258664131165
    },
    {
      "classification_loss": 0.7017029523849487,
      "epoch": 2.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 621,
      "total_loss": 0.7017029523849487
    },
    {
      "classification_loss": 0.6405217051506042,
      "epoch": 2.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 622,
      "total_loss": 0.6405217051506042
    },
    {
      "classification_loss": 0.5752021074295044,
      "epoch": 2.042622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 623,
      "total_loss": 0.5752021074295044
    },
    {
      "classification_loss": 0.6312670111656189,
      "epoch": 2.045901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 624,
      "total_loss": 0.6312670111656189
    },
    {
      "classification_loss": 0.6342020630836487,
      "epoch": 2.0491803278688523,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 625,
      "total_loss": 0.6342020630836487
    },
    {
      "classification_loss": 0.6810859441757202,
      "epoch": 2.0524590163934424,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 626,
      "total_loss": 0.6810859441757202
    },
    {
      "classification_loss": 0.6694324016571045,
      "epoch": 2.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 627,
      "total_loss": 0.6694324016571045
    },
    {
      "classification_loss": 0.5900853872299194,
      "epoch": 2.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 628,
      "total_loss": 0.5900853872299194
    },
    {
      "classification_loss": 0.6419001817703247,
      "epoch": 2.0622950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 629,
      "total_loss": 0.6419001817703247
    },
    {
      "classification_loss": 0.6685182452201843,
      "epoch": 2.0655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 630,
      "total_loss": 0.6685182452201843
    },
    {
      "classification_loss": 0.6527522802352905,
      "epoch": 2.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 631,
      "total_loss": 0.6527522802352905
    },
    {
      "classification_loss": 0.6275098919868469,
      "epoch": 2.0721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 632,
      "total_loss": 0.6275098919868469
    },
    {
      "classification_loss": 0.6363979578018188,
      "epoch": 2.0754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 633,
      "total_loss": 0.6363979578018188
    },
    {
      "classification_loss": 0.6989892721176147,
      "epoch": 2.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 634,
      "total_loss": 0.6989892721176147
    },
    {
      "classification_loss": 0.6662885546684265,
      "epoch": 2.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 635,
      "total_loss": 0.6662885546684265
    },
    {
      "classification_loss": 0.6186323165893555,
      "epoch": 2.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 636,
      "total_loss": 0.6186323165893555
    },
    {
      "classification_loss": 0.681086003780365,
      "epoch": 2.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 637,
      "total_loss": 0.681086003780365
    },
    {
      "classification_loss": 0.5973480343818665,
      "epoch": 2.091803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 638,
      "total_loss": 0.5973480343818665
    },
    {
      "classification_loss": 0.6127945780754089,
      "epoch": 2.095081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 639,
      "total_loss": 0.6127945780754089
    },
    {
      "classification_loss": 0.6166046857833862,
      "epoch": 2.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 640,
      "total_loss": 0.6166046857833862
    },
    {
      "classification_loss": 0.7296397089958191,
      "epoch": 2.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 641,
      "total_loss": 0.7296397089958191
    },
    {
      "classification_loss": 0.6046603322029114,
      "epoch": 2.1049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 642,
      "total_loss": 0.6046603322029114
    },
    {
      "classification_loss": 0.6351684331893921,
      "epoch": 2.1081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 643,
      "total_loss": 0.6351684331893921
    },
    {
      "classification_loss": 0.6777194738388062,
      "epoch": 2.1114754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 644,
      "total_loss": 0.6777194738388062
    },
    {
      "classification_loss": 0.608546793460846,
      "epoch": 2.1147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 645,
      "total_loss": 0.608546793460846
    },
    {
      "classification_loss": 0.5731123089790344,
      "epoch": 2.1180327868852458,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 646,
      "total_loss": 0.5731123089790344
    },
    {
      "classification_loss": 0.6157625913619995,
      "epoch": 2.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 647,
      "total_loss": 0.6157625913619995
    },
    {
      "classification_loss": 0.6742538809776306,
      "epoch": 2.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 648,
      "total_loss": 0.6742538809776306
    },
    {
      "classification_loss": 0.5543784499168396,
      "epoch": 2.1278688524590166,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 649,
      "total_loss": 0.5543784499168396
    },
    {
      "classification_loss": 0.6416159272193909,
      "epoch": 2.1311475409836067,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 650,
      "total_loss": 0.6416159272193909
    },
    {
      "classification_loss": 0.6636183857917786,
      "epoch": 2.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 651,
      "total_loss": 0.6636183857917786
    },
    {
      "classification_loss": 0.7281912565231323,
      "epoch": 2.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 652,
      "total_loss": 0.7281912565231323
    },
    {
      "classification_loss": 0.6609664559364319,
      "epoch": 2.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 653,
      "total_loss": 0.6609664559364319
    },
    {
      "classification_loss": 0.7208912372589111,
      "epoch": 2.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 654,
      "total_loss": 0.7208912372589111
    },
    {
      "classification_loss": 0.6357686519622803,
      "epoch": 2.1475409836065573,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 655,
      "total_loss": 0.6357686519622803
    },
    {
      "classification_loss": 0.6465966105461121,
      "epoch": 2.1508196721311474,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 656,
      "total_loss": 0.6465966105461121
    },
    {
      "classification_loss": 0.6632183790206909,
      "epoch": 2.1540983606557376,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 657,
      "total_loss": 0.6632183790206909
    },
    {
      "classification_loss": 0.7014584541320801,
      "epoch": 2.1573770491803277,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 658,
      "total_loss": 0.7014584541320801
    },
    {
      "classification_loss": 0.6379348635673523,
      "epoch": 2.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 659,
      "total_loss": 0.6379348635673523
    },
    {
      "classification_loss": 0.6287696957588196,
      "epoch": 2.1639344262295084,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 660,
      "total_loss": 0.6287696957588196
    },
    {
      "classification_loss": 0.6621063947677612,
      "epoch": 2.1672131147540985,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 661,
      "total_loss": 0.6621063947677612
    },
    {
      "classification_loss": 0.6251619458198547,
      "epoch": 2.1704918032786886,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 662,
      "total_loss": 0.6251619458198547
    },
    {
      "classification_loss": 0.5917546153068542,
      "epoch": 2.1737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 663,
      "total_loss": 0.5917546153068542
    },
    {
      "classification_loss": 0.6364070177078247,
      "epoch": 2.177049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 664,
      "total_loss": 0.6364070177078247
    },
    {
      "classification_loss": 0.6229299306869507,
      "epoch": 2.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 665,
      "total_loss": 0.6229299306869507
    },
    {
      "classification_loss": 0.603290319442749,
      "epoch": 2.183606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 666,
      "total_loss": 0.603290319442749
    },
    {
      "classification_loss": 0.6712124347686768,
      "epoch": 2.1868852459016392,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 667,
      "total_loss": 0.6712124347686768
    },
    {
      "classification_loss": 0.6622190475463867,
      "epoch": 2.1901639344262294,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 668,
      "total_loss": 0.6622190475463867
    },
    {
      "classification_loss": 0.6293405294418335,
      "epoch": 2.1934426229508195,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 669,
      "total_loss": 0.6293405294418335
    },
    {
      "classification_loss": 0.6544433832168579,
      "epoch": 2.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 670,
      "total_loss": 0.6544433832168579
    },
    {
      "classification_loss": 0.6557403206825256,
      "epoch": 2.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 671,
      "total_loss": 0.6557403206825256
    },
    {
      "classification_loss": 0.6503519415855408,
      "epoch": 2.2032786885245903,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 672,
      "total_loss": 0.6503519415855408
    },
    {
      "classification_loss": 0.6752119660377502,
      "epoch": 2.2065573770491804,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 673,
      "total_loss": 0.6752119660377502
    },
    {
      "classification_loss": 0.6439279913902283,
      "epoch": 2.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 674,
      "total_loss": 0.6439279913902283
    },
    {
      "classification_loss": 0.6061301827430725,
      "epoch": 2.2131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 675,
      "total_loss": 0.6061301827430725
    },
    {
      "classification_loss": 0.6402472257614136,
      "epoch": 2.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 676,
      "total_loss": 0.6402472257614136
    },
    {
      "classification_loss": 0.6655811667442322,
      "epoch": 2.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 677,
      "total_loss": 0.6655811667442322
    },
    {
      "classification_loss": 0.6362366676330566,
      "epoch": 2.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 678,
      "total_loss": 0.6362366676330566
    },
    {
      "classification_loss": 0.6889700293540955,
      "epoch": 2.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 679,
      "total_loss": 0.6889700293540955
    },
    {
      "classification_loss": 0.6633920073509216,
      "epoch": 2.2295081967213113,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 680,
      "total_loss": 0.6633920073509216
    },
    {
      "classification_loss": 0.6790302991867065,
      "epoch": 2.2327868852459014,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 681,
      "total_loss": 0.6790302991867065
    },
    {
      "classification_loss": 0.6755275726318359,
      "epoch": 2.236065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 682,
      "total_loss": 0.6755275726318359
    },
    {
      "classification_loss": 0.6033339500427246,
      "epoch": 2.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 683,
      "total_loss": 0.6033339500427246
    },
    {
      "classification_loss": 0.6290706992149353,
      "epoch": 2.2426229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 684,
      "total_loss": 0.6290706992149353
    },
    {
      "classification_loss": 0.6768598556518555,
      "epoch": 2.2459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 685,
      "total_loss": 0.6768598556518555
    },
    {
      "classification_loss": 0.7705265283584595,
      "epoch": 2.2491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 686,
      "total_loss": 0.7705265283584595
    },
    {
      "classification_loss": 0.7038679718971252,
      "epoch": 2.2524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 687,
      "total_loss": 0.7038679718971252
    },
    {
      "classification_loss": 0.6418219804763794,
      "epoch": 2.2557377049180327,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 688,
      "total_loss": 0.6418219804763794
    },
    {
      "classification_loss": 0.6359134316444397,
      "epoch": 2.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 689,
      "total_loss": 0.6359134316444397
    },
    {
      "classification_loss": 0.7144468426704407,
      "epoch": 2.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 690,
      "total_loss": 0.7144468426704407
    },
    {
      "classification_loss": 0.6699197292327881,
      "epoch": 2.265573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 691,
      "total_loss": 0.6699197292327881
    },
    {
      "classification_loss": 0.6238967180252075,
      "epoch": 2.2688524590163937,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 692,
      "total_loss": 0.6238967180252075
    },
    {
      "classification_loss": 0.6489255428314209,
      "epoch": 2.2721311475409838,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 693,
      "total_loss": 0.6489255428314209
    },
    {
      "classification_loss": 0.6157221794128418,
      "epoch": 2.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 694,
      "total_loss": 0.6157221794128418
    },
    {
      "classification_loss": 0.6476417183876038,
      "epoch": 2.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 695,
      "total_loss": 0.6476417183876038
    },
    {
      "classification_loss": 0.6163961887359619,
      "epoch": 2.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 696,
      "total_loss": 0.6163961887359619
    },
    {
      "classification_loss": 0.6184711456298828,
      "epoch": 2.2852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 697,
      "total_loss": 0.6184711456298828
    },
    {
      "classification_loss": 0.7043271064758301,
      "epoch": 2.2885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 698,
      "total_loss": 0.7043271064758301
    },
    {
      "classification_loss": 0.6633787155151367,
      "epoch": 2.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 699,
      "total_loss": 0.6633787155151367
    },
    {
      "epoch": 2.2950819672131146,
      "grad_norm": 5.344283103942871,
      "learning_rate": 0.00018003333333333334,
      "loss": 0.6494,
      "step": 700
    },
    {
      "classification_loss": 0.6015912294387817,
      "epoch": 2.2950819672131146,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 700,
      "total_loss": 0.6015912294387817
    },
    {
      "classification_loss": 0.6313098669052124,
      "epoch": 2.2983606557377048,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 701,
      "total_loss": 0.6313098669052124
    },
    {
      "classification_loss": 0.6559452414512634,
      "epoch": 2.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 702,
      "total_loss": 0.6559452414512634
    },
    {
      "classification_loss": 0.6160898804664612,
      "epoch": 2.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 703,
      "total_loss": 0.6160898804664612
    },
    {
      "classification_loss": 0.6739492416381836,
      "epoch": 2.3081967213114756,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 704,
      "total_loss": 0.6739492416381836
    },
    {
      "classification_loss": 0.6791262030601501,
      "epoch": 2.3114754098360657,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 705,
      "total_loss": 0.6791262030601501
    },
    {
      "classification_loss": 0.6436153650283813,
      "epoch": 2.314754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 706,
      "total_loss": 0.6436153650283813
    },
    {
      "classification_loss": 0.636262834072113,
      "epoch": 2.318032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 707,
      "total_loss": 0.636262834072113
    },
    {
      "classification_loss": 0.6206358075141907,
      "epoch": 2.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 708,
      "total_loss": 0.6206358075141907
    },
    {
      "classification_loss": 0.6354017853736877,
      "epoch": 2.324590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 709,
      "total_loss": 0.6354017853736877
    },
    {
      "classification_loss": 0.6266064047813416,
      "epoch": 2.3278688524590163,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 710,
      "total_loss": 0.6266064047813416
    },
    {
      "classification_loss": 0.6030377745628357,
      "epoch": 2.3311475409836064,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 711,
      "total_loss": 0.6030377745628357
    },
    {
      "classification_loss": 0.7067434787750244,
      "epoch": 2.3344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 712,
      "total_loss": 0.7067434787750244
    },
    {
      "classification_loss": 0.653399646282196,
      "epoch": 2.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 713,
      "total_loss": 0.653399646282196
    },
    {
      "classification_loss": 0.6295281648635864,
      "epoch": 2.3409836065573773,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 714,
      "total_loss": 0.6295281648635864
    },
    {
      "classification_loss": 0.5760452151298523,
      "epoch": 2.3442622950819674,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 715,
      "total_loss": 0.5760452151298523
    },
    {
      "classification_loss": 0.6848753690719604,
      "epoch": 2.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 716,
      "total_loss": 0.6848753690719604
    },
    {
      "classification_loss": 0.6319652199745178,
      "epoch": 2.3508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 717,
      "total_loss": 0.6319652199745178
    },
    {
      "classification_loss": 0.6993268728256226,
      "epoch": 2.3540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 718,
      "total_loss": 0.6993268728256226
    },
    {
      "classification_loss": 0.6004312038421631,
      "epoch": 2.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 719,
      "total_loss": 0.6004312038421631
    },
    {
      "classification_loss": 0.6745848655700684,
      "epoch": 2.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 720,
      "total_loss": 0.6745848655700684
    },
    {
      "classification_loss": 0.6830859780311584,
      "epoch": 2.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 721,
      "total_loss": 0.6830859780311584
    },
    {
      "classification_loss": 0.5681923627853394,
      "epoch": 2.3672131147540982,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 722,
      "total_loss": 0.5681923627853394
    },
    {
      "classification_loss": 0.6823729872703552,
      "epoch": 2.3704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 723,
      "total_loss": 0.6823729872703552
    },
    {
      "classification_loss": 0.6283948421478271,
      "epoch": 2.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 724,
      "total_loss": 0.6283948421478271
    },
    {
      "classification_loss": 0.605022668838501,
      "epoch": 2.3770491803278686,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 725,
      "total_loss": 0.605022668838501
    },
    {
      "classification_loss": 0.6485809683799744,
      "epoch": 2.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 726,
      "total_loss": 0.6485809683799744
    },
    {
      "classification_loss": 0.673221230506897,
      "epoch": 2.3836065573770493,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 727,
      "total_loss": 0.673221230506897
    },
    {
      "classification_loss": 0.6369732618331909,
      "epoch": 2.3868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 728,
      "total_loss": 0.6369732618331909
    },
    {
      "classification_loss": 0.6934646368026733,
      "epoch": 2.3901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 729,
      "total_loss": 0.6934646368026733
    },
    {
      "classification_loss": 0.6480852961540222,
      "epoch": 2.3934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 730,
      "total_loss": 0.6480852961540222
    },
    {
      "classification_loss": 0.6743218898773193,
      "epoch": 2.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 731,
      "total_loss": 0.6743218898773193
    },
    {
      "classification_loss": 0.6566219329833984,
      "epoch": 2.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 732,
      "total_loss": 0.6566219329833984
    },
    {
      "classification_loss": 0.5913503766059875,
      "epoch": 2.40327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 733,
      "total_loss": 0.5913503766059875
    },
    {
      "classification_loss": 0.6156702637672424,
      "epoch": 2.40655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 734,
      "total_loss": 0.6156702637672424
    },
    {
      "classification_loss": 0.7036681771278381,
      "epoch": 2.4098360655737707,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 735,
      "total_loss": 0.7036681771278381
    },
    {
      "classification_loss": 0.6936730742454529,
      "epoch": 2.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 736,
      "total_loss": 0.6936730742454529
    },
    {
      "classification_loss": 0.6418623924255371,
      "epoch": 2.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 737,
      "total_loss": 0.6418623924255371
    },
    {
      "classification_loss": 0.6957152485847473,
      "epoch": 2.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 738,
      "total_loss": 0.6957152485847473
    },
    {
      "classification_loss": 0.6273118853569031,
      "epoch": 2.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 739,
      "total_loss": 0.6273118853569031
    },
    {
      "classification_loss": 0.6374371647834778,
      "epoch": 2.4262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 740,
      "total_loss": 0.6374371647834778
    },
    {
      "classification_loss": 0.6537522673606873,
      "epoch": 2.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 741,
      "total_loss": 0.6537522673606873
    },
    {
      "classification_loss": 0.5974616408348083,
      "epoch": 2.4327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 742,
      "total_loss": 0.5974616408348083
    },
    {
      "classification_loss": 0.6037794351577759,
      "epoch": 2.4360655737704917,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 743,
      "total_loss": 0.6037794351577759
    },
    {
      "classification_loss": 0.6868628263473511,
      "epoch": 2.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 744,
      "total_loss": 0.6868628263473511
    },
    {
      "classification_loss": 0.6581462621688843,
      "epoch": 2.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 745,
      "total_loss": 0.6581462621688843
    },
    {
      "classification_loss": 0.6530681252479553,
      "epoch": 2.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 746,
      "total_loss": 0.6530681252479553
    },
    {
      "classification_loss": 0.642918586730957,
      "epoch": 2.4491803278688526,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 747,
      "total_loss": 0.642918586730957
    },
    {
      "classification_loss": 0.6546857357025146,
      "epoch": 2.4524590163934428,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 748,
      "total_loss": 0.6546857357025146
    },
    {
      "classification_loss": 0.6840333938598633,
      "epoch": 2.455737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 749,
      "total_loss": 0.6840333938598633
    },
    {
      "classification_loss": 0.6529117822647095,
      "epoch": 2.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 750,
      "total_loss": 0.6529117822647095
    },
    {
      "classification_loss": 0.6581474542617798,
      "epoch": 2.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 751,
      "total_loss": 0.6581474542617798
    },
    {
      "classification_loss": 0.6244432330131531,
      "epoch": 2.4655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 752,
      "total_loss": 0.6244432330131531
    },
    {
      "classification_loss": 0.6419389247894287,
      "epoch": 2.4688524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 753,
      "total_loss": 0.6419389247894287
    },
    {
      "classification_loss": 0.6826913356781006,
      "epoch": 2.4721311475409835,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 754,
      "total_loss": 0.6826913356781006
    },
    {
      "classification_loss": 0.6254174709320068,
      "epoch": 2.4754098360655736,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 755,
      "total_loss": 0.6254174709320068
    },
    {
      "classification_loss": 0.6638511419296265,
      "epoch": 2.4786885245901638,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 756,
      "total_loss": 0.6638511419296265
    },
    {
      "classification_loss": 0.642062783241272,
      "epoch": 2.4819672131147543,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 757,
      "total_loss": 0.642062783241272
    },
    {
      "classification_loss": 0.6375603675842285,
      "epoch": 2.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 758,
      "total_loss": 0.6375603675842285
    },
    {
      "classification_loss": 0.644275426864624,
      "epoch": 2.4885245901639346,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 759,
      "total_loss": 0.644275426864624
    },
    {
      "classification_loss": 0.6752374172210693,
      "epoch": 2.4918032786885247,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 760,
      "total_loss": 0.6752374172210693
    },
    {
      "classification_loss": 0.6247467398643494,
      "epoch": 2.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 761,
      "total_loss": 0.6247467398643494
    },
    {
      "classification_loss": 0.6735555529594421,
      "epoch": 2.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 762,
      "total_loss": 0.6735555529594421
    },
    {
      "classification_loss": 0.6276712417602539,
      "epoch": 2.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 763,
      "total_loss": 0.6276712417602539
    },
    {
      "classification_loss": 0.6891867518424988,
      "epoch": 2.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 764,
      "total_loss": 0.6891867518424988
    },
    {
      "classification_loss": 0.7071112990379333,
      "epoch": 2.5081967213114753,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 765,
      "total_loss": 0.7071112990379333
    },
    {
      "classification_loss": 0.6198432445526123,
      "epoch": 2.5114754098360654,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 766,
      "total_loss": 0.6198432445526123
    },
    {
      "classification_loss": 0.647577702999115,
      "epoch": 2.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 767,
      "total_loss": 0.647577702999115
    },
    {
      "classification_loss": 0.6843294501304626,
      "epoch": 2.5180327868852457,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 768,
      "total_loss": 0.6843294501304626
    },
    {
      "classification_loss": 0.6392567157745361,
      "epoch": 2.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 769,
      "total_loss": 0.6392567157745361
    },
    {
      "classification_loss": 0.6806952953338623,
      "epoch": 2.5245901639344264,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 770,
      "total_loss": 0.6806952953338623
    },
    {
      "classification_loss": 0.5686100125312805,
      "epoch": 2.5278688524590165,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 771,
      "total_loss": 0.5686100125312805
    },
    {
      "classification_loss": 0.6038901805877686,
      "epoch": 2.5311475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 772,
      "total_loss": 0.6038901805877686
    },
    {
      "classification_loss": 0.6205811500549316,
      "epoch": 2.5344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 773,
      "total_loss": 0.6205811500549316
    },
    {
      "classification_loss": 0.5908520221710205,
      "epoch": 2.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 774,
      "total_loss": 0.5908520221710205
    },
    {
      "classification_loss": 0.6034255623817444,
      "epoch": 2.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 775,
      "total_loss": 0.6034255623817444
    },
    {
      "classification_loss": 0.648836076259613,
      "epoch": 2.544262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 776,
      "total_loss": 0.648836076259613
    },
    {
      "classification_loss": 0.7126756310462952,
      "epoch": 2.5475409836065572,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 777,
      "total_loss": 0.7126756310462952
    },
    {
      "classification_loss": 0.6650545001029968,
      "epoch": 2.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 778,
      "total_loss": 0.6650545001029968
    },
    {
      "classification_loss": 0.6158191561698914,
      "epoch": 2.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 779,
      "total_loss": 0.6158191561698914
    },
    {
      "classification_loss": 0.6380303502082825,
      "epoch": 2.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 780,
      "total_loss": 0.6380303502082825
    },
    {
      "classification_loss": 0.6430254578590393,
      "epoch": 2.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 781,
      "total_loss": 0.6430254578590393
    },
    {
      "classification_loss": 0.7424637675285339,
      "epoch": 2.5639344262295083,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 782,
      "total_loss": 0.7424637675285339
    },
    {
      "classification_loss": 0.6411867737770081,
      "epoch": 2.5672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 783,
      "total_loss": 0.6411867737770081
    },
    {
      "classification_loss": 0.6482566595077515,
      "epoch": 2.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 784,
      "total_loss": 0.6482566595077515
    },
    {
      "classification_loss": 0.6574856042861938,
      "epoch": 2.5737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 785,
      "total_loss": 0.6574856042861938
    },
    {
      "classification_loss": 0.6309465169906616,
      "epoch": 2.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 786,
      "total_loss": 0.6309465169906616
    },
    {
      "classification_loss": 0.6034931540489197,
      "epoch": 2.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 787,
      "total_loss": 0.6034931540489197
    },
    {
      "classification_loss": 0.6567918658256531,
      "epoch": 2.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 788,
      "total_loss": 0.6567918658256531
    },
    {
      "classification_loss": 0.6430277824401855,
      "epoch": 2.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 789,
      "total_loss": 0.6430277824401855
    },
    {
      "classification_loss": 0.6831601858139038,
      "epoch": 2.5901639344262293,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 790,
      "total_loss": 0.6831601858139038
    },
    {
      "classification_loss": 0.644782304763794,
      "epoch": 2.5934426229508194,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 791,
      "total_loss": 0.644782304763794
    },
    {
      "classification_loss": 0.6441203355789185,
      "epoch": 2.59672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 792,
      "total_loss": 0.6441203355789185
    },
    {
      "classification_loss": 0.5802457332611084,
      "epoch": 2.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 793,
      "total_loss": 0.5802457332611084
    },
    {
      "classification_loss": 0.6385731101036072,
      "epoch": 2.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 794,
      "total_loss": 0.6385731101036072
    },
    {
      "classification_loss": 0.5878680348396301,
      "epoch": 2.6065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 795,
      "total_loss": 0.5878680348396301
    },
    {
      "classification_loss": 0.5290157198905945,
      "epoch": 2.6098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 796,
      "total_loss": 0.5290157198905945
    },
    {
      "classification_loss": 0.609389066696167,
      "epoch": 2.6131147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 797,
      "total_loss": 0.609389066696167
    },
    {
      "classification_loss": 0.7104659080505371,
      "epoch": 2.6163934426229507,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 798,
      "total_loss": 0.7104659080505371
    },
    {
      "classification_loss": 0.5846385955810547,
      "epoch": 2.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 799,
      "total_loss": 0.5846385955810547
    },
    {
      "epoch": 2.6229508196721314,
      "grad_norm": 1.482114553451538,
      "learning_rate": 0.00017669999999999999,
      "loss": 0.6441,
      "step": 800
    },
    {
      "classification_loss": 0.6165276765823364,
      "epoch": 2.6229508196721314,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 800,
      "total_loss": 0.6165276765823364
    },
    {
      "classification_loss": 0.6798444986343384,
      "epoch": 2.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 801,
      "total_loss": 0.6798444986343384
    },
    {
      "classification_loss": 0.671801745891571,
      "epoch": 2.6295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 802,
      "total_loss": 0.671801745891571
    },
    {
      "classification_loss": 0.6536487340927124,
      "epoch": 2.6327868852459018,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 803,
      "total_loss": 0.6536487340927124
    },
    {
      "classification_loss": 0.5797766447067261,
      "epoch": 2.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 804,
      "total_loss": 0.5797766447067261
    },
    {
      "classification_loss": 0.5981506109237671,
      "epoch": 2.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 805,
      "total_loss": 0.5981506109237671
    },
    {
      "classification_loss": 0.6688668131828308,
      "epoch": 2.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 806,
      "total_loss": 0.6688668131828308
    },
    {
      "classification_loss": 0.6629514694213867,
      "epoch": 2.6459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 807,
      "total_loss": 0.6629514694213867
    },
    {
      "classification_loss": 0.6459838151931763,
      "epoch": 2.6491803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 808,
      "total_loss": 0.6459838151931763
    },
    {
      "classification_loss": 0.5677611231803894,
      "epoch": 2.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 809,
      "total_loss": 0.5677611231803894
    },
    {
      "classification_loss": 0.6546405553817749,
      "epoch": 2.6557377049180326,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 810,
      "total_loss": 0.6546405553817749
    },
    {
      "classification_loss": 0.632031261920929,
      "epoch": 2.6590163934426227,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 811,
      "total_loss": 0.632031261920929
    },
    {
      "classification_loss": 0.7074254155158997,
      "epoch": 2.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 812,
      "total_loss": 0.7074254155158997
    },
    {
      "classification_loss": 0.5981698632240295,
      "epoch": 2.6655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 813,
      "total_loss": 0.5981698632240295
    },
    {
      "classification_loss": 0.6755303144454956,
      "epoch": 2.6688524590163936,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 814,
      "total_loss": 0.6755303144454956
    },
    {
      "classification_loss": 0.6654611229896545,
      "epoch": 2.6721311475409837,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 815,
      "total_loss": 0.6654611229896545
    },
    {
      "classification_loss": 0.6948223114013672,
      "epoch": 2.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 816,
      "total_loss": 0.6948223114013672
    },
    {
      "classification_loss": 0.649530827999115,
      "epoch": 2.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 817,
      "total_loss": 0.649530827999115
    },
    {
      "classification_loss": 0.6528067588806152,
      "epoch": 2.681967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 818,
      "total_loss": 0.6528067588806152
    },
    {
      "classification_loss": 0.7650780081748962,
      "epoch": 2.685245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 819,
      "total_loss": 0.7650780081748962
    },
    {
      "classification_loss": 0.6585376262664795,
      "epoch": 2.6885245901639343,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 820,
      "total_loss": 0.6585376262664795
    },
    {
      "classification_loss": 0.6575019359588623,
      "epoch": 2.6918032786885244,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 821,
      "total_loss": 0.6575019359588623
    },
    {
      "classification_loss": 0.6250102519989014,
      "epoch": 2.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 822,
      "total_loss": 0.6250102519989014
    },
    {
      "classification_loss": 0.6061476469039917,
      "epoch": 2.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 823,
      "total_loss": 0.6061476469039917
    },
    {
      "classification_loss": 0.6287143230438232,
      "epoch": 2.7016393442622952,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 824,
      "total_loss": 0.6287143230438232
    },
    {
      "classification_loss": 0.6821833848953247,
      "epoch": 2.7049180327868854,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 825,
      "total_loss": 0.6821833848953247
    },
    {
      "classification_loss": 0.6674641370773315,
      "epoch": 2.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 826,
      "total_loss": 0.6674641370773315
    },
    {
      "classification_loss": 0.6783576011657715,
      "epoch": 2.7114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 827,
      "total_loss": 0.6783576011657715
    },
    {
      "classification_loss": 0.6821754574775696,
      "epoch": 2.7147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 828,
      "total_loss": 0.6821754574775696
    },
    {
      "classification_loss": 0.5874418020248413,
      "epoch": 2.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 829,
      "total_loss": 0.5874418020248413
    },
    {
      "classification_loss": 0.6580792665481567,
      "epoch": 2.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 830,
      "total_loss": 0.6580792665481567
    },
    {
      "classification_loss": 0.7029463052749634,
      "epoch": 2.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 831,
      "total_loss": 0.7029463052749634
    },
    {
      "classification_loss": 0.6528286337852478,
      "epoch": 2.7278688524590162,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 832,
      "total_loss": 0.6528286337852478
    },
    {
      "classification_loss": 0.6596006751060486,
      "epoch": 2.7311475409836063,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 833,
      "total_loss": 0.6596006751060486
    },
    {
      "classification_loss": 0.5583503842353821,
      "epoch": 2.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 834,
      "total_loss": 0.5583503842353821
    },
    {
      "classification_loss": 0.6728929877281189,
      "epoch": 2.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 835,
      "total_loss": 0.6728929877281189
    },
    {
      "classification_loss": 0.640954852104187,
      "epoch": 2.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 836,
      "total_loss": 0.640954852104187
    },
    {
      "classification_loss": 0.6514449119567871,
      "epoch": 2.7442622950819673,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 837,
      "total_loss": 0.6514449119567871
    },
    {
      "classification_loss": 0.6648597717285156,
      "epoch": 2.7475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 838,
      "total_loss": 0.6648597717285156
    },
    {
      "classification_loss": 0.6474226713180542,
      "epoch": 2.7508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 839,
      "total_loss": 0.6474226713180542
    },
    {
      "classification_loss": 0.6880812644958496,
      "epoch": 2.7540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 840,
      "total_loss": 0.6880812644958496
    },
    {
      "classification_loss": 0.6593073010444641,
      "epoch": 2.7573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 841,
      "total_loss": 0.6593073010444641
    },
    {
      "classification_loss": 0.684220016002655,
      "epoch": 2.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 842,
      "total_loss": 0.684220016002655
    },
    {
      "classification_loss": 0.6175960302352905,
      "epoch": 2.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 843,
      "total_loss": 0.6175960302352905
    },
    {
      "classification_loss": 0.7068725824356079,
      "epoch": 2.7672131147540986,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 844,
      "total_loss": 0.7068725824356079
    },
    {
      "classification_loss": 0.5608829259872437,
      "epoch": 2.7704918032786887,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 845,
      "total_loss": 0.5608829259872437
    },
    {
      "classification_loss": 0.6269100308418274,
      "epoch": 2.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 846,
      "total_loss": 0.6269100308418274
    },
    {
      "classification_loss": 0.5393357276916504,
      "epoch": 2.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 847,
      "total_loss": 0.5393357276916504
    },
    {
      "classification_loss": 0.6503341197967529,
      "epoch": 2.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 848,
      "total_loss": 0.6503341197967529
    },
    {
      "classification_loss": 0.6777204871177673,
      "epoch": 2.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 849,
      "total_loss": 0.6777204871177673
    },
    {
      "classification_loss": 0.6062805652618408,
      "epoch": 2.7868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 850,
      "total_loss": 0.6062805652618408
    },
    {
      "classification_loss": 0.6439261436462402,
      "epoch": 2.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 851,
      "total_loss": 0.6439261436462402
    },
    {
      "classification_loss": 0.6762831807136536,
      "epoch": 2.7934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 852,
      "total_loss": 0.6762831807136536
    },
    {
      "classification_loss": 0.68703693151474,
      "epoch": 2.7967213114754097,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 853,
      "total_loss": 0.68703693151474
    },
    {
      "classification_loss": 0.7455493807792664,
      "epoch": 2.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 854,
      "total_loss": 0.7455493807792664
    },
    {
      "classification_loss": 0.6454305648803711,
      "epoch": 2.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 855,
      "total_loss": 0.6454305648803711
    },
    {
      "classification_loss": 0.7326847910881042,
      "epoch": 2.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 856,
      "total_loss": 0.7326847910881042
    },
    {
      "classification_loss": 0.6488208770751953,
      "epoch": 2.8098360655737706,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 857,
      "total_loss": 0.6488208770751953
    },
    {
      "classification_loss": 0.652259111404419,
      "epoch": 2.8131147540983608,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 858,
      "total_loss": 0.652259111404419
    },
    {
      "classification_loss": 0.6303403973579407,
      "epoch": 2.816393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 859,
      "total_loss": 0.6303403973579407
    },
    {
      "classification_loss": 0.6200801134109497,
      "epoch": 2.819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 860,
      "total_loss": 0.6200801134109497
    },
    {
      "classification_loss": 0.5335289239883423,
      "epoch": 2.822950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 861,
      "total_loss": 0.5335289239883423
    },
    {
      "classification_loss": 0.5886074304580688,
      "epoch": 2.8262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 862,
      "total_loss": 0.5886074304580688
    },
    {
      "classification_loss": 0.6773108243942261,
      "epoch": 2.8295081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 863,
      "total_loss": 0.6773108243942261
    },
    {
      "classification_loss": 0.6562666296958923,
      "epoch": 2.8327868852459015,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 864,
      "total_loss": 0.6562666296958923
    },
    {
      "classification_loss": 0.6234915256500244,
      "epoch": 2.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 865,
      "total_loss": 0.6234915256500244
    },
    {
      "classification_loss": 0.6734471321105957,
      "epoch": 2.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 866,
      "total_loss": 0.6734471321105957
    },
    {
      "classification_loss": 0.5595992207527161,
      "epoch": 2.8426229508196723,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 867,
      "total_loss": 0.5595992207527161
    },
    {
      "classification_loss": 0.584645688533783,
      "epoch": 2.8459016393442624,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 868,
      "total_loss": 0.584645688533783
    },
    {
      "classification_loss": 0.6149096488952637,
      "epoch": 2.8491803278688526,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 869,
      "total_loss": 0.6149096488952637
    },
    {
      "classification_loss": 0.7014881372451782,
      "epoch": 2.8524590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 870,
      "total_loss": 0.7014881372451782
    },
    {
      "classification_loss": 0.6081861853599548,
      "epoch": 2.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 871,
      "total_loss": 0.6081861853599548
    },
    {
      "classification_loss": 0.6137315630912781,
      "epoch": 2.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 872,
      "total_loss": 0.6137315630912781
    },
    {
      "classification_loss": 0.6450225710868835,
      "epoch": 2.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 873,
      "total_loss": 0.6450225710868835
    },
    {
      "classification_loss": 0.617633044719696,
      "epoch": 2.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 874,
      "total_loss": 0.617633044719696
    },
    {
      "classification_loss": 0.6157092452049255,
      "epoch": 2.8688524590163933,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 875,
      "total_loss": 0.6157092452049255
    },
    {
      "classification_loss": 0.6017945408821106,
      "epoch": 2.8721311475409834,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 876,
      "total_loss": 0.6017945408821106
    },
    {
      "classification_loss": 0.7463010549545288,
      "epoch": 2.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 877,
      "total_loss": 0.7463010549545288
    },
    {
      "classification_loss": 0.6346688270568848,
      "epoch": 2.8786885245901637,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 878,
      "total_loss": 0.6346688270568848
    },
    {
      "classification_loss": 0.7352220416069031,
      "epoch": 2.8819672131147542,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 879,
      "total_loss": 0.7352220416069031
    },
    {
      "classification_loss": 0.6773791313171387,
      "epoch": 2.8852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 880,
      "total_loss": 0.6773791313171387
    },
    {
      "classification_loss": 0.6793453097343445,
      "epoch": 2.8885245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 881,
      "total_loss": 0.6793453097343445
    },
    {
      "classification_loss": 0.6323679685592651,
      "epoch": 2.8918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 882,
      "total_loss": 0.6323679685592651
    },
    {
      "classification_loss": 0.6817277073860168,
      "epoch": 2.8950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 883,
      "total_loss": 0.6817277073860168
    },
    {
      "classification_loss": 0.631052553653717,
      "epoch": 2.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 884,
      "total_loss": 0.631052553653717
    },
    {
      "classification_loss": 0.6746972799301147,
      "epoch": 2.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 885,
      "total_loss": 0.6746972799301147
    },
    {
      "classification_loss": 0.708028256893158,
      "epoch": 2.904918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 886,
      "total_loss": 0.708028256893158
    },
    {
      "classification_loss": 0.6278062462806702,
      "epoch": 2.9081967213114757,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 887,
      "total_loss": 0.6278062462806702
    },
    {
      "classification_loss": 0.6293867230415344,
      "epoch": 2.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 888,
      "total_loss": 0.6293867230415344
    },
    {
      "classification_loss": 0.6529548764228821,
      "epoch": 2.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 889,
      "total_loss": 0.6529548764228821
    },
    {
      "classification_loss": 0.6471044421195984,
      "epoch": 2.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 890,
      "total_loss": 0.6471044421195984
    },
    {
      "classification_loss": 0.6408422589302063,
      "epoch": 2.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 891,
      "total_loss": 0.6408422589302063
    },
    {
      "classification_loss": 0.6021150946617126,
      "epoch": 2.9245901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 892,
      "total_loss": 0.6021150946617126
    },
    {
      "classification_loss": 0.6501173377037048,
      "epoch": 2.9278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 893,
      "total_loss": 0.6501173377037048
    },
    {
      "classification_loss": 0.6190425753593445,
      "epoch": 2.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 894,
      "total_loss": 0.6190425753593445
    },
    {
      "classification_loss": 0.7293383479118347,
      "epoch": 2.9344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 895,
      "total_loss": 0.7293383479118347
    },
    {
      "classification_loss": 0.7526530623435974,
      "epoch": 2.9377049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 896,
      "total_loss": 0.7526530623435974
    },
    {
      "classification_loss": 0.6091666221618652,
      "epoch": 2.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 897,
      "total_loss": 0.6091666221618652
    },
    {
      "classification_loss": 0.6777676343917847,
      "epoch": 2.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 898,
      "total_loss": 0.6777676343917847
    },
    {
      "classification_loss": 0.6265581846237183,
      "epoch": 2.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 899,
      "total_loss": 0.6265581846237183
    },
    {
      "epoch": 2.9508196721311473,
      "grad_norm": 1.4460543394088745,
      "learning_rate": 0.0001733666666666667,
      "loss": 0.649,
      "step": 900
    },
    {
      "classification_loss": 0.5869249105453491,
      "epoch": 2.9508196721311473,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 900,
      "total_loss": 0.5869249105453491
    },
    {
      "classification_loss": 0.5750132203102112,
      "epoch": 2.954098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 901,
      "total_loss": 0.5750132203102112
    },
    {
      "classification_loss": 0.5730440020561218,
      "epoch": 2.957377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 902,
      "total_loss": 0.5730440020561218
    },
    {
      "classification_loss": 0.6622456908226013,
      "epoch": 2.960655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 903,
      "total_loss": 0.6622456908226013
    },
    {
      "classification_loss": 0.617550790309906,
      "epoch": 2.963934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 904,
      "total_loss": 0.617550790309906
    },
    {
      "classification_loss": 0.6462339758872986,
      "epoch": 2.9672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 905,
      "total_loss": 0.6462339758872986
    },
    {
      "classification_loss": 0.6930297017097473,
      "epoch": 2.9704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 906,
      "total_loss": 0.6930297017097473
    },
    {
      "classification_loss": 0.6748668551445007,
      "epoch": 2.9737704918032786,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 907,
      "total_loss": 0.6748668551445007
    },
    {
      "classification_loss": 0.6548202037811279,
      "epoch": 2.9770491803278687,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 908,
      "total_loss": 0.6548202037811279
    },
    {
      "classification_loss": 0.6681627035140991,
      "epoch": 2.9803278688524593,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 909,
      "total_loss": 0.6681627035140991
    },
    {
      "classification_loss": 0.5931771397590637,
      "epoch": 2.9836065573770494,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 910,
      "total_loss": 0.5931771397590637
    },
    {
      "classification_loss": 0.6538668274879456,
      "epoch": 2.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 911,
      "total_loss": 0.6538668274879456
    },
    {
      "classification_loss": 0.6365863084793091,
      "epoch": 2.9901639344262296,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 912,
      "total_loss": 0.6365863084793091
    },
    {
      "classification_loss": 0.5810474157333374,
      "epoch": 2.9934426229508198,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 913,
      "total_loss": 0.5810474157333374
    },
    {
      "classification_loss": 0.6142060160636902,
      "epoch": 2.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 914,
      "total_loss": 0.6142060160636902
    },
    {
      "classification_loss": 0.7848630547523499,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.7848630547523499
    },
    {
      "classification_loss": 0.785969614982605,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.785969614982605
    },
    {
      "classification_loss": 0.7556406855583191,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.7556406855583191
    },
    {
      "classification_loss": 0.7785823941230774,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.7785823941230774
    },
    {
      "classification_loss": 0.7843592166900635,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.7843592166900635
    },
    {
      "classification_loss": 0.7554011940956116,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.7554011940956116
    },
    {
      "classification_loss": 0.794223964214325,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.794223964214325
    },
    {
      "classification_loss": 0.8033287525177002,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.8033287525177002
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.392,
      "eval_f1": 0.0379746835443038,
      "eval_loss": 0.7797433137893677,
      "eval_precision": 0.75,
      "eval_recall": 0.01948051948051948,
      "eval_runtime": 6.0232,
      "eval_samples_per_second": 166.024,
      "eval_steps_per_second": 1.328,
      "step": 915
    },
    {
      "classification_loss": 0.6391879320144653,
      "epoch": 3.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 915,
      "total_loss": 0.6391879320144653
    },
    {
      "classification_loss": 0.6081079840660095,
      "epoch": 3.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 916,
      "total_loss": 0.6081079840660095
    },
    {
      "classification_loss": 0.6723644137382507,
      "epoch": 3.0065573770491802,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 917,
      "total_loss": 0.6723644137382507
    },
    {
      "classification_loss": 0.6447715759277344,
      "epoch": 3.0098360655737704,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 918,
      "total_loss": 0.6447715759277344
    },
    {
      "classification_loss": 0.6658512949943542,
      "epoch": 3.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 919,
      "total_loss": 0.6658512949943542
    },
    {
      "classification_loss": 0.5669940114021301,
      "epoch": 3.0163934426229506,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 920,
      "total_loss": 0.5669940114021301
    },
    {
      "classification_loss": 0.7018253803253174,
      "epoch": 3.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 921,
      "total_loss": 0.7018253803253174
    },
    {
      "classification_loss": 0.5741744637489319,
      "epoch": 3.0229508196721313,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 922,
      "total_loss": 0.5741744637489319
    },
    {
      "classification_loss": 0.6204573512077332,
      "epoch": 3.0262295081967214,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 923,
      "total_loss": 0.6204573512077332
    },
    {
      "classification_loss": 0.6704285740852356,
      "epoch": 3.0295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 924,
      "total_loss": 0.6704285740852356
    },
    {
      "classification_loss": 0.6762174963951111,
      "epoch": 3.0327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 925,
      "total_loss": 0.6762174963951111
    },
    {
      "classification_loss": 0.5830672383308411,
      "epoch": 3.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 926,
      "total_loss": 0.5830672383308411
    },
    {
      "classification_loss": 0.6026861667633057,
      "epoch": 3.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 927,
      "total_loss": 0.6026861667633057
    },
    {
      "classification_loss": 0.6512871384620667,
      "epoch": 3.042622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 928,
      "total_loss": 0.6512871384620667
    },
    {
      "classification_loss": 0.602051317691803,
      "epoch": 3.045901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 929,
      "total_loss": 0.602051317691803
    },
    {
      "classification_loss": 0.6075294613838196,
      "epoch": 3.0491803278688523,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 930,
      "total_loss": 0.6075294613838196
    },
    {
      "classification_loss": 0.6545624136924744,
      "epoch": 3.0524590163934424,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 931,
      "total_loss": 0.6545624136924744
    },
    {
      "classification_loss": 0.6966668367385864,
      "epoch": 3.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 932,
      "total_loss": 0.6966668367385864
    },
    {
      "classification_loss": 0.5700237154960632,
      "epoch": 3.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 933,
      "total_loss": 0.5700237154960632
    },
    {
      "classification_loss": 0.608841598033905,
      "epoch": 3.0622950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 934,
      "total_loss": 0.608841598033905
    },
    {
      "classification_loss": 0.6373609304428101,
      "epoch": 3.0655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 935,
      "total_loss": 0.6373609304428101
    },
    {
      "classification_loss": 0.742216944694519,
      "epoch": 3.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 936,
      "total_loss": 0.742216944694519
    },
    {
      "classification_loss": 0.6519098877906799,
      "epoch": 3.0721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 937,
      "total_loss": 0.6519098877906799
    },
    {
      "classification_loss": 0.6123955845832825,
      "epoch": 3.0754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 938,
      "total_loss": 0.6123955845832825
    },
    {
      "classification_loss": 0.6014568209648132,
      "epoch": 3.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 939,
      "total_loss": 0.6014568209648132
    },
    {
      "classification_loss": 0.5809444189071655,
      "epoch": 3.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 940,
      "total_loss": 0.5809444189071655
    },
    {
      "classification_loss": 0.6497688293457031,
      "epoch": 3.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 941,
      "total_loss": 0.6497688293457031
    },
    {
      "classification_loss": 0.6174978017807007,
      "epoch": 3.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 942,
      "total_loss": 0.6174978017807007
    },
    {
      "classification_loss": 0.6269055008888245,
      "epoch": 3.091803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 943,
      "total_loss": 0.6269055008888245
    },
    {
      "classification_loss": 0.5952149033546448,
      "epoch": 3.095081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 944,
      "total_loss": 0.5952149033546448
    },
    {
      "classification_loss": 0.6380160450935364,
      "epoch": 3.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 945,
      "total_loss": 0.6380160450935364
    },
    {
      "classification_loss": 0.6470086574554443,
      "epoch": 3.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 946,
      "total_loss": 0.6470086574554443
    },
    {
      "classification_loss": 0.6282336711883545,
      "epoch": 3.1049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 947,
      "total_loss": 0.6282336711883545
    },
    {
      "classification_loss": 0.6267358660697937,
      "epoch": 3.1081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 948,
      "total_loss": 0.6267358660697937
    },
    {
      "classification_loss": 0.6368218064308167,
      "epoch": 3.1114754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 949,
      "total_loss": 0.6368218064308167
    },
    {
      "classification_loss": 0.5747142434120178,
      "epoch": 3.1147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 950,
      "total_loss": 0.5747142434120178
    },
    {
      "classification_loss": 0.6117197275161743,
      "epoch": 3.1180327868852458,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 951,
      "total_loss": 0.6117197275161743
    },
    {
      "classification_loss": 0.632226824760437,
      "epoch": 3.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 952,
      "total_loss": 0.632226824760437
    },
    {
      "classification_loss": 0.5954176187515259,
      "epoch": 3.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 953,
      "total_loss": 0.5954176187515259
    },
    {
      "classification_loss": 0.6407992839813232,
      "epoch": 3.1278688524590166,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 954,
      "total_loss": 0.6407992839813232
    },
    {
      "classification_loss": 0.5929293036460876,
      "epoch": 3.1311475409836067,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 955,
      "total_loss": 0.5929293036460876
    },
    {
      "classification_loss": 0.5927863717079163,
      "epoch": 3.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 956,
      "total_loss": 0.5927863717079163
    },
    {
      "classification_loss": 0.559292733669281,
      "epoch": 3.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 957,
      "total_loss": 0.559292733669281
    },
    {
      "classification_loss": 0.5586149096488953,
      "epoch": 3.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 958,
      "total_loss": 0.5586149096488953
    },
    {
      "classification_loss": 0.7186868786811829,
      "epoch": 3.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 959,
      "total_loss": 0.7186868786811829
    },
    {
      "classification_loss": 0.7261694669723511,
      "epoch": 3.1475409836065573,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 960,
      "total_loss": 0.7261694669723511
    },
    {
      "classification_loss": 0.5767861604690552,
      "epoch": 3.1508196721311474,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 961,
      "total_loss": 0.5767861604690552
    },
    {
      "classification_loss": 0.5827582478523254,
      "epoch": 3.1540983606557376,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 962,
      "total_loss": 0.5827582478523254
    },
    {
      "classification_loss": 0.6350870132446289,
      "epoch": 3.1573770491803277,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 963,
      "total_loss": 0.6350870132446289
    },
    {
      "classification_loss": 0.6064724922180176,
      "epoch": 3.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 964,
      "total_loss": 0.6064724922180176
    },
    {
      "classification_loss": 0.5499287843704224,
      "epoch": 3.1639344262295084,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 965,
      "total_loss": 0.5499287843704224
    },
    {
      "classification_loss": 0.6468144655227661,
      "epoch": 3.1672131147540985,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 966,
      "total_loss": 0.6468144655227661
    },
    {
      "classification_loss": 0.521653950214386,
      "epoch": 3.1704918032786886,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 967,
      "total_loss": 0.521653950214386
    },
    {
      "classification_loss": 0.6805787086486816,
      "epoch": 3.1737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 968,
      "total_loss": 0.6805787086486816
    },
    {
      "classification_loss": 0.6390178799629211,
      "epoch": 3.177049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 969,
      "total_loss": 0.6390178799629211
    },
    {
      "classification_loss": 0.6033177971839905,
      "epoch": 3.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 970,
      "total_loss": 0.6033177971839905
    },
    {
      "classification_loss": 0.6594963073730469,
      "epoch": 3.183606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 971,
      "total_loss": 0.6594963073730469
    },
    {
      "classification_loss": 0.5286180377006531,
      "epoch": 3.1868852459016392,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 972,
      "total_loss": 0.5286180377006531
    },
    {
      "classification_loss": 0.7068979144096375,
      "epoch": 3.1901639344262294,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 973,
      "total_loss": 0.7068979144096375
    },
    {
      "classification_loss": 0.5527566075325012,
      "epoch": 3.1934426229508195,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 974,
      "total_loss": 0.5527566075325012
    },
    {
      "classification_loss": 0.6204661726951599,
      "epoch": 3.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 975,
      "total_loss": 0.6204661726951599
    },
    {
      "classification_loss": 0.6108192801475525,
      "epoch": 3.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 976,
      "total_loss": 0.6108192801475525
    },
    {
      "classification_loss": 0.5753327012062073,
      "epoch": 3.2032786885245903,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 977,
      "total_loss": 0.5753327012062073
    },
    {
      "classification_loss": 0.6134668588638306,
      "epoch": 3.2065573770491804,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 978,
      "total_loss": 0.6134668588638306
    },
    {
      "classification_loss": 0.6153162121772766,
      "epoch": 3.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 979,
      "total_loss": 0.6153162121772766
    },
    {
      "classification_loss": 0.5831570029258728,
      "epoch": 3.2131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 980,
      "total_loss": 0.5831570029258728
    },
    {
      "classification_loss": 0.6439107060432434,
      "epoch": 3.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 981,
      "total_loss": 0.6439107060432434
    },
    {
      "classification_loss": 0.6118214726448059,
      "epoch": 3.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 982,
      "total_loss": 0.6118214726448059
    },
    {
      "classification_loss": 0.6288046836853027,
      "epoch": 3.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 983,
      "total_loss": 0.6288046836853027
    },
    {
      "classification_loss": 0.6458501815795898,
      "epoch": 3.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 984,
      "total_loss": 0.6458501815795898
    },
    {
      "classification_loss": 0.5964792370796204,
      "epoch": 3.2295081967213113,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 985,
      "total_loss": 0.5964792370796204
    },
    {
      "classification_loss": 0.6127866506576538,
      "epoch": 3.2327868852459014,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 986,
      "total_loss": 0.6127866506576538
    },
    {
      "classification_loss": 0.6554199457168579,
      "epoch": 3.236065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 987,
      "total_loss": 0.6554199457168579
    },
    {
      "classification_loss": 0.6423786282539368,
      "epoch": 3.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 988,
      "total_loss": 0.6423786282539368
    },
    {
      "classification_loss": 0.5882832407951355,
      "epoch": 3.2426229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 989,
      "total_loss": 0.5882832407951355
    },
    {
      "classification_loss": 0.5678666234016418,
      "epoch": 3.2459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 990,
      "total_loss": 0.5678666234016418
    },
    {
      "classification_loss": 0.6802240610122681,
      "epoch": 3.2491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 991,
      "total_loss": 0.6802240610122681
    },
    {
      "classification_loss": 0.543483555316925,
      "epoch": 3.2524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 992,
      "total_loss": 0.543483555316925
    },
    {
      "classification_loss": 0.6963410377502441,
      "epoch": 3.2557377049180327,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 993,
      "total_loss": 0.6963410377502441
    },
    {
      "classification_loss": 0.6023328304290771,
      "epoch": 3.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 994,
      "total_loss": 0.6023328304290771
    },
    {
      "classification_loss": 0.5815756916999817,
      "epoch": 3.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 995,
      "total_loss": 0.5815756916999817
    },
    {
      "classification_loss": 0.5731878280639648,
      "epoch": 3.265573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 996,
      "total_loss": 0.5731878280639648
    },
    {
      "classification_loss": 0.5729439854621887,
      "epoch": 3.2688524590163937,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 997,
      "total_loss": 0.5729439854621887
    },
    {
      "classification_loss": 0.6187911033630371,
      "epoch": 3.2721311475409838,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 998,
      "total_loss": 0.6187911033630371
    },
    {
      "classification_loss": 0.5907732248306274,
      "epoch": 3.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 999,
      "total_loss": 0.5907732248306274
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 5.53153657913208,
      "learning_rate": 0.00017003333333333334,
      "loss": 0.6211,
      "step": 1000
    },
    {
      "classification_loss": 0.6306383013725281,
      "epoch": 3.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1000,
      "total_loss": 0.6306383013725281
    },
    {
      "classification_loss": 0.6699001789093018,
      "epoch": 3.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1001,
      "total_loss": 0.6699001789093018
    },
    {
      "classification_loss": 0.6348069310188293,
      "epoch": 3.2852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1002,
      "total_loss": 0.6348069310188293
    },
    {
      "classification_loss": 0.7274527549743652,
      "epoch": 3.2885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1003,
      "total_loss": 0.7274527549743652
    },
    {
      "classification_loss": 0.7048348784446716,
      "epoch": 3.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1004,
      "total_loss": 0.7048348784446716
    },
    {
      "classification_loss": 0.6323686242103577,
      "epoch": 3.2950819672131146,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1005,
      "total_loss": 0.6323686242103577
    },
    {
      "classification_loss": 0.7063965797424316,
      "epoch": 3.2983606557377048,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1006,
      "total_loss": 0.7063965797424316
    },
    {
      "classification_loss": 0.6382759213447571,
      "epoch": 3.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1007,
      "total_loss": 0.6382759213447571
    },
    {
      "classification_loss": 0.5665464401245117,
      "epoch": 3.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1008,
      "total_loss": 0.5665464401245117
    },
    {
      "classification_loss": 0.6729834675788879,
      "epoch": 3.3081967213114756,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1009,
      "total_loss": 0.6729834675788879
    },
    {
      "classification_loss": 0.695097029209137,
      "epoch": 3.3114754098360657,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1010,
      "total_loss": 0.695097029209137
    },
    {
      "classification_loss": 0.6395645141601562,
      "epoch": 3.314754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1011,
      "total_loss": 0.6395645141601562
    },
    {
      "classification_loss": 0.637386679649353,
      "epoch": 3.318032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1012,
      "total_loss": 0.637386679649353
    },
    {
      "classification_loss": 0.6634384393692017,
      "epoch": 3.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1013,
      "total_loss": 0.6634384393692017
    },
    {
      "classification_loss": 0.6133981943130493,
      "epoch": 3.324590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1014,
      "total_loss": 0.6133981943130493
    },
    {
      "classification_loss": 0.6239176988601685,
      "epoch": 3.3278688524590163,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1015,
      "total_loss": 0.6239176988601685
    },
    {
      "classification_loss": 0.5201412439346313,
      "epoch": 3.3311475409836064,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1016,
      "total_loss": 0.5201412439346313
    },
    {
      "classification_loss": 0.6052525043487549,
      "epoch": 3.3344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1017,
      "total_loss": 0.6052525043487549
    },
    {
      "classification_loss": 0.584935188293457,
      "epoch": 3.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1018,
      "total_loss": 0.584935188293457
    },
    {
      "classification_loss": 0.6371868848800659,
      "epoch": 3.3409836065573773,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1019,
      "total_loss": 0.6371868848800659
    },
    {
      "classification_loss": 0.596892237663269,
      "epoch": 3.3442622950819674,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1020,
      "total_loss": 0.596892237663269
    },
    {
      "classification_loss": 0.6822124123573303,
      "epoch": 3.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1021,
      "total_loss": 0.6822124123573303
    },
    {
      "classification_loss": 0.7165191769599915,
      "epoch": 3.3508196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1022,
      "total_loss": 0.7165191769599915
    },
    {
      "classification_loss": 0.6081511974334717,
      "epoch": 3.3540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1023,
      "total_loss": 0.6081511974334717
    },
    {
      "classification_loss": 0.6222435235977173,
      "epoch": 3.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1024,
      "total_loss": 0.6222435235977173
    },
    {
      "classification_loss": 0.6680335998535156,
      "epoch": 3.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1025,
      "total_loss": 0.6680335998535156
    },
    {
      "classification_loss": 0.6703540086746216,
      "epoch": 3.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1026,
      "total_loss": 0.6703540086746216
    },
    {
      "classification_loss": 0.6134849190711975,
      "epoch": 3.3672131147540982,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1027,
      "total_loss": 0.6134849190711975
    },
    {
      "classification_loss": 0.5712637901306152,
      "epoch": 3.3704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1028,
      "total_loss": 0.5712637901306152
    },
    {
      "classification_loss": 0.6804757714271545,
      "epoch": 3.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1029,
      "total_loss": 0.6804757714271545
    },
    {
      "classification_loss": 0.5528501272201538,
      "epoch": 3.3770491803278686,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1030,
      "total_loss": 0.5528501272201538
    },
    {
      "classification_loss": 0.5479506850242615,
      "epoch": 3.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1031,
      "total_loss": 0.5479506850242615
    },
    {
      "classification_loss": 0.5959928035736084,
      "epoch": 3.3836065573770493,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1032,
      "total_loss": 0.5959928035736084
    },
    {
      "classification_loss": 0.5815926194190979,
      "epoch": 3.3868852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1033,
      "total_loss": 0.5815926194190979
    },
    {
      "classification_loss": 0.6203131079673767,
      "epoch": 3.3901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1034,
      "total_loss": 0.6203131079673767
    },
    {
      "classification_loss": 0.6791132688522339,
      "epoch": 3.3934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1035,
      "total_loss": 0.6791132688522339
    },
    {
      "classification_loss": 0.6327658295631409,
      "epoch": 3.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1036,
      "total_loss": 0.6327658295631409
    },
    {
      "classification_loss": 0.6739094853401184,
      "epoch": 3.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1037,
      "total_loss": 0.6739094853401184
    },
    {
      "classification_loss": 0.6896265149116516,
      "epoch": 3.40327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1038,
      "total_loss": 0.6896265149116516
    },
    {
      "classification_loss": 0.6339268684387207,
      "epoch": 3.40655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1039,
      "total_loss": 0.6339268684387207
    },
    {
      "classification_loss": 0.6379804611206055,
      "epoch": 3.4098360655737707,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1040,
      "total_loss": 0.6379804611206055
    },
    {
      "classification_loss": 0.5847087502479553,
      "epoch": 3.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1041,
      "total_loss": 0.5847087502479553
    },
    {
      "classification_loss": 0.6166040301322937,
      "epoch": 3.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1042,
      "total_loss": 0.6166040301322937
    },
    {
      "classification_loss": 0.5847687125205994,
      "epoch": 3.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1043,
      "total_loss": 0.5847687125205994
    },
    {
      "classification_loss": 0.6352771520614624,
      "epoch": 3.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1044,
      "total_loss": 0.6352771520614624
    },
    {
      "classification_loss": 0.6246709823608398,
      "epoch": 3.4262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1045,
      "total_loss": 0.6246709823608398
    },
    {
      "classification_loss": 0.635512113571167,
      "epoch": 3.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1046,
      "total_loss": 0.635512113571167
    },
    {
      "classification_loss": 0.671753466129303,
      "epoch": 3.4327868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1047,
      "total_loss": 0.671753466129303
    },
    {
      "classification_loss": 0.691548764705658,
      "epoch": 3.4360655737704917,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1048,
      "total_loss": 0.691548764705658
    },
    {
      "classification_loss": 0.6960029006004333,
      "epoch": 3.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1049,
      "total_loss": 0.6960029006004333
    },
    {
      "classification_loss": 0.5964454412460327,
      "epoch": 3.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1050,
      "total_loss": 0.5964454412460327
    },
    {
      "classification_loss": 0.578647792339325,
      "epoch": 3.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1051,
      "total_loss": 0.578647792339325
    },
    {
      "classification_loss": 0.6500298976898193,
      "epoch": 3.4491803278688526,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1052,
      "total_loss": 0.6500298976898193
    },
    {
      "classification_loss": 0.6424834132194519,
      "epoch": 3.4524590163934428,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1053,
      "total_loss": 0.6424834132194519
    },
    {
      "classification_loss": 0.6384720802307129,
      "epoch": 3.455737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1054,
      "total_loss": 0.6384720802307129
    },
    {
      "classification_loss": 0.6169339418411255,
      "epoch": 3.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1055,
      "total_loss": 0.6169339418411255
    },
    {
      "classification_loss": 0.6252519488334656,
      "epoch": 3.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1056,
      "total_loss": 0.6252519488334656
    },
    {
      "classification_loss": 0.6755489706993103,
      "epoch": 3.4655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1057,
      "total_loss": 0.6755489706993103
    },
    {
      "classification_loss": 0.6152545213699341,
      "epoch": 3.4688524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1058,
      "total_loss": 0.6152545213699341
    },
    {
      "classification_loss": 0.6246526837348938,
      "epoch": 3.4721311475409835,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1059,
      "total_loss": 0.6246526837348938
    },
    {
      "classification_loss": 0.5910590291023254,
      "epoch": 3.4754098360655736,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1060,
      "total_loss": 0.5910590291023254
    },
    {
      "classification_loss": 0.5544180870056152,
      "epoch": 3.4786885245901638,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1061,
      "total_loss": 0.5544180870056152
    },
    {
      "classification_loss": 0.5343322157859802,
      "epoch": 3.4819672131147543,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1062,
      "total_loss": 0.5343322157859802
    },
    {
      "classification_loss": 0.6027756333351135,
      "epoch": 3.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1063,
      "total_loss": 0.6027756333351135
    },
    {
      "classification_loss": 0.6711416244506836,
      "epoch": 3.4885245901639346,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1064,
      "total_loss": 0.6711416244506836
    },
    {
      "classification_loss": 0.6919649243354797,
      "epoch": 3.4918032786885247,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1065,
      "total_loss": 0.6919649243354797
    },
    {
      "classification_loss": 0.7710225582122803,
      "epoch": 3.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1066,
      "total_loss": 0.7710225582122803
    },
    {
      "classification_loss": 0.7296631932258606,
      "epoch": 3.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1067,
      "total_loss": 0.7296631932258606
    },
    {
      "classification_loss": 0.6448307037353516,
      "epoch": 3.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1068,
      "total_loss": 0.6448307037353516
    },
    {
      "classification_loss": 0.6578924655914307,
      "epoch": 3.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1069,
      "total_loss": 0.6578924655914307
    },
    {
      "classification_loss": 0.6385699510574341,
      "epoch": 3.5081967213114753,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1070,
      "total_loss": 0.6385699510574341
    },
    {
      "classification_loss": 0.5746889710426331,
      "epoch": 3.5114754098360654,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1071,
      "total_loss": 0.5746889710426331
    },
    {
      "classification_loss": 0.6259164214134216,
      "epoch": 3.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1072,
      "total_loss": 0.6259164214134216
    },
    {
      "classification_loss": 0.5777879357337952,
      "epoch": 3.5180327868852457,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1073,
      "total_loss": 0.5777879357337952
    },
    {
      "classification_loss": 0.5789260864257812,
      "epoch": 3.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1074,
      "total_loss": 0.5789260864257812
    },
    {
      "classification_loss": 0.6240569353103638,
      "epoch": 3.5245901639344264,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1075,
      "total_loss": 0.6240569353103638
    },
    {
      "classification_loss": 0.6227736473083496,
      "epoch": 3.5278688524590165,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1076,
      "total_loss": 0.6227736473083496
    },
    {
      "classification_loss": 0.6418671607971191,
      "epoch": 3.5311475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1077,
      "total_loss": 0.6418671607971191
    },
    {
      "classification_loss": 0.6324038505554199,
      "epoch": 3.5344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1078,
      "total_loss": 0.6324038505554199
    },
    {
      "classification_loss": 0.5862008333206177,
      "epoch": 3.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1079,
      "total_loss": 0.5862008333206177
    },
    {
      "classification_loss": 0.6539157629013062,
      "epoch": 3.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1080,
      "total_loss": 0.6539157629013062
    },
    {
      "classification_loss": 0.580960214138031,
      "epoch": 3.544262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1081,
      "total_loss": 0.580960214138031
    },
    {
      "classification_loss": 0.6045177578926086,
      "epoch": 3.5475409836065572,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1082,
      "total_loss": 0.6045177578926086
    },
    {
      "classification_loss": 0.5970606803894043,
      "epoch": 3.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1083,
      "total_loss": 0.5970606803894043
    },
    {
      "classification_loss": 0.6491187810897827,
      "epoch": 3.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1084,
      "total_loss": 0.6491187810897827
    },
    {
      "classification_loss": 0.6440609693527222,
      "epoch": 3.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1085,
      "total_loss": 0.6440609693527222
    },
    {
      "classification_loss": 0.6120240092277527,
      "epoch": 3.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1086,
      "total_loss": 0.6120240092277527
    },
    {
      "classification_loss": 0.6401466131210327,
      "epoch": 3.5639344262295083,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1087,
      "total_loss": 0.6401466131210327
    },
    {
      "classification_loss": 0.6333215832710266,
      "epoch": 3.5672131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1088,
      "total_loss": 0.6333215832710266
    },
    {
      "classification_loss": 0.6450108289718628,
      "epoch": 3.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1089,
      "total_loss": 0.6450108289718628
    },
    {
      "classification_loss": 0.6707785129547119,
      "epoch": 3.5737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1090,
      "total_loss": 0.6707785129547119
    },
    {
      "classification_loss": 0.5530345439910889,
      "epoch": 3.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1091,
      "total_loss": 0.5530345439910889
    },
    {
      "classification_loss": 0.591323733329773,
      "epoch": 3.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1092,
      "total_loss": 0.591323733329773
    },
    {
      "classification_loss": 0.6768133044242859,
      "epoch": 3.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1093,
      "total_loss": 0.6768133044242859
    },
    {
      "classification_loss": 0.7086431980133057,
      "epoch": 3.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1094,
      "total_loss": 0.7086431980133057
    },
    {
      "classification_loss": 0.5802810788154602,
      "epoch": 3.5901639344262293,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1095,
      "total_loss": 0.5802810788154602
    },
    {
      "classification_loss": 0.6960800886154175,
      "epoch": 3.5934426229508194,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1096,
      "total_loss": 0.6960800886154175
    },
    {
      "classification_loss": 0.6731969714164734,
      "epoch": 3.59672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1097,
      "total_loss": 0.6731969714164734
    },
    {
      "classification_loss": 0.6278107762336731,
      "epoch": 3.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1098,
      "total_loss": 0.6278107762336731
    },
    {
      "classification_loss": 0.6283719539642334,
      "epoch": 3.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1099,
      "total_loss": 0.6283719539642334
    },
    {
      "epoch": 3.6065573770491803,
      "grad_norm": 2.5482726097106934,
      "learning_rate": 0.0001667,
      "loss": 0.633,
      "step": 1100
    },
    {
      "classification_loss": 0.6095505356788635,
      "epoch": 3.6065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1100,
      "total_loss": 0.6095505356788635
    },
    {
      "classification_loss": 0.625938355922699,
      "epoch": 3.6098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1101,
      "total_loss": 0.625938355922699
    },
    {
      "classification_loss": 0.6475554704666138,
      "epoch": 3.6131147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1102,
      "total_loss": 0.6475554704666138
    },
    {
      "classification_loss": 0.6618005633354187,
      "epoch": 3.6163934426229507,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1103,
      "total_loss": 0.6618005633354187
    },
    {
      "classification_loss": 0.6485204100608826,
      "epoch": 3.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1104,
      "total_loss": 0.6485204100608826
    },
    {
      "classification_loss": 0.6137438416481018,
      "epoch": 3.6229508196721314,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1105,
      "total_loss": 0.6137438416481018
    },
    {
      "classification_loss": 0.6468526124954224,
      "epoch": 3.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1106,
      "total_loss": 0.6468526124954224
    },
    {
      "classification_loss": 0.6746812462806702,
      "epoch": 3.6295081967213116,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1107,
      "total_loss": 0.6746812462806702
    },
    {
      "classification_loss": 0.6366666555404663,
      "epoch": 3.6327868852459018,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1108,
      "total_loss": 0.6366666555404663
    },
    {
      "classification_loss": 0.6180174946784973,
      "epoch": 3.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1109,
      "total_loss": 0.6180174946784973
    },
    {
      "classification_loss": 0.7323210835456848,
      "epoch": 3.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1110,
      "total_loss": 0.7323210835456848
    },
    {
      "classification_loss": 0.6590652465820312,
      "epoch": 3.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1111,
      "total_loss": 0.6590652465820312
    },
    {
      "classification_loss": 0.61629718542099,
      "epoch": 3.6459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1112,
      "total_loss": 0.61629718542099
    },
    {
      "classification_loss": 0.6367692947387695,
      "epoch": 3.6491803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1113,
      "total_loss": 0.6367692947387695
    },
    {
      "classification_loss": 0.586384117603302,
      "epoch": 3.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1114,
      "total_loss": 0.586384117603302
    },
    {
      "classification_loss": 0.6681204438209534,
      "epoch": 3.6557377049180326,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1115,
      "total_loss": 0.6681204438209534
    },
    {
      "classification_loss": 0.6824667453765869,
      "epoch": 3.6590163934426227,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1116,
      "total_loss": 0.6824667453765869
    },
    {
      "classification_loss": 0.5717865824699402,
      "epoch": 3.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1117,
      "total_loss": 0.5717865824699402
    },
    {
      "classification_loss": 0.5906731486320496,
      "epoch": 3.6655737704918034,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1118,
      "total_loss": 0.5906731486320496
    },
    {
      "classification_loss": 0.5703739523887634,
      "epoch": 3.6688524590163936,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1119,
      "total_loss": 0.5703739523887634
    },
    {
      "classification_loss": 0.6369026303291321,
      "epoch": 3.6721311475409837,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1120,
      "total_loss": 0.6369026303291321
    },
    {
      "classification_loss": 0.6672720909118652,
      "epoch": 3.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1121,
      "total_loss": 0.6672720909118652
    },
    {
      "classification_loss": 0.6268528699874878,
      "epoch": 3.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1122,
      "total_loss": 0.6268528699874878
    },
    {
      "classification_loss": 0.6185177564620972,
      "epoch": 3.681967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1123,
      "total_loss": 0.6185177564620972
    },
    {
      "classification_loss": 0.6006613373756409,
      "epoch": 3.685245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1124,
      "total_loss": 0.6006613373756409
    },
    {
      "classification_loss": 0.5987418293952942,
      "epoch": 3.6885245901639343,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1125,
      "total_loss": 0.5987418293952942
    },
    {
      "classification_loss": 0.6049272418022156,
      "epoch": 3.6918032786885244,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1126,
      "total_loss": 0.6049272418022156
    },
    {
      "classification_loss": 0.5849695205688477,
      "epoch": 3.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1127,
      "total_loss": 0.5849695205688477
    },
    {
      "classification_loss": 0.638906717300415,
      "epoch": 3.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1128,
      "total_loss": 0.638906717300415
    },
    {
      "classification_loss": 0.6042098999023438,
      "epoch": 3.7016393442622952,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1129,
      "total_loss": 0.6042098999023438
    },
    {
      "classification_loss": 0.5965882539749146,
      "epoch": 3.7049180327868854,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1130,
      "total_loss": 0.5965882539749146
    },
    {
      "classification_loss": 0.6358504295349121,
      "epoch": 3.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1131,
      "total_loss": 0.6358504295349121
    },
    {
      "classification_loss": 0.6960458159446716,
      "epoch": 3.7114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1132,
      "total_loss": 0.6960458159446716
    },
    {
      "classification_loss": 0.5966853499412537,
      "epoch": 3.7147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1133,
      "total_loss": 0.5966853499412537
    },
    {
      "classification_loss": 0.6270977258682251,
      "epoch": 3.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1134,
      "total_loss": 0.6270977258682251
    },
    {
      "classification_loss": 0.6230767369270325,
      "epoch": 3.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1135,
      "total_loss": 0.6230767369270325
    },
    {
      "classification_loss": 0.5947087407112122,
      "epoch": 3.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1136,
      "total_loss": 0.5947087407112122
    },
    {
      "classification_loss": 0.6746556162834167,
      "epoch": 3.7278688524590162,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1137,
      "total_loss": 0.6746556162834167
    },
    {
      "classification_loss": 0.5611866116523743,
      "epoch": 3.7311475409836063,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1138,
      "total_loss": 0.5611866116523743
    },
    {
      "classification_loss": 0.6133644580841064,
      "epoch": 3.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1139,
      "total_loss": 0.6133644580841064
    },
    {
      "classification_loss": 0.7137611508369446,
      "epoch": 3.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1140,
      "total_loss": 0.7137611508369446
    },
    {
      "classification_loss": 0.6152480244636536,
      "epoch": 3.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1141,
      "total_loss": 0.6152480244636536
    },
    {
      "classification_loss": 0.6030460000038147,
      "epoch": 3.7442622950819673,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1142,
      "total_loss": 0.6030460000038147
    },
    {
      "classification_loss": 0.5855371356010437,
      "epoch": 3.7475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1143,
      "total_loss": 0.5855371356010437
    },
    {
      "classification_loss": 0.5615608096122742,
      "epoch": 3.7508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1144,
      "total_loss": 0.5615608096122742
    },
    {
      "classification_loss": 0.5946676135063171,
      "epoch": 3.7540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1145,
      "total_loss": 0.5946676135063171
    },
    {
      "classification_loss": 0.6503653526306152,
      "epoch": 3.7573770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1146,
      "total_loss": 0.6503653526306152
    },
    {
      "classification_loss": 0.605129599571228,
      "epoch": 3.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1147,
      "total_loss": 0.605129599571228
    },
    {
      "classification_loss": 0.6871640682220459,
      "epoch": 3.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1148,
      "total_loss": 0.6871640682220459
    },
    {
      "classification_loss": 0.6971414685249329,
      "epoch": 3.7672131147540986,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1149,
      "total_loss": 0.6971414685249329
    },
    {
      "classification_loss": 0.6062591075897217,
      "epoch": 3.7704918032786887,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1150,
      "total_loss": 0.6062591075897217
    },
    {
      "classification_loss": 0.5507655143737793,
      "epoch": 3.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1151,
      "total_loss": 0.5507655143737793
    },
    {
      "classification_loss": 0.6464734673500061,
      "epoch": 3.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1152,
      "total_loss": 0.6464734673500061
    },
    {
      "classification_loss": 0.6584529280662537,
      "epoch": 3.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1153,
      "total_loss": 0.6584529280662537
    },
    {
      "classification_loss": 0.6397233009338379,
      "epoch": 3.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1154,
      "total_loss": 0.6397233009338379
    },
    {
      "classification_loss": 0.5783952474594116,
      "epoch": 3.7868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1155,
      "total_loss": 0.5783952474594116
    },
    {
      "classification_loss": 0.6954275369644165,
      "epoch": 3.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1156,
      "total_loss": 0.6954275369644165
    },
    {
      "classification_loss": 0.5760709643363953,
      "epoch": 3.7934426229508196,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1157,
      "total_loss": 0.5760709643363953
    },
    {
      "classification_loss": 0.5879453420639038,
      "epoch": 3.7967213114754097,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1158,
      "total_loss": 0.5879453420639038
    },
    {
      "classification_loss": 0.6791242957115173,
      "epoch": 3.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1159,
      "total_loss": 0.6791242957115173
    },
    {
      "classification_loss": 0.6437327265739441,
      "epoch": 3.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1160,
      "total_loss": 0.6437327265739441
    },
    {
      "classification_loss": 0.6242661476135254,
      "epoch": 3.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1161,
      "total_loss": 0.6242661476135254
    },
    {
      "classification_loss": 0.5995579957962036,
      "epoch": 3.8098360655737706,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1162,
      "total_loss": 0.5995579957962036
    },
    {
      "classification_loss": 0.582169234752655,
      "epoch": 3.8131147540983608,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1163,
      "total_loss": 0.582169234752655
    },
    {
      "classification_loss": 0.7383379936218262,
      "epoch": 3.816393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1164,
      "total_loss": 0.7383379936218262
    },
    {
      "classification_loss": 0.5368362069129944,
      "epoch": 3.819672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1165,
      "total_loss": 0.5368362069129944
    },
    {
      "classification_loss": 0.6785664558410645,
      "epoch": 3.822950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1166,
      "total_loss": 0.6785664558410645
    },
    {
      "classification_loss": 0.6297599077224731,
      "epoch": 3.8262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1167,
      "total_loss": 0.6297599077224731
    },
    {
      "classification_loss": 0.5851531624794006,
      "epoch": 3.8295081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1168,
      "total_loss": 0.5851531624794006
    },
    {
      "classification_loss": 0.6387592554092407,
      "epoch": 3.8327868852459015,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1169,
      "total_loss": 0.6387592554092407
    },
    {
      "classification_loss": 0.5942277908325195,
      "epoch": 3.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1170,
      "total_loss": 0.5942277908325195
    },
    {
      "classification_loss": 0.6418381333351135,
      "epoch": 3.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1171,
      "total_loss": 0.6418381333351135
    },
    {
      "classification_loss": 0.5826167464256287,
      "epoch": 3.8426229508196723,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1172,
      "total_loss": 0.5826167464256287
    },
    {
      "classification_loss": 0.6670812368392944,
      "epoch": 3.8459016393442624,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1173,
      "total_loss": 0.6670812368392944
    },
    {
      "classification_loss": 0.6416741013526917,
      "epoch": 3.8491803278688526,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1174,
      "total_loss": 0.6416741013526917
    },
    {
      "classification_loss": 0.586075484752655,
      "epoch": 3.8524590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1175,
      "total_loss": 0.586075484752655
    },
    {
      "classification_loss": 0.6535174250602722,
      "epoch": 3.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1176,
      "total_loss": 0.6535174250602722
    },
    {
      "classification_loss": 0.6581873893737793,
      "epoch": 3.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1177,
      "total_loss": 0.6581873893737793
    },
    {
      "classification_loss": 0.670222282409668,
      "epoch": 3.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1178,
      "total_loss": 0.670222282409668
    },
    {
      "classification_loss": 0.6480275392532349,
      "epoch": 3.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1179,
      "total_loss": 0.6480275392532349
    },
    {
      "classification_loss": 0.5734622478485107,
      "epoch": 3.8688524590163933,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1180,
      "total_loss": 0.5734622478485107
    },
    {
      "classification_loss": 0.5949715375900269,
      "epoch": 3.8721311475409834,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1181,
      "total_loss": 0.5949715375900269
    },
    {
      "classification_loss": 0.6463958621025085,
      "epoch": 3.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1182,
      "total_loss": 0.6463958621025085
    },
    {
      "classification_loss": 0.6758001446723938,
      "epoch": 3.8786885245901637,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1183,
      "total_loss": 0.6758001446723938
    },
    {
      "classification_loss": 0.6267231702804565,
      "epoch": 3.8819672131147542,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1184,
      "total_loss": 0.6267231702804565
    },
    {
      "classification_loss": 0.5824413299560547,
      "epoch": 3.8852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1185,
      "total_loss": 0.5824413299560547
    },
    {
      "classification_loss": 0.6618664860725403,
      "epoch": 3.8885245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1186,
      "total_loss": 0.6618664860725403
    },
    {
      "classification_loss": 0.6948312520980835,
      "epoch": 3.8918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1187,
      "total_loss": 0.6948312520980835
    },
    {
      "classification_loss": 0.6204938292503357,
      "epoch": 3.8950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1188,
      "total_loss": 0.6204938292503357
    },
    {
      "classification_loss": 0.62480628490448,
      "epoch": 3.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1189,
      "total_loss": 0.62480628490448
    },
    {
      "classification_loss": 0.6303942203521729,
      "epoch": 3.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1190,
      "total_loss": 0.6303942203521729
    },
    {
      "classification_loss": 0.6034244894981384,
      "epoch": 3.904918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1191,
      "total_loss": 0.6034244894981384
    },
    {
      "classification_loss": 0.6057844161987305,
      "epoch": 3.9081967213114757,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1192,
      "total_loss": 0.6057844161987305
    },
    {
      "classification_loss": 0.6032863259315491,
      "epoch": 3.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1193,
      "total_loss": 0.6032863259315491
    },
    {
      "classification_loss": 0.5982041358947754,
      "epoch": 3.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1194,
      "total_loss": 0.5982041358947754
    },
    {
      "classification_loss": 0.5758277773857117,
      "epoch": 3.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1195,
      "total_loss": 0.5758277773857117
    },
    {
      "classification_loss": 0.5942794680595398,
      "epoch": 3.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1196,
      "total_loss": 0.5942794680595398
    },
    {
      "classification_loss": 0.6323970556259155,
      "epoch": 3.9245901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1197,
      "total_loss": 0.6323970556259155
    },
    {
      "classification_loss": 0.7095960378646851,
      "epoch": 3.9278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1198,
      "total_loss": 0.7095960378646851
    },
    {
      "classification_loss": 0.6213348507881165,
      "epoch": 3.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1199,
      "total_loss": 0.6213348507881165
    },
    {
      "epoch": 3.9344262295081966,
      "grad_norm": 4.178915977478027,
      "learning_rate": 0.00016336666666666666,
      "loss": 0.6271,
      "step": 1200
    },
    {
      "classification_loss": 0.6185882091522217,
      "epoch": 3.9344262295081966,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1200,
      "total_loss": 0.6185882091522217
    },
    {
      "classification_loss": 0.6030643582344055,
      "epoch": 3.9377049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1201,
      "total_loss": 0.6030643582344055
    },
    {
      "classification_loss": 0.636794924736023,
      "epoch": 3.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1202,
      "total_loss": 0.636794924736023
    },
    {
      "classification_loss": 0.6624141931533813,
      "epoch": 3.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1203,
      "total_loss": 0.6624141931533813
    },
    {
      "classification_loss": 0.6549594402313232,
      "epoch": 3.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1204,
      "total_loss": 0.6549594402313232
    },
    {
      "classification_loss": 0.641276478767395,
      "epoch": 3.9508196721311473,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1205,
      "total_loss": 0.641276478767395
    },
    {
      "classification_loss": 0.6116706728935242,
      "epoch": 3.954098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1206,
      "total_loss": 0.6116706728935242
    },
    {
      "classification_loss": 0.5953999161720276,
      "epoch": 3.957377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1207,
      "total_loss": 0.5953999161720276
    },
    {
      "classification_loss": 0.63658607006073,
      "epoch": 3.960655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1208,
      "total_loss": 0.63658607006073
    },
    {
      "classification_loss": 0.5810570120811462,
      "epoch": 3.963934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1209,
      "total_loss": 0.5810570120811462
    },
    {
      "classification_loss": 0.6099850535392761,
      "epoch": 3.9672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1210,
      "total_loss": 0.6099850535392761
    },
    {
      "classification_loss": 0.6335389018058777,
      "epoch": 3.9704918032786884,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1211,
      "total_loss": 0.6335389018058777
    },
    {
      "classification_loss": 0.6076453924179077,
      "epoch": 3.9737704918032786,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1212,
      "total_loss": 0.6076453924179077
    },
    {
      "classification_loss": 0.7183029651641846,
      "epoch": 3.9770491803278687,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1213,
      "total_loss": 0.7183029651641846
    },
    {
      "classification_loss": 0.6700876951217651,
      "epoch": 3.9803278688524593,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1214,
      "total_loss": 0.6700876951217651
    },
    {
      "classification_loss": 0.5573686361312866,
      "epoch": 3.9836065573770494,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1215,
      "total_loss": 0.5573686361312866
    },
    {
      "classification_loss": 0.6862971186637878,
      "epoch": 3.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1216,
      "total_loss": 0.6862971186637878
    },
    {
      "classification_loss": 0.6296688914299011,
      "epoch": 3.9901639344262296,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1217,
      "total_loss": 0.6296688914299011
    },
    {
      "classification_loss": 0.6013713479042053,
      "epoch": 3.9934426229508198,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1218,
      "total_loss": 0.6013713479042053
    },
    {
      "classification_loss": 0.6257572174072266,
      "epoch": 3.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1219,
      "total_loss": 0.6257572174072266
    },
    {
      "classification_loss": 0.8212490677833557,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8212490677833557
    },
    {
      "classification_loss": 0.8217374086380005,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8217374086380005
    },
    {
      "classification_loss": 0.7754959464073181,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.7754959464073181
    },
    {
      "classification_loss": 0.8208010792732239,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8208010792732239
    },
    {
      "classification_loss": 0.8367973566055298,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8367973566055298
    },
    {
      "classification_loss": 0.7884128093719482,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.7884128093719482
    },
    {
      "classification_loss": 0.8238359093666077,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8238359093666077
    },
    {
      "classification_loss": 0.8569602370262146,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.8569602370262146
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.384,
      "eval_f1": 0.00964630225080386,
      "eval_loss": 0.8172301054000854,
      "eval_precision": 0.5,
      "eval_recall": 0.00487012987012987,
      "eval_runtime": 6.0374,
      "eval_samples_per_second": 165.633,
      "eval_steps_per_second": 1.325,
      "step": 1220
    },
    {
      "classification_loss": 0.5334285497665405,
      "epoch": 4.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1220,
      "total_loss": 0.5334285497665405
    },
    {
      "classification_loss": 0.5034911036491394,
      "epoch": 4.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1221,
      "total_loss": 0.5034911036491394
    },
    {
      "classification_loss": 0.6219667196273804,
      "epoch": 4.00655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1222,
      "total_loss": 0.6219667196273804
    },
    {
      "classification_loss": 0.6639007329940796,
      "epoch": 4.00983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1223,
      "total_loss": 0.6639007329940796
    },
    {
      "classification_loss": 0.5670825242996216,
      "epoch": 4.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1224,
      "total_loss": 0.5670825242996216
    },
    {
      "classification_loss": 0.6064474582672119,
      "epoch": 4.016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1225,
      "total_loss": 0.6064474582672119
    },
    {
      "classification_loss": 0.600990355014801,
      "epoch": 4.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1226,
      "total_loss": 0.600990355014801
    },
    {
      "classification_loss": 0.6724435687065125,
      "epoch": 4.022950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1227,
      "total_loss": 0.6724435687065125
    },
    {
      "classification_loss": 0.6395429372787476,
      "epoch": 4.026229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1228,
      "total_loss": 0.6395429372787476
    },
    {
      "classification_loss": 0.6191545128822327,
      "epoch": 4.029508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1229,
      "total_loss": 0.6191545128822327
    },
    {
      "classification_loss": 0.6119126081466675,
      "epoch": 4.032786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1230,
      "total_loss": 0.6119126081466675
    },
    {
      "classification_loss": 0.5728321671485901,
      "epoch": 4.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1231,
      "total_loss": 0.5728321671485901
    },
    {
      "classification_loss": 0.6168491840362549,
      "epoch": 4.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1232,
      "total_loss": 0.6168491840362549
    },
    {
      "classification_loss": 0.5322439670562744,
      "epoch": 4.0426229508196725,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1233,
      "total_loss": 0.5322439670562744
    },
    {
      "classification_loss": 0.6310785412788391,
      "epoch": 4.045901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1234,
      "total_loss": 0.6310785412788391
    },
    {
      "classification_loss": 0.5981759428977966,
      "epoch": 4.049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1235,
      "total_loss": 0.5981759428977966
    },
    {
      "classification_loss": 0.6710205078125,
      "epoch": 4.052459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1236,
      "total_loss": 0.6710205078125
    },
    {
      "classification_loss": 0.5856115818023682,
      "epoch": 4.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1237,
      "total_loss": 0.5856115818023682
    },
    {
      "classification_loss": 0.7196425795555115,
      "epoch": 4.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1238,
      "total_loss": 0.7196425795555115
    },
    {
      "classification_loss": 0.6252133846282959,
      "epoch": 4.062295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1239,
      "total_loss": 0.6252133846282959
    },
    {
      "classification_loss": 0.6539038419723511,
      "epoch": 4.065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1240,
      "total_loss": 0.6539038419723511
    },
    {
      "classification_loss": 0.6334773302078247,
      "epoch": 4.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1241,
      "total_loss": 0.6334773302078247
    },
    {
      "classification_loss": 0.617982804775238,
      "epoch": 4.072131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1242,
      "total_loss": 0.617982804775238
    },
    {
      "classification_loss": 0.5753446817398071,
      "epoch": 4.075409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1243,
      "total_loss": 0.5753446817398071
    },
    {
      "classification_loss": 0.5648188591003418,
      "epoch": 4.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1244,
      "total_loss": 0.5648188591003418
    },
    {
      "classification_loss": 0.5744845271110535,
      "epoch": 4.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1245,
      "total_loss": 0.5744845271110535
    },
    {
      "classification_loss": 0.6341339349746704,
      "epoch": 4.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1246,
      "total_loss": 0.6341339349746704
    },
    {
      "classification_loss": 0.6062183380126953,
      "epoch": 4.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1247,
      "total_loss": 0.6062183380126953
    },
    {
      "classification_loss": 0.5677247643470764,
      "epoch": 4.091803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1248,
      "total_loss": 0.5677247643470764
    },
    {
      "classification_loss": 0.7205540537834167,
      "epoch": 4.0950819672131145,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1249,
      "total_loss": 0.7205540537834167
    },
    {
      "classification_loss": 0.6465232372283936,
      "epoch": 4.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1250,
      "total_loss": 0.6465232372283936
    },
    {
      "classification_loss": 0.6824342608451843,
      "epoch": 4.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1251,
      "total_loss": 0.6824342608451843
    },
    {
      "classification_loss": 0.7030136585235596,
      "epoch": 4.104918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1252,
      "total_loss": 0.7030136585235596
    },
    {
      "classification_loss": 0.5458102226257324,
      "epoch": 4.108196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1253,
      "total_loss": 0.5458102226257324
    },
    {
      "classification_loss": 0.6348831653594971,
      "epoch": 4.111475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1254,
      "total_loss": 0.6348831653594971
    },
    {
      "classification_loss": 0.6759592890739441,
      "epoch": 4.114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1255,
      "total_loss": 0.6759592890739441
    },
    {
      "classification_loss": 0.6993638873100281,
      "epoch": 4.118032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1256,
      "total_loss": 0.6993638873100281
    },
    {
      "classification_loss": 0.6387314200401306,
      "epoch": 4.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1257,
      "total_loss": 0.6387314200401306
    },
    {
      "classification_loss": 0.6291816830635071,
      "epoch": 4.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1258,
      "total_loss": 0.6291816830635071
    },
    {
      "classification_loss": 0.5713538527488708,
      "epoch": 4.127868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1259,
      "total_loss": 0.5713538527488708
    },
    {
      "classification_loss": 0.589431881904602,
      "epoch": 4.131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1260,
      "total_loss": 0.589431881904602
    },
    {
      "classification_loss": 0.5916886925697327,
      "epoch": 4.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1261,
      "total_loss": 0.5916886925697327
    },
    {
      "classification_loss": 0.7394014596939087,
      "epoch": 4.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1262,
      "total_loss": 0.7394014596939087
    },
    {
      "classification_loss": 0.5497375130653381,
      "epoch": 4.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1263,
      "total_loss": 0.5497375130653381
    },
    {
      "classification_loss": 0.668899655342102,
      "epoch": 4.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1264,
      "total_loss": 0.668899655342102
    },
    {
      "classification_loss": 0.5826314091682434,
      "epoch": 4.147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1265,
      "total_loss": 0.5826314091682434
    },
    {
      "classification_loss": 0.6912441849708557,
      "epoch": 4.150819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1266,
      "total_loss": 0.6912441849708557
    },
    {
      "classification_loss": 0.5986135601997375,
      "epoch": 4.154098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1267,
      "total_loss": 0.5986135601997375
    },
    {
      "classification_loss": 0.587160587310791,
      "epoch": 4.157377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1268,
      "total_loss": 0.587160587310791
    },
    {
      "classification_loss": 0.6477934718132019,
      "epoch": 4.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1269,
      "total_loss": 0.6477934718132019
    },
    {
      "classification_loss": 0.6364204287528992,
      "epoch": 4.163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1270,
      "total_loss": 0.6364204287528992
    },
    {
      "classification_loss": 0.5965926647186279,
      "epoch": 4.167213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1271,
      "total_loss": 0.5965926647186279
    },
    {
      "classification_loss": 0.627007246017456,
      "epoch": 4.170491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1272,
      "total_loss": 0.627007246017456
    },
    {
      "classification_loss": 0.6698298454284668,
      "epoch": 4.173770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1273,
      "total_loss": 0.6698298454284668
    },
    {
      "classification_loss": 0.5672253370285034,
      "epoch": 4.177049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1274,
      "total_loss": 0.5672253370285034
    },
    {
      "classification_loss": 0.5495153665542603,
      "epoch": 4.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1275,
      "total_loss": 0.5495153665542603
    },
    {
      "classification_loss": 0.6188547611236572,
      "epoch": 4.18360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1276,
      "total_loss": 0.6188547611236572
    },
    {
      "classification_loss": 0.5170352458953857,
      "epoch": 4.18688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1277,
      "total_loss": 0.5170352458953857
    },
    {
      "classification_loss": 0.6557828187942505,
      "epoch": 4.19016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1278,
      "total_loss": 0.6557828187942505
    },
    {
      "classification_loss": 0.5736050009727478,
      "epoch": 4.19344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1279,
      "total_loss": 0.5736050009727478
    },
    {
      "classification_loss": 0.6362711787223816,
      "epoch": 4.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1280,
      "total_loss": 0.6362711787223816
    },
    {
      "classification_loss": 0.6022069454193115,
      "epoch": 4.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1281,
      "total_loss": 0.6022069454193115
    },
    {
      "classification_loss": 0.5939323306083679,
      "epoch": 4.20327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1282,
      "total_loss": 0.5939323306083679
    },
    {
      "classification_loss": 0.6000717878341675,
      "epoch": 4.20655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1283,
      "total_loss": 0.6000717878341675
    },
    {
      "classification_loss": 0.6810312867164612,
      "epoch": 4.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1284,
      "total_loss": 0.6810312867164612
    },
    {
      "classification_loss": 0.5926063060760498,
      "epoch": 4.213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1285,
      "total_loss": 0.5926063060760498
    },
    {
      "classification_loss": 0.6172260642051697,
      "epoch": 4.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1286,
      "total_loss": 0.6172260642051697
    },
    {
      "classification_loss": 0.6978726387023926,
      "epoch": 4.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1287,
      "total_loss": 0.6978726387023926
    },
    {
      "classification_loss": 0.5408263206481934,
      "epoch": 4.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1288,
      "total_loss": 0.5408263206481934
    },
    {
      "classification_loss": 0.5898886919021606,
      "epoch": 4.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1289,
      "total_loss": 0.5898886919021606
    },
    {
      "classification_loss": 0.6914870142936707,
      "epoch": 4.229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1290,
      "total_loss": 0.6914870142936707
    },
    {
      "classification_loss": 0.6059459447860718,
      "epoch": 4.232786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1291,
      "total_loss": 0.6059459447860718
    },
    {
      "classification_loss": 0.6174843311309814,
      "epoch": 4.2360655737704915,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1292,
      "total_loss": 0.6174843311309814
    },
    {
      "classification_loss": 0.4993628263473511,
      "epoch": 4.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1293,
      "total_loss": 0.4993628263473511
    },
    {
      "classification_loss": 0.5477465987205505,
      "epoch": 4.242622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1294,
      "total_loss": 0.5477465987205505
    },
    {
      "classification_loss": 0.6513434648513794,
      "epoch": 4.245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1295,
      "total_loss": 0.6513434648513794
    },
    {
      "classification_loss": 0.6152762770652771,
      "epoch": 4.249180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1296,
      "total_loss": 0.6152762770652771
    },
    {
      "classification_loss": 0.6048642992973328,
      "epoch": 4.252459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1297,
      "total_loss": 0.6048642992973328
    },
    {
      "classification_loss": 0.649816632270813,
      "epoch": 4.255737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1298,
      "total_loss": 0.649816632270813
    },
    {
      "classification_loss": 0.6367855668067932,
      "epoch": 4.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1299,
      "total_loss": 0.6367855668067932
    },
    {
      "epoch": 4.262295081967213,
      "grad_norm": 1.664127230644226,
      "learning_rate": 0.00016003333333333334,
      "loss": 0.6188,
      "step": 1300
    },
    {
      "classification_loss": 0.6370482444763184,
      "epoch": 4.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1300,
      "total_loss": 0.6370482444763184
    },
    {
      "classification_loss": 0.5862963199615479,
      "epoch": 4.2655737704918035,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1301,
      "total_loss": 0.5862963199615479
    },
    {
      "classification_loss": 0.590988278388977,
      "epoch": 4.268852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1302,
      "total_loss": 0.590988278388977
    },
    {
      "classification_loss": 0.7040570378303528,
      "epoch": 4.272131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1303,
      "total_loss": 0.7040570378303528
    },
    {
      "classification_loss": 0.6336939930915833,
      "epoch": 4.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1304,
      "total_loss": 0.6336939930915833
    },
    {
      "classification_loss": 0.6391438841819763,
      "epoch": 4.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1305,
      "total_loss": 0.6391438841819763
    },
    {
      "classification_loss": 0.641226589679718,
      "epoch": 4.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1306,
      "total_loss": 0.641226589679718
    },
    {
      "classification_loss": 0.5670445561408997,
      "epoch": 4.285245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1307,
      "total_loss": 0.5670445561408997
    },
    {
      "classification_loss": 0.5640461444854736,
      "epoch": 4.288524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1308,
      "total_loss": 0.5640461444854736
    },
    {
      "classification_loss": 0.5655810236930847,
      "epoch": 4.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1309,
      "total_loss": 0.5655810236930847
    },
    {
      "classification_loss": 0.6442525386810303,
      "epoch": 4.295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1310,
      "total_loss": 0.6442525386810303
    },
    {
      "classification_loss": 0.699187695980072,
      "epoch": 4.298360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1311,
      "total_loss": 0.699187695980072
    },
    {
      "classification_loss": 0.5997182130813599,
      "epoch": 4.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1312,
      "total_loss": 0.5997182130813599
    },
    {
      "classification_loss": 0.6317111253738403,
      "epoch": 4.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1313,
      "total_loss": 0.6317111253738403
    },
    {
      "classification_loss": 0.6238917708396912,
      "epoch": 4.308196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1314,
      "total_loss": 0.6238917708396912
    },
    {
      "classification_loss": 0.560796856880188,
      "epoch": 4.311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1315,
      "total_loss": 0.560796856880188
    },
    {
      "classification_loss": 0.5513686537742615,
      "epoch": 4.314754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1316,
      "total_loss": 0.5513686537742615
    },
    {
      "classification_loss": 0.6047563552856445,
      "epoch": 4.3180327868852455,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1317,
      "total_loss": 0.6047563552856445
    },
    {
      "classification_loss": 0.5721820592880249,
      "epoch": 4.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1318,
      "total_loss": 0.5721820592880249
    },
    {
      "classification_loss": 0.7131564021110535,
      "epoch": 4.324590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1319,
      "total_loss": 0.7131564021110535
    },
    {
      "classification_loss": 0.632783830165863,
      "epoch": 4.327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1320,
      "total_loss": 0.632783830165863
    },
    {
      "classification_loss": 0.6432363390922546,
      "epoch": 4.331147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1321,
      "total_loss": 0.6432363390922546
    },
    {
      "classification_loss": 0.5497670769691467,
      "epoch": 4.334426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1322,
      "total_loss": 0.5497670769691467
    },
    {
      "classification_loss": 0.5885797142982483,
      "epoch": 4.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1323,
      "total_loss": 0.5885797142982483
    },
    {
      "classification_loss": 0.5626479387283325,
      "epoch": 4.340983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1324,
      "total_loss": 0.5626479387283325
    },
    {
      "classification_loss": 0.6483970880508423,
      "epoch": 4.344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1325,
      "total_loss": 0.6483970880508423
    },
    {
      "classification_loss": 0.5798713564872742,
      "epoch": 4.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1326,
      "total_loss": 0.5798713564872742
    },
    {
      "classification_loss": 0.617935299873352,
      "epoch": 4.350819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1327,
      "total_loss": 0.617935299873352
    },
    {
      "classification_loss": 0.625336766242981,
      "epoch": 4.354098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1328,
      "total_loss": 0.625336766242981
    },
    {
      "classification_loss": 0.6306458115577698,
      "epoch": 4.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1329,
      "total_loss": 0.6306458115577698
    },
    {
      "classification_loss": 0.6256660223007202,
      "epoch": 4.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1330,
      "total_loss": 0.6256660223007202
    },
    {
      "classification_loss": 0.661848247051239,
      "epoch": 4.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1331,
      "total_loss": 0.661848247051239
    },
    {
      "classification_loss": 0.5774208903312683,
      "epoch": 4.367213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1332,
      "total_loss": 0.5774208903312683
    },
    {
      "classification_loss": 0.5757571458816528,
      "epoch": 4.370491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1333,
      "total_loss": 0.5757571458816528
    },
    {
      "classification_loss": 0.6148219704627991,
      "epoch": 4.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1334,
      "total_loss": 0.6148219704627991
    },
    {
      "classification_loss": 0.6338549256324768,
      "epoch": 4.377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1335,
      "total_loss": 0.6338549256324768
    },
    {
      "classification_loss": 0.6287692785263062,
      "epoch": 4.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1336,
      "total_loss": 0.6287692785263062
    },
    {
      "classification_loss": 0.504151463508606,
      "epoch": 4.383606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1337,
      "total_loss": 0.504151463508606
    },
    {
      "classification_loss": 0.7237502932548523,
      "epoch": 4.386885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1338,
      "total_loss": 0.7237502932548523
    },
    {
      "classification_loss": 0.6263548135757446,
      "epoch": 4.390163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1339,
      "total_loss": 0.6263548135757446
    },
    {
      "classification_loss": 0.6338133811950684,
      "epoch": 4.39344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1340,
      "total_loss": 0.6338133811950684
    },
    {
      "classification_loss": 0.6325380206108093,
      "epoch": 4.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1341,
      "total_loss": 0.6325380206108093
    },
    {
      "classification_loss": 0.6752043962478638,
      "epoch": 4.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1342,
      "total_loss": 0.6752043962478638
    },
    {
      "classification_loss": 0.6438328623771667,
      "epoch": 4.4032786885245905,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1343,
      "total_loss": 0.6438328623771667
    },
    {
      "classification_loss": 0.6784514784812927,
      "epoch": 4.406557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1344,
      "total_loss": 0.6784514784812927
    },
    {
      "classification_loss": 0.6900307536125183,
      "epoch": 4.409836065573771,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1345,
      "total_loss": 0.6900307536125183
    },
    {
      "classification_loss": 0.6430552005767822,
      "epoch": 4.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1346,
      "total_loss": 0.6430552005767822
    },
    {
      "classification_loss": 0.6332566738128662,
      "epoch": 4.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1347,
      "total_loss": 0.6332566738128662
    },
    {
      "classification_loss": 0.5686779022216797,
      "epoch": 4.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1348,
      "total_loss": 0.5686779022216797
    },
    {
      "classification_loss": 0.6242983341217041,
      "epoch": 4.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1349,
      "total_loss": 0.6242983341217041
    },
    {
      "classification_loss": 0.5511518120765686,
      "epoch": 4.426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1350,
      "total_loss": 0.5511518120765686
    },
    {
      "classification_loss": 0.6525681614875793,
      "epoch": 4.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1351,
      "total_loss": 0.6525681614875793
    },
    {
      "classification_loss": 0.5969465374946594,
      "epoch": 4.432786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1352,
      "total_loss": 0.5969465374946594
    },
    {
      "classification_loss": 0.6676412224769592,
      "epoch": 4.436065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1353,
      "total_loss": 0.6676412224769592
    },
    {
      "classification_loss": 0.6152837872505188,
      "epoch": 4.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1354,
      "total_loss": 0.6152837872505188
    },
    {
      "classification_loss": 0.5457140803337097,
      "epoch": 4.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1355,
      "total_loss": 0.5457140803337097
    },
    {
      "classification_loss": 0.5779566168785095,
      "epoch": 4.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1356,
      "total_loss": 0.5779566168785095
    },
    {
      "classification_loss": 0.570010244846344,
      "epoch": 4.449180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1357,
      "total_loss": 0.570010244846344
    },
    {
      "classification_loss": 0.5641583800315857,
      "epoch": 4.452459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1358,
      "total_loss": 0.5641583800315857
    },
    {
      "classification_loss": 0.49657219648361206,
      "epoch": 4.4557377049180324,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1359,
      "total_loss": 0.49657219648361206
    },
    {
      "classification_loss": 0.6227476596832275,
      "epoch": 4.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1360,
      "total_loss": 0.6227476596832275
    },
    {
      "classification_loss": 0.5842375159263611,
      "epoch": 4.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1361,
      "total_loss": 0.5842375159263611
    },
    {
      "classification_loss": 0.6218135952949524,
      "epoch": 4.465573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1362,
      "total_loss": 0.6218135952949524
    },
    {
      "classification_loss": 0.59451824426651,
      "epoch": 4.468852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1363,
      "total_loss": 0.59451824426651
    },
    {
      "classification_loss": 0.6106052994728088,
      "epoch": 4.472131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1364,
      "total_loss": 0.6106052994728088
    },
    {
      "classification_loss": 0.5999407172203064,
      "epoch": 4.475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1365,
      "total_loss": 0.5999407172203064
    },
    {
      "classification_loss": 0.6557719111442566,
      "epoch": 4.478688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1366,
      "total_loss": 0.6557719111442566
    },
    {
      "classification_loss": 0.6020012497901917,
      "epoch": 4.481967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1367,
      "total_loss": 0.6020012497901917
    },
    {
      "classification_loss": 0.5742073059082031,
      "epoch": 4.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1368,
      "total_loss": 0.5742073059082031
    },
    {
      "classification_loss": 0.5899466276168823,
      "epoch": 4.488524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1369,
      "total_loss": 0.5899466276168823
    },
    {
      "classification_loss": 0.5193959474563599,
      "epoch": 4.491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1370,
      "total_loss": 0.5193959474563599
    },
    {
      "classification_loss": 0.5596458315849304,
      "epoch": 4.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1371,
      "total_loss": 0.5596458315849304
    },
    {
      "classification_loss": 0.6320549249649048,
      "epoch": 4.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1372,
      "total_loss": 0.6320549249649048
    },
    {
      "classification_loss": 0.5676828622817993,
      "epoch": 4.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1373,
      "total_loss": 0.5676828622817993
    },
    {
      "classification_loss": 0.5696626305580139,
      "epoch": 4.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1374,
      "total_loss": 0.5696626305580139
    },
    {
      "classification_loss": 0.6719072461128235,
      "epoch": 4.508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1375,
      "total_loss": 0.6719072461128235
    },
    {
      "classification_loss": 0.622886061668396,
      "epoch": 4.511475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1376,
      "total_loss": 0.622886061668396
    },
    {
      "classification_loss": 0.5999605059623718,
      "epoch": 4.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1377,
      "total_loss": 0.5999605059623718
    },
    {
      "classification_loss": 0.5925474762916565,
      "epoch": 4.518032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1378,
      "total_loss": 0.5925474762916565
    },
    {
      "classification_loss": 0.5273272395133972,
      "epoch": 4.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1379,
      "total_loss": 0.5273272395133972
    },
    {
      "classification_loss": 0.5912189483642578,
      "epoch": 4.524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1380,
      "total_loss": 0.5912189483642578
    },
    {
      "classification_loss": 0.6253887414932251,
      "epoch": 4.527868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1381,
      "total_loss": 0.6253887414932251
    },
    {
      "classification_loss": 0.6320371627807617,
      "epoch": 4.531147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1382,
      "total_loss": 0.6320371627807617
    },
    {
      "classification_loss": 0.6162731051445007,
      "epoch": 4.534426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1383,
      "total_loss": 0.6162731051445007
    },
    {
      "classification_loss": 0.6810347437858582,
      "epoch": 4.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1384,
      "total_loss": 0.6810347437858582
    },
    {
      "classification_loss": 0.6418379545211792,
      "epoch": 4.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1385,
      "total_loss": 0.6418379545211792
    },
    {
      "classification_loss": 0.6949424147605896,
      "epoch": 4.5442622950819676,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1386,
      "total_loss": 0.6949424147605896
    },
    {
      "classification_loss": 0.652403712272644,
      "epoch": 4.547540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1387,
      "total_loss": 0.652403712272644
    },
    {
      "classification_loss": 0.6912940740585327,
      "epoch": 4.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1388,
      "total_loss": 0.6912940740585327
    },
    {
      "classification_loss": 0.6709737181663513,
      "epoch": 4.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1389,
      "total_loss": 0.6709737181663513
    },
    {
      "classification_loss": 0.516912043094635,
      "epoch": 4.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1390,
      "total_loss": 0.516912043094635
    },
    {
      "classification_loss": 0.6007283926010132,
      "epoch": 4.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1391,
      "total_loss": 0.6007283926010132
    },
    {
      "classification_loss": 0.6521639823913574,
      "epoch": 4.563934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1392,
      "total_loss": 0.6521639823913574
    },
    {
      "classification_loss": 0.5566262602806091,
      "epoch": 4.567213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1393,
      "total_loss": 0.5566262602806091
    },
    {
      "classification_loss": 0.6835458278656006,
      "epoch": 4.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1394,
      "total_loss": 0.6835458278656006
    },
    {
      "classification_loss": 0.7109161615371704,
      "epoch": 4.573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1395,
      "total_loss": 0.7109161615371704
    },
    {
      "classification_loss": 0.6480072140693665,
      "epoch": 4.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1396,
      "total_loss": 0.6480072140693665
    },
    {
      "classification_loss": 0.5362540483474731,
      "epoch": 4.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1397,
      "total_loss": 0.5362540483474731
    },
    {
      "classification_loss": 0.6556361317634583,
      "epoch": 4.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1398,
      "total_loss": 0.6556361317634583
    },
    {
      "classification_loss": 0.6665355563163757,
      "epoch": 4.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1399,
      "total_loss": 0.6665355563163757
    },
    {
      "epoch": 4.590163934426229,
      "grad_norm": 2.022935628890991,
      "learning_rate": 0.00015670000000000001,
      "loss": 0.6149,
      "step": 1400
    },
    {
      "classification_loss": 0.5802262425422668,
      "epoch": 4.590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1400,
      "total_loss": 0.5802262425422668
    },
    {
      "classification_loss": 0.602824866771698,
      "epoch": 4.593442622950819,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1401,
      "total_loss": 0.602824866771698
    },
    {
      "classification_loss": 0.5065091252326965,
      "epoch": 4.5967213114754095,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1402,
      "total_loss": 0.5065091252326965
    },
    {
      "classification_loss": 0.6327073574066162,
      "epoch": 4.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1403,
      "total_loss": 0.6327073574066162
    },
    {
      "classification_loss": 0.6406164765357971,
      "epoch": 4.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1404,
      "total_loss": 0.6406164765357971
    },
    {
      "classification_loss": 0.7187238931655884,
      "epoch": 4.60655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1405,
      "total_loss": 0.7187238931655884
    },
    {
      "classification_loss": 0.6656099557876587,
      "epoch": 4.60983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1406,
      "total_loss": 0.6656099557876587
    },
    {
      "classification_loss": 0.6848244071006775,
      "epoch": 4.613114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1407,
      "total_loss": 0.6848244071006775
    },
    {
      "classification_loss": 0.534838080406189,
      "epoch": 4.616393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1408,
      "total_loss": 0.534838080406189
    },
    {
      "classification_loss": 0.6082494258880615,
      "epoch": 4.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1409,
      "total_loss": 0.6082494258880615
    },
    {
      "classification_loss": 0.5394712686538696,
      "epoch": 4.622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1410,
      "total_loss": 0.5394712686538696
    },
    {
      "classification_loss": 0.6804863214492798,
      "epoch": 4.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1411,
      "total_loss": 0.6804863214492798
    },
    {
      "classification_loss": 0.6429297924041748,
      "epoch": 4.629508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1412,
      "total_loss": 0.6429297924041748
    },
    {
      "classification_loss": 0.6630289554595947,
      "epoch": 4.632786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1413,
      "total_loss": 0.6630289554595947
    },
    {
      "classification_loss": 0.5708599090576172,
      "epoch": 4.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1414,
      "total_loss": 0.5708599090576172
    },
    {
      "classification_loss": 0.7441596984863281,
      "epoch": 4.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1415,
      "total_loss": 0.7441596984863281
    },
    {
      "classification_loss": 0.5457546710968018,
      "epoch": 4.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1416,
      "total_loss": 0.5457546710968018
    },
    {
      "classification_loss": 0.6295934319496155,
      "epoch": 4.645901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1417,
      "total_loss": 0.6295934319496155
    },
    {
      "classification_loss": 0.6232603192329407,
      "epoch": 4.649180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1418,
      "total_loss": 0.6232603192329407
    },
    {
      "classification_loss": 0.6330050826072693,
      "epoch": 4.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1419,
      "total_loss": 0.6330050826072693
    },
    {
      "classification_loss": 0.5723171234130859,
      "epoch": 4.655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1420,
      "total_loss": 0.5723171234130859
    },
    {
      "classification_loss": 0.5773890614509583,
      "epoch": 4.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1421,
      "total_loss": 0.5773890614509583
    },
    {
      "classification_loss": 0.6340546011924744,
      "epoch": 4.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1422,
      "total_loss": 0.6340546011924744
    },
    {
      "classification_loss": 0.5603237152099609,
      "epoch": 4.665573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1423,
      "total_loss": 0.5603237152099609
    },
    {
      "classification_loss": 0.6813889145851135,
      "epoch": 4.668852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1424,
      "total_loss": 0.6813889145851135
    },
    {
      "classification_loss": 0.5734073519706726,
      "epoch": 4.672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1425,
      "total_loss": 0.5734073519706726
    },
    {
      "classification_loss": 0.6716152429580688,
      "epoch": 4.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1426,
      "total_loss": 0.6716152429580688
    },
    {
      "classification_loss": 0.5833923816680908,
      "epoch": 4.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1427,
      "total_loss": 0.5833923816680908
    },
    {
      "classification_loss": 0.6378040313720703,
      "epoch": 4.6819672131147545,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1428,
      "total_loss": 0.6378040313720703
    },
    {
      "classification_loss": 0.657463788986206,
      "epoch": 4.685245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1429,
      "total_loss": 0.657463788986206
    },
    {
      "classification_loss": 0.5945281386375427,
      "epoch": 4.688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1430,
      "total_loss": 0.5945281386375427
    },
    {
      "classification_loss": 0.6690351366996765,
      "epoch": 4.691803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1431,
      "total_loss": 0.6690351366996765
    },
    {
      "classification_loss": 0.612931489944458,
      "epoch": 4.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1432,
      "total_loss": 0.612931489944458
    },
    {
      "classification_loss": 0.6456058621406555,
      "epoch": 4.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1433,
      "total_loss": 0.6456058621406555
    },
    {
      "classification_loss": 0.6302449703216553,
      "epoch": 4.701639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1434,
      "total_loss": 0.6302449703216553
    },
    {
      "classification_loss": 0.5479193925857544,
      "epoch": 4.704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1435,
      "total_loss": 0.5479193925857544
    },
    {
      "classification_loss": 0.6021906137466431,
      "epoch": 4.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1436,
      "total_loss": 0.6021906137466431
    },
    {
      "classification_loss": 0.6155139803886414,
      "epoch": 4.711475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1437,
      "total_loss": 0.6155139803886414
    },
    {
      "classification_loss": 0.535419762134552,
      "epoch": 4.714754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1438,
      "total_loss": 0.535419762134552
    },
    {
      "classification_loss": 0.596276044845581,
      "epoch": 4.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1439,
      "total_loss": 0.596276044845581
    },
    {
      "classification_loss": 0.5838643908500671,
      "epoch": 4.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1440,
      "total_loss": 0.5838643908500671
    },
    {
      "classification_loss": 0.5823084712028503,
      "epoch": 4.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1441,
      "total_loss": 0.5823084712028503
    },
    {
      "classification_loss": 0.5719428658485413,
      "epoch": 4.727868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1442,
      "total_loss": 0.5719428658485413
    },
    {
      "classification_loss": 0.589242160320282,
      "epoch": 4.731147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1443,
      "total_loss": 0.589242160320282
    },
    {
      "classification_loss": 0.6964853405952454,
      "epoch": 4.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1444,
      "total_loss": 0.6964853405952454
    },
    {
      "classification_loss": 0.6596203446388245,
      "epoch": 4.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1445,
      "total_loss": 0.6596203446388245
    },
    {
      "classification_loss": 0.5999756455421448,
      "epoch": 4.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1446,
      "total_loss": 0.5999756455421448
    },
    {
      "classification_loss": 0.6108867526054382,
      "epoch": 4.744262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1447,
      "total_loss": 0.6108867526054382
    },
    {
      "classification_loss": 0.5542850494384766,
      "epoch": 4.747540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1448,
      "total_loss": 0.5542850494384766
    },
    {
      "classification_loss": 0.678331196308136,
      "epoch": 4.750819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1449,
      "total_loss": 0.678331196308136
    },
    {
      "classification_loss": 0.5807459354400635,
      "epoch": 4.754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1450,
      "total_loss": 0.5807459354400635
    },
    {
      "classification_loss": 0.49327757954597473,
      "epoch": 4.757377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1451,
      "total_loss": 0.49327757954597473
    },
    {
      "classification_loss": 0.5539350509643555,
      "epoch": 4.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1452,
      "total_loss": 0.5539350509643555
    },
    {
      "classification_loss": 0.6757616400718689,
      "epoch": 4.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1453,
      "total_loss": 0.6757616400718689
    },
    {
      "classification_loss": 0.5822446346282959,
      "epoch": 4.767213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1454,
      "total_loss": 0.5822446346282959
    },
    {
      "classification_loss": 0.6334037780761719,
      "epoch": 4.770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1455,
      "total_loss": 0.6334037780761719
    },
    {
      "classification_loss": 0.6515371799468994,
      "epoch": 4.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1456,
      "total_loss": 0.6515371799468994
    },
    {
      "classification_loss": 0.5325434803962708,
      "epoch": 4.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1457,
      "total_loss": 0.5325434803962708
    },
    {
      "classification_loss": 0.643088161945343,
      "epoch": 4.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1458,
      "total_loss": 0.643088161945343
    },
    {
      "classification_loss": 0.591598629951477,
      "epoch": 4.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1459,
      "total_loss": 0.591598629951477
    },
    {
      "classification_loss": 0.5771383047103882,
      "epoch": 4.786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1460,
      "total_loss": 0.5771383047103882
    },
    {
      "classification_loss": 0.6282232999801636,
      "epoch": 4.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1461,
      "total_loss": 0.6282232999801636
    },
    {
      "classification_loss": 0.6169959902763367,
      "epoch": 4.79344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1462,
      "total_loss": 0.6169959902763367
    },
    {
      "classification_loss": 0.597118616104126,
      "epoch": 4.79672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1463,
      "total_loss": 0.597118616104126
    },
    {
      "classification_loss": 0.6193616390228271,
      "epoch": 4.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1464,
      "total_loss": 0.6193616390228271
    },
    {
      "classification_loss": 0.7381879091262817,
      "epoch": 4.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1465,
      "total_loss": 0.7381879091262817
    },
    {
      "classification_loss": 0.6327188611030579,
      "epoch": 4.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1466,
      "total_loss": 0.6327188611030579
    },
    {
      "classification_loss": 0.601210355758667,
      "epoch": 4.80983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1467,
      "total_loss": 0.601210355758667
    },
    {
      "classification_loss": 0.6204874515533447,
      "epoch": 4.81311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1468,
      "total_loss": 0.6204874515533447
    },
    {
      "classification_loss": 0.6089308261871338,
      "epoch": 4.81639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1469,
      "total_loss": 0.6089308261871338
    },
    {
      "classification_loss": 0.6305131912231445,
      "epoch": 4.8196721311475414,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1470,
      "total_loss": 0.6305131912231445
    },
    {
      "classification_loss": 0.6772041320800781,
      "epoch": 4.822950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1471,
      "total_loss": 0.6772041320800781
    },
    {
      "classification_loss": 0.6538732051849365,
      "epoch": 4.826229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1472,
      "total_loss": 0.6538732051849365
    },
    {
      "classification_loss": 0.6183979511260986,
      "epoch": 4.829508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1473,
      "total_loss": 0.6183979511260986
    },
    {
      "classification_loss": 0.5237435698509216,
      "epoch": 4.832786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1474,
      "total_loss": 0.5237435698509216
    },
    {
      "classification_loss": 0.679257869720459,
      "epoch": 4.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1475,
      "total_loss": 0.679257869720459
    },
    {
      "classification_loss": 0.5708352327346802,
      "epoch": 4.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1476,
      "total_loss": 0.5708352327346802
    },
    {
      "classification_loss": 0.6174558401107788,
      "epoch": 4.842622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1477,
      "total_loss": 0.6174558401107788
    },
    {
      "classification_loss": 0.631921648979187,
      "epoch": 4.845901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1478,
      "total_loss": 0.631921648979187
    },
    {
      "classification_loss": 0.5952916145324707,
      "epoch": 4.849180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1479,
      "total_loss": 0.5952916145324707
    },
    {
      "classification_loss": 0.572384774684906,
      "epoch": 4.852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1480,
      "total_loss": 0.572384774684906
    },
    {
      "classification_loss": 0.6426143646240234,
      "epoch": 4.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1481,
      "total_loss": 0.6426143646240234
    },
    {
      "classification_loss": 0.6005951762199402,
      "epoch": 4.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1482,
      "total_loss": 0.6005951762199402
    },
    {
      "classification_loss": 0.6066238880157471,
      "epoch": 4.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1483,
      "total_loss": 0.6066238880157471
    },
    {
      "classification_loss": 0.6129755973815918,
      "epoch": 4.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1484,
      "total_loss": 0.6129755973815918
    },
    {
      "classification_loss": 0.7210013270378113,
      "epoch": 4.868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1485,
      "total_loss": 0.7210013270378113
    },
    {
      "classification_loss": 0.6132560968399048,
      "epoch": 4.872131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1486,
      "total_loss": 0.6132560968399048
    },
    {
      "classification_loss": 0.6648959517478943,
      "epoch": 4.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1487,
      "total_loss": 0.6648959517478943
    },
    {
      "classification_loss": 0.579983115196228,
      "epoch": 4.878688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1488,
      "total_loss": 0.579983115196228
    },
    {
      "classification_loss": 0.6198024153709412,
      "epoch": 4.881967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1489,
      "total_loss": 0.6198024153709412
    },
    {
      "classification_loss": 0.5582086443901062,
      "epoch": 4.885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1490,
      "total_loss": 0.5582086443901062
    },
    {
      "classification_loss": 0.5637299418449402,
      "epoch": 4.888524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1491,
      "total_loss": 0.5637299418449402
    },
    {
      "classification_loss": 0.6098572611808777,
      "epoch": 4.891803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1492,
      "total_loss": 0.6098572611808777
    },
    {
      "classification_loss": 0.5764316916465759,
      "epoch": 4.895081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1493,
      "total_loss": 0.5764316916465759
    },
    {
      "classification_loss": 0.6418242454528809,
      "epoch": 4.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1494,
      "total_loss": 0.6418242454528809
    },
    {
      "classification_loss": 0.5161649584770203,
      "epoch": 4.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1495,
      "total_loss": 0.5161649584770203
    },
    {
      "classification_loss": 0.5547545552253723,
      "epoch": 4.9049180327868855,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1496,
      "total_loss": 0.5547545552253723
    },
    {
      "classification_loss": 0.6770390868186951,
      "epoch": 4.908196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1497,
      "total_loss": 0.6770390868186951
    },
    {
      "classification_loss": 0.5151958465576172,
      "epoch": 4.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1498,
      "total_loss": 0.5151958465576172
    },
    {
      "classification_loss": 0.6009490489959717,
      "epoch": 4.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1499,
      "total_loss": 0.6009490489959717
    },
    {
      "epoch": 4.918032786885246,
      "grad_norm": 3.3738842010498047,
      "learning_rate": 0.0001533666666666667,
      "loss": 0.612,
      "step": 1500
    },
    {
      "classification_loss": 0.5180524587631226,
      "epoch": 4.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1500,
      "total_loss": 0.5180524587631226
    },
    {
      "classification_loss": 0.6664366126060486,
      "epoch": 4.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1501,
      "total_loss": 0.6664366126060486
    },
    {
      "classification_loss": 0.6937834024429321,
      "epoch": 4.924590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1502,
      "total_loss": 0.6937834024429321
    },
    {
      "classification_loss": 0.7206858992576599,
      "epoch": 4.927868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1503,
      "total_loss": 0.7206858992576599
    },
    {
      "classification_loss": 0.6322591304779053,
      "epoch": 4.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1504,
      "total_loss": 0.6322591304779053
    },
    {
      "classification_loss": 0.629467785358429,
      "epoch": 4.934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1505,
      "total_loss": 0.629467785358429
    },
    {
      "classification_loss": 0.6099069118499756,
      "epoch": 4.937704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1506,
      "total_loss": 0.6099069118499756
    },
    {
      "classification_loss": 0.579878568649292,
      "epoch": 4.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1507,
      "total_loss": 0.579878568649292
    },
    {
      "classification_loss": 0.6222012042999268,
      "epoch": 4.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1508,
      "total_loss": 0.6222012042999268
    },
    {
      "classification_loss": 0.5786590576171875,
      "epoch": 4.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1509,
      "total_loss": 0.5786590576171875
    },
    {
      "classification_loss": 0.5829073786735535,
      "epoch": 4.950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1510,
      "total_loss": 0.5829073786735535
    },
    {
      "classification_loss": 0.6704398989677429,
      "epoch": 4.954098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1511,
      "total_loss": 0.6704398989677429
    },
    {
      "classification_loss": 0.6392284631729126,
      "epoch": 4.9573770491803275,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1512,
      "total_loss": 0.6392284631729126
    },
    {
      "classification_loss": 0.5982922911643982,
      "epoch": 4.9606557377049185,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1513,
      "total_loss": 0.5982922911643982
    },
    {
      "classification_loss": 0.648367702960968,
      "epoch": 4.963934426229509,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1514,
      "total_loss": 0.648367702960968
    },
    {
      "classification_loss": 0.5569722652435303,
      "epoch": 4.967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1515,
      "total_loss": 0.5569722652435303
    },
    {
      "classification_loss": 0.6504996418952942,
      "epoch": 4.970491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1516,
      "total_loss": 0.6504996418952942
    },
    {
      "classification_loss": 0.5489656329154968,
      "epoch": 4.973770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1517,
      "total_loss": 0.5489656329154968
    },
    {
      "classification_loss": 0.5668359994888306,
      "epoch": 4.977049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1518,
      "total_loss": 0.5668359994888306
    },
    {
      "classification_loss": 0.6903090476989746,
      "epoch": 4.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1519,
      "total_loss": 0.6903090476989746
    },
    {
      "classification_loss": 0.5722302198410034,
      "epoch": 4.983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1520,
      "total_loss": 0.5722302198410034
    },
    {
      "classification_loss": 0.619776725769043,
      "epoch": 4.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1521,
      "total_loss": 0.619776725769043
    },
    {
      "classification_loss": 0.6290905475616455,
      "epoch": 4.99016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1522,
      "total_loss": 0.6290905475616455
    },
    {
      "classification_loss": 0.607042670249939,
      "epoch": 4.99344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1523,
      "total_loss": 0.607042670249939
    },
    {
      "classification_loss": 0.5477498173713684,
      "epoch": 4.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1524,
      "total_loss": 0.5477498173713684
    },
    {
      "classification_loss": 0.8081842660903931,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.8081842660903931
    },
    {
      "classification_loss": 0.815520167350769,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.815520167350769
    },
    {
      "classification_loss": 0.7694676518440247,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.7694676518440247
    },
    {
      "classification_loss": 0.8044621348381042,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.8044621348381042
    },
    {
      "classification_loss": 0.8242750763893127,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.8242750763893127
    },
    {
      "classification_loss": 0.770676851272583,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.770676851272583
    },
    {
      "classification_loss": 0.8163133263587952,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.8163133263587952
    },
    {
      "classification_loss": 0.8460747003555298,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.8460747003555298
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.386,
      "eval_f1": 0.046583850931677016,
      "eval_loss": 0.8059309124946594,
      "eval_precision": 0.5357142857142857,
      "eval_recall": 0.024350649350649352,
      "eval_runtime": 6.015,
      "eval_samples_per_second": 166.252,
      "eval_steps_per_second": 1.33,
      "step": 1525
    },
    {
      "classification_loss": 0.5608336925506592,
      "epoch": 5.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1525,
      "total_loss": 0.5608336925506592
    },
    {
      "classification_loss": 0.5898687839508057,
      "epoch": 5.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1526,
      "total_loss": 0.5898687839508057
    },
    {
      "classification_loss": 0.6637603044509888,
      "epoch": 5.00655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1527,
      "total_loss": 0.6637603044509888
    },
    {
      "classification_loss": 0.593955934047699,
      "epoch": 5.00983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1528,
      "total_loss": 0.593955934047699
    },
    {
      "classification_loss": 0.6609741449356079,
      "epoch": 5.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1529,
      "total_loss": 0.6609741449356079
    },
    {
      "classification_loss": 0.5648731589317322,
      "epoch": 5.016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1530,
      "total_loss": 0.5648731589317322
    },
    {
      "classification_loss": 0.5929698944091797,
      "epoch": 5.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1531,
      "total_loss": 0.5929698944091797
    },
    {
      "classification_loss": 0.5762847661972046,
      "epoch": 5.022950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1532,
      "total_loss": 0.5762847661972046
    },
    {
      "classification_loss": 0.5371336340904236,
      "epoch": 5.026229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1533,
      "total_loss": 0.5371336340904236
    },
    {
      "classification_loss": 0.5840566754341125,
      "epoch": 5.029508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1534,
      "total_loss": 0.5840566754341125
    },
    {
      "classification_loss": 0.602897584438324,
      "epoch": 5.032786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1535,
      "total_loss": 0.602897584438324
    },
    {
      "classification_loss": 0.5043368935585022,
      "epoch": 5.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1536,
      "total_loss": 0.5043368935585022
    },
    {
      "classification_loss": 0.5269388556480408,
      "epoch": 5.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1537,
      "total_loss": 0.5269388556480408
    },
    {
      "classification_loss": 0.6331909894943237,
      "epoch": 5.0426229508196725,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1538,
      "total_loss": 0.6331909894943237
    },
    {
      "classification_loss": 0.6130141019821167,
      "epoch": 5.045901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1539,
      "total_loss": 0.6130141019821167
    },
    {
      "classification_loss": 0.5986696481704712,
      "epoch": 5.049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1540,
      "total_loss": 0.5986696481704712
    },
    {
      "classification_loss": 0.6080790162086487,
      "epoch": 5.052459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1541,
      "total_loss": 0.6080790162086487
    },
    {
      "classification_loss": 0.6772029399871826,
      "epoch": 5.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1542,
      "total_loss": 0.6772029399871826
    },
    {
      "classification_loss": 0.5647454857826233,
      "epoch": 5.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1543,
      "total_loss": 0.5647454857826233
    },
    {
      "classification_loss": 0.5856652855873108,
      "epoch": 5.062295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1544,
      "total_loss": 0.5856652855873108
    },
    {
      "classification_loss": 0.5962874293327332,
      "epoch": 5.065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1545,
      "total_loss": 0.5962874293327332
    },
    {
      "classification_loss": 0.5992951989173889,
      "epoch": 5.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1546,
      "total_loss": 0.5992951989173889
    },
    {
      "classification_loss": 0.6053336262702942,
      "epoch": 5.072131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1547,
      "total_loss": 0.6053336262702942
    },
    {
      "classification_loss": 0.5976468920707703,
      "epoch": 5.075409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1548,
      "total_loss": 0.5976468920707703
    },
    {
      "classification_loss": 0.6101208329200745,
      "epoch": 5.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1549,
      "total_loss": 0.6101208329200745
    },
    {
      "classification_loss": 0.6728189587593079,
      "epoch": 5.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1550,
      "total_loss": 0.6728189587593079
    },
    {
      "classification_loss": 0.5777475833892822,
      "epoch": 5.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1551,
      "total_loss": 0.5777475833892822
    },
    {
      "classification_loss": 0.4995563328266144,
      "epoch": 5.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1552,
      "total_loss": 0.4995563328266144
    },
    {
      "classification_loss": 0.5589964389801025,
      "epoch": 5.091803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1553,
      "total_loss": 0.5589964389801025
    },
    {
      "classification_loss": 0.5891236066818237,
      "epoch": 5.0950819672131145,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1554,
      "total_loss": 0.5891236066818237
    },
    {
      "classification_loss": 0.6093260049819946,
      "epoch": 5.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1555,
      "total_loss": 0.6093260049819946
    },
    {
      "classification_loss": 0.6040255427360535,
      "epoch": 5.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1556,
      "total_loss": 0.6040255427360535
    },
    {
      "classification_loss": 0.5608054995536804,
      "epoch": 5.104918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1557,
      "total_loss": 0.5608054995536804
    },
    {
      "classification_loss": 0.5738340020179749,
      "epoch": 5.108196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1558,
      "total_loss": 0.5738340020179749
    },
    {
      "classification_loss": 0.574418306350708,
      "epoch": 5.111475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1559,
      "total_loss": 0.574418306350708
    },
    {
      "classification_loss": 0.560662031173706,
      "epoch": 5.114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1560,
      "total_loss": 0.560662031173706
    },
    {
      "classification_loss": 0.5616984367370605,
      "epoch": 5.118032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1561,
      "total_loss": 0.5616984367370605
    },
    {
      "classification_loss": 0.6527803540229797,
      "epoch": 5.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1562,
      "total_loss": 0.6527803540229797
    },
    {
      "classification_loss": 0.5562193989753723,
      "epoch": 5.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1563,
      "total_loss": 0.5562193989753723
    },
    {
      "classification_loss": 0.4718753397464752,
      "epoch": 5.127868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1564,
      "total_loss": 0.4718753397464752
    },
    {
      "classification_loss": 0.5704919695854187,
      "epoch": 5.131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1565,
      "total_loss": 0.5704919695854187
    },
    {
      "classification_loss": 0.6324613094329834,
      "epoch": 5.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1566,
      "total_loss": 0.6324613094329834
    },
    {
      "classification_loss": 0.5951934456825256,
      "epoch": 5.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1567,
      "total_loss": 0.5951934456825256
    },
    {
      "classification_loss": 0.5582968592643738,
      "epoch": 5.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1568,
      "total_loss": 0.5582968592643738
    },
    {
      "classification_loss": 0.5497429966926575,
      "epoch": 5.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1569,
      "total_loss": 0.5497429966926575
    },
    {
      "classification_loss": 0.6545443534851074,
      "epoch": 5.147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1570,
      "total_loss": 0.6545443534851074
    },
    {
      "classification_loss": 0.5451280474662781,
      "epoch": 5.150819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1571,
      "total_loss": 0.5451280474662781
    },
    {
      "classification_loss": 0.6512765288352966,
      "epoch": 5.154098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1572,
      "total_loss": 0.6512765288352966
    },
    {
      "classification_loss": 0.5509798526763916,
      "epoch": 5.157377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1573,
      "total_loss": 0.5509798526763916
    },
    {
      "classification_loss": 0.5744193196296692,
      "epoch": 5.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1574,
      "total_loss": 0.5744193196296692
    },
    {
      "classification_loss": 0.6578912138938904,
      "epoch": 5.163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1575,
      "total_loss": 0.6578912138938904
    },
    {
      "classification_loss": 0.6630329489707947,
      "epoch": 5.167213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1576,
      "total_loss": 0.6630329489707947
    },
    {
      "classification_loss": 0.6306357383728027,
      "epoch": 5.170491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1577,
      "total_loss": 0.6306357383728027
    },
    {
      "classification_loss": 0.5530909895896912,
      "epoch": 5.173770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1578,
      "total_loss": 0.5530909895896912
    },
    {
      "classification_loss": 0.5886719822883606,
      "epoch": 5.177049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1579,
      "total_loss": 0.5886719822883606
    },
    {
      "classification_loss": 0.6155627965927124,
      "epoch": 5.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1580,
      "total_loss": 0.6155627965927124
    },
    {
      "classification_loss": 0.6304884552955627,
      "epoch": 5.18360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1581,
      "total_loss": 0.6304884552955627
    },
    {
      "classification_loss": 0.6037725806236267,
      "epoch": 5.18688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1582,
      "total_loss": 0.6037725806236267
    },
    {
      "classification_loss": 0.622494637966156,
      "epoch": 5.19016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1583,
      "total_loss": 0.622494637966156
    },
    {
      "classification_loss": 0.5899410247802734,
      "epoch": 5.19344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1584,
      "total_loss": 0.5899410247802734
    },
    {
      "classification_loss": 0.7251695990562439,
      "epoch": 5.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1585,
      "total_loss": 0.7251695990562439
    },
    {
      "classification_loss": 0.5426695346832275,
      "epoch": 5.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1586,
      "total_loss": 0.5426695346832275
    },
    {
      "classification_loss": 0.6821722984313965,
      "epoch": 5.20327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1587,
      "total_loss": 0.6821722984313965
    },
    {
      "classification_loss": 0.6697131991386414,
      "epoch": 5.20655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1588,
      "total_loss": 0.6697131991386414
    },
    {
      "classification_loss": 0.5591906905174255,
      "epoch": 5.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1589,
      "total_loss": 0.5591906905174255
    },
    {
      "classification_loss": 0.660535454750061,
      "epoch": 5.213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1590,
      "total_loss": 0.660535454750061
    },
    {
      "classification_loss": 0.6295778155326843,
      "epoch": 5.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1591,
      "total_loss": 0.6295778155326843
    },
    {
      "classification_loss": 0.5362180471420288,
      "epoch": 5.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1592,
      "total_loss": 0.5362180471420288
    },
    {
      "classification_loss": 0.6859857439994812,
      "epoch": 5.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1593,
      "total_loss": 0.6859857439994812
    },
    {
      "classification_loss": 0.6794723868370056,
      "epoch": 5.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1594,
      "total_loss": 0.6794723868370056
    },
    {
      "classification_loss": 0.6883529424667358,
      "epoch": 5.229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1595,
      "total_loss": 0.6883529424667358
    },
    {
      "classification_loss": 0.5358390212059021,
      "epoch": 5.232786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1596,
      "total_loss": 0.5358390212059021
    },
    {
      "classification_loss": 0.6095877289772034,
      "epoch": 5.2360655737704915,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1597,
      "total_loss": 0.6095877289772034
    },
    {
      "classification_loss": 0.5070148706436157,
      "epoch": 5.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1598,
      "total_loss": 0.5070148706436157
    },
    {
      "classification_loss": 0.5653550028800964,
      "epoch": 5.242622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1599,
      "total_loss": 0.5653550028800964
    },
    {
      "epoch": 5.245901639344262,
      "grad_norm": 1.6196744441986084,
      "learning_rate": 0.00015003333333333334,
      "loss": 0.6017,
      "step": 1600
    },
    {
      "classification_loss": 0.5777466297149658,
      "epoch": 5.245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1600,
      "total_loss": 0.5777466297149658
    },
    {
      "classification_loss": 0.5001081228256226,
      "epoch": 5.249180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1601,
      "total_loss": 0.5001081228256226
    },
    {
      "classification_loss": 0.49101752042770386,
      "epoch": 5.252459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1602,
      "total_loss": 0.49101752042770386
    },
    {
      "classification_loss": 0.5569146871566772,
      "epoch": 5.255737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1603,
      "total_loss": 0.5569146871566772
    },
    {
      "classification_loss": 0.6671104431152344,
      "epoch": 5.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1604,
      "total_loss": 0.6671104431152344
    },
    {
      "classification_loss": 0.5830817222595215,
      "epoch": 5.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1605,
      "total_loss": 0.5830817222595215
    },
    {
      "classification_loss": 0.4943763315677643,
      "epoch": 5.2655737704918035,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1606,
      "total_loss": 0.4943763315677643
    },
    {
      "classification_loss": 0.5794508457183838,
      "epoch": 5.268852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1607,
      "total_loss": 0.5794508457183838
    },
    {
      "classification_loss": 0.6015356779098511,
      "epoch": 5.272131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1608,
      "total_loss": 0.6015356779098511
    },
    {
      "classification_loss": 0.6085029244422913,
      "epoch": 5.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1609,
      "total_loss": 0.6085029244422913
    },
    {
      "classification_loss": 0.6482090950012207,
      "epoch": 5.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1610,
      "total_loss": 0.6482090950012207
    },
    {
      "classification_loss": 0.5961523056030273,
      "epoch": 5.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1611,
      "total_loss": 0.5961523056030273
    },
    {
      "classification_loss": 0.7050100564956665,
      "epoch": 5.285245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1612,
      "total_loss": 0.7050100564956665
    },
    {
      "classification_loss": 0.576784610748291,
      "epoch": 5.288524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1613,
      "total_loss": 0.576784610748291
    },
    {
      "classification_loss": 0.6166484355926514,
      "epoch": 5.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1614,
      "total_loss": 0.6166484355926514
    },
    {
      "classification_loss": 0.5398461222648621,
      "epoch": 5.295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1615,
      "total_loss": 0.5398461222648621
    },
    {
      "classification_loss": 0.5468605756759644,
      "epoch": 5.298360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1616,
      "total_loss": 0.5468605756759644
    },
    {
      "classification_loss": 0.5693497657775879,
      "epoch": 5.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1617,
      "total_loss": 0.5693497657775879
    },
    {
      "classification_loss": 0.49168848991394043,
      "epoch": 5.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1618,
      "total_loss": 0.49168848991394043
    },
    {
      "classification_loss": 0.6423971652984619,
      "epoch": 5.308196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1619,
      "total_loss": 0.6423971652984619
    },
    {
      "classification_loss": 0.5992878079414368,
      "epoch": 5.311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1620,
      "total_loss": 0.5992878079414368
    },
    {
      "classification_loss": 0.5967979431152344,
      "epoch": 5.314754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1621,
      "total_loss": 0.5967979431152344
    },
    {
      "classification_loss": 0.5879791378974915,
      "epoch": 5.3180327868852455,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1622,
      "total_loss": 0.5879791378974915
    },
    {
      "classification_loss": 0.6241641044616699,
      "epoch": 5.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1623,
      "total_loss": 0.6241641044616699
    },
    {
      "classification_loss": 0.6980471014976501,
      "epoch": 5.324590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1624,
      "total_loss": 0.6980471014976501
    },
    {
      "classification_loss": 0.5684847831726074,
      "epoch": 5.327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1625,
      "total_loss": 0.5684847831726074
    },
    {
      "classification_loss": 0.6320127844810486,
      "epoch": 5.331147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1626,
      "total_loss": 0.6320127844810486
    },
    {
      "classification_loss": 0.6152425408363342,
      "epoch": 5.334426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1627,
      "total_loss": 0.6152425408363342
    },
    {
      "classification_loss": 0.5110061764717102,
      "epoch": 5.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1628,
      "total_loss": 0.5110061764717102
    },
    {
      "classification_loss": 0.5750390291213989,
      "epoch": 5.340983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1629,
      "total_loss": 0.5750390291213989
    },
    {
      "classification_loss": 0.6328524947166443,
      "epoch": 5.344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1630,
      "total_loss": 0.6328524947166443
    },
    {
      "classification_loss": 0.6433120369911194,
      "epoch": 5.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1631,
      "total_loss": 0.6433120369911194
    },
    {
      "classification_loss": 0.6108385920524597,
      "epoch": 5.350819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1632,
      "total_loss": 0.6108385920524597
    },
    {
      "classification_loss": 0.5302122235298157,
      "epoch": 5.354098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1633,
      "total_loss": 0.5302122235298157
    },
    {
      "classification_loss": 0.569660484790802,
      "epoch": 5.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1634,
      "total_loss": 0.569660484790802
    },
    {
      "classification_loss": 0.5546441674232483,
      "epoch": 5.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1635,
      "total_loss": 0.5546441674232483
    },
    {
      "classification_loss": 0.6399850249290466,
      "epoch": 5.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1636,
      "total_loss": 0.6399850249290466
    },
    {
      "classification_loss": 0.6427546739578247,
      "epoch": 5.367213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1637,
      "total_loss": 0.6427546739578247
    },
    {
      "classification_loss": 0.6320439577102661,
      "epoch": 5.370491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1638,
      "total_loss": 0.6320439577102661
    },
    {
      "classification_loss": 0.6394643187522888,
      "epoch": 5.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1639,
      "total_loss": 0.6394643187522888
    },
    {
      "classification_loss": 0.6342782974243164,
      "epoch": 5.377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1640,
      "total_loss": 0.6342782974243164
    },
    {
      "classification_loss": 0.5587713718414307,
      "epoch": 5.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1641,
      "total_loss": 0.5587713718414307
    },
    {
      "classification_loss": 0.5247108936309814,
      "epoch": 5.383606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1642,
      "total_loss": 0.5247108936309814
    },
    {
      "classification_loss": 0.6050195097923279,
      "epoch": 5.386885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1643,
      "total_loss": 0.6050195097923279
    },
    {
      "classification_loss": 0.5530580878257751,
      "epoch": 5.390163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1644,
      "total_loss": 0.5530580878257751
    },
    {
      "classification_loss": 0.5912889838218689,
      "epoch": 5.39344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1645,
      "total_loss": 0.5912889838218689
    },
    {
      "classification_loss": 0.6289838552474976,
      "epoch": 5.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1646,
      "total_loss": 0.6289838552474976
    },
    {
      "classification_loss": 0.5532776713371277,
      "epoch": 5.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1647,
      "total_loss": 0.5532776713371277
    },
    {
      "classification_loss": 0.6154069900512695,
      "epoch": 5.4032786885245905,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1648,
      "total_loss": 0.6154069900512695
    },
    {
      "classification_loss": 0.5850673913955688,
      "epoch": 5.406557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1649,
      "total_loss": 0.5850673913955688
    },
    {
      "classification_loss": 0.6587686538696289,
      "epoch": 5.409836065573771,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1650,
      "total_loss": 0.6587686538696289
    },
    {
      "classification_loss": 0.7529624700546265,
      "epoch": 5.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1651,
      "total_loss": 0.7529624700546265
    },
    {
      "classification_loss": 0.7050322890281677,
      "epoch": 5.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1652,
      "total_loss": 0.7050322890281677
    },
    {
      "classification_loss": 0.6511422991752625,
      "epoch": 5.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1653,
      "total_loss": 0.6511422991752625
    },
    {
      "classification_loss": 0.5750051140785217,
      "epoch": 5.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1654,
      "total_loss": 0.5750051140785217
    },
    {
      "classification_loss": 0.5509589910507202,
      "epoch": 5.426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1655,
      "total_loss": 0.5509589910507202
    },
    {
      "classification_loss": 0.5702080130577087,
      "epoch": 5.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1656,
      "total_loss": 0.5702080130577087
    },
    {
      "classification_loss": 0.5930655002593994,
      "epoch": 5.432786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1657,
      "total_loss": 0.5930655002593994
    },
    {
      "classification_loss": 0.6670597791671753,
      "epoch": 5.436065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1658,
      "total_loss": 0.6670597791671753
    },
    {
      "classification_loss": 0.5344065427780151,
      "epoch": 5.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1659,
      "total_loss": 0.5344065427780151
    },
    {
      "classification_loss": 0.5460860729217529,
      "epoch": 5.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1660,
      "total_loss": 0.5460860729217529
    },
    {
      "classification_loss": 0.6024317145347595,
      "epoch": 5.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1661,
      "total_loss": 0.6024317145347595
    },
    {
      "classification_loss": 0.613056480884552,
      "epoch": 5.449180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1662,
      "total_loss": 0.613056480884552
    },
    {
      "classification_loss": 0.5895558595657349,
      "epoch": 5.452459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1663,
      "total_loss": 0.5895558595657349
    },
    {
      "classification_loss": 0.5755303502082825,
      "epoch": 5.4557377049180324,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1664,
      "total_loss": 0.5755303502082825
    },
    {
      "classification_loss": 0.626845121383667,
      "epoch": 5.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1665,
      "total_loss": 0.626845121383667
    },
    {
      "classification_loss": 0.6045891046524048,
      "epoch": 5.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1666,
      "total_loss": 0.6045891046524048
    },
    {
      "classification_loss": 0.5554234385490417,
      "epoch": 5.465573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1667,
      "total_loss": 0.5554234385490417
    },
    {
      "classification_loss": 0.5260471701622009,
      "epoch": 5.468852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1668,
      "total_loss": 0.5260471701622009
    },
    {
      "classification_loss": 0.5812584757804871,
      "epoch": 5.472131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1669,
      "total_loss": 0.5812584757804871
    },
    {
      "classification_loss": 0.6239568591117859,
      "epoch": 5.475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1670,
      "total_loss": 0.6239568591117859
    },
    {
      "classification_loss": 0.5184000730514526,
      "epoch": 5.478688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1671,
      "total_loss": 0.5184000730514526
    },
    {
      "classification_loss": 0.6587947010993958,
      "epoch": 5.481967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1672,
      "total_loss": 0.6587947010993958
    },
    {
      "classification_loss": 0.5460141897201538,
      "epoch": 5.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1673,
      "total_loss": 0.5460141897201538
    },
    {
      "classification_loss": 0.585055947303772,
      "epoch": 5.488524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1674,
      "total_loss": 0.585055947303772
    },
    {
      "classification_loss": 0.522449791431427,
      "epoch": 5.491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1675,
      "total_loss": 0.522449791431427
    },
    {
      "classification_loss": 0.6672071218490601,
      "epoch": 5.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1676,
      "total_loss": 0.6672071218490601
    },
    {
      "classification_loss": 0.6076526045799255,
      "epoch": 5.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1677,
      "total_loss": 0.6076526045799255
    },
    {
      "classification_loss": 0.5779394507408142,
      "epoch": 5.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1678,
      "total_loss": 0.5779394507408142
    },
    {
      "classification_loss": 0.6194155216217041,
      "epoch": 5.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1679,
      "total_loss": 0.6194155216217041
    },
    {
      "classification_loss": 0.5409916043281555,
      "epoch": 5.508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1680,
      "total_loss": 0.5409916043281555
    },
    {
      "classification_loss": 0.5870317816734314,
      "epoch": 5.511475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1681,
      "total_loss": 0.5870317816734314
    },
    {
      "classification_loss": 0.6525362730026245,
      "epoch": 5.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1682,
      "total_loss": 0.6525362730026245
    },
    {
      "classification_loss": 0.6839011907577515,
      "epoch": 5.518032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1683,
      "total_loss": 0.6839011907577515
    },
    {
      "classification_loss": 0.6016508340835571,
      "epoch": 5.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1684,
      "total_loss": 0.6016508340835571
    },
    {
      "classification_loss": 0.6618828177452087,
      "epoch": 5.524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1685,
      "total_loss": 0.6618828177452087
    },
    {
      "classification_loss": 0.6105896234512329,
      "epoch": 5.527868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1686,
      "total_loss": 0.6105896234512329
    },
    {
      "classification_loss": 0.5134607553482056,
      "epoch": 5.531147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1687,
      "total_loss": 0.5134607553482056
    },
    {
      "classification_loss": 0.5308201313018799,
      "epoch": 5.534426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1688,
      "total_loss": 0.5308201313018799
    },
    {
      "classification_loss": 0.5711118578910828,
      "epoch": 5.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1689,
      "total_loss": 0.5711118578910828
    },
    {
      "classification_loss": 0.538690984249115,
      "epoch": 5.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1690,
      "total_loss": 0.538690984249115
    },
    {
      "classification_loss": 0.6983269453048706,
      "epoch": 5.5442622950819676,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1691,
      "total_loss": 0.6983269453048706
    },
    {
      "classification_loss": 0.6079597473144531,
      "epoch": 5.547540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1692,
      "total_loss": 0.6079597473144531
    },
    {
      "classification_loss": 0.5836083889007568,
      "epoch": 5.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1693,
      "total_loss": 0.5836083889007568
    },
    {
      "classification_loss": 0.609467625617981,
      "epoch": 5.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1694,
      "total_loss": 0.609467625617981
    },
    {
      "classification_loss": 0.6502703428268433,
      "epoch": 5.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1695,
      "total_loss": 0.6502703428268433
    },
    {
      "classification_loss": 0.6099626421928406,
      "epoch": 5.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1696,
      "total_loss": 0.6099626421928406
    },
    {
      "classification_loss": 0.6460134983062744,
      "epoch": 5.563934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1697,
      "total_loss": 0.6460134983062744
    },
    {
      "classification_loss": 0.489798367023468,
      "epoch": 5.567213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1698,
      "total_loss": 0.489798367023468
    },
    {
      "classification_loss": 0.6280614137649536,
      "epoch": 5.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1699,
      "total_loss": 0.6280614137649536
    },
    {
      "epoch": 5.573770491803279,
      "grad_norm": 6.2505784034729,
      "learning_rate": 0.00014670000000000002,
      "loss": 0.5957,
      "step": 1700
    },
    {
      "classification_loss": 0.6168277263641357,
      "epoch": 5.573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1700,
      "total_loss": 0.6168277263641357
    },
    {
      "classification_loss": 0.569088339805603,
      "epoch": 5.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1701,
      "total_loss": 0.569088339805603
    },
    {
      "classification_loss": 0.648637592792511,
      "epoch": 5.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1702,
      "total_loss": 0.648637592792511
    },
    {
      "classification_loss": 0.598726212978363,
      "epoch": 5.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1703,
      "total_loss": 0.598726212978363
    },
    {
      "classification_loss": 0.6538802981376648,
      "epoch": 5.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1704,
      "total_loss": 0.6538802981376648
    },
    {
      "classification_loss": 0.6928440928459167,
      "epoch": 5.590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1705,
      "total_loss": 0.6928440928459167
    },
    {
      "classification_loss": 0.619903028011322,
      "epoch": 5.593442622950819,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1706,
      "total_loss": 0.619903028011322
    },
    {
      "classification_loss": 0.6174445152282715,
      "epoch": 5.5967213114754095,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1707,
      "total_loss": 0.6174445152282715
    },
    {
      "classification_loss": 0.5855605602264404,
      "epoch": 5.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1708,
      "total_loss": 0.5855605602264404
    },
    {
      "classification_loss": 0.5358385443687439,
      "epoch": 5.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1709,
      "total_loss": 0.5358385443687439
    },
    {
      "classification_loss": 0.5965628623962402,
      "epoch": 5.60655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1710,
      "total_loss": 0.5965628623962402
    },
    {
      "classification_loss": 0.6421135663986206,
      "epoch": 5.60983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1711,
      "total_loss": 0.6421135663986206
    },
    {
      "classification_loss": 0.6769149303436279,
      "epoch": 5.613114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1712,
      "total_loss": 0.6769149303436279
    },
    {
      "classification_loss": 0.5638856291770935,
      "epoch": 5.616393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1713,
      "total_loss": 0.5638856291770935
    },
    {
      "classification_loss": 0.6148120164871216,
      "epoch": 5.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1714,
      "total_loss": 0.6148120164871216
    },
    {
      "classification_loss": 0.5803239941596985,
      "epoch": 5.622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1715,
      "total_loss": 0.5803239941596985
    },
    {
      "classification_loss": 0.5448226928710938,
      "epoch": 5.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1716,
      "total_loss": 0.5448226928710938
    },
    {
      "classification_loss": 0.5740092396736145,
      "epoch": 5.629508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1717,
      "total_loss": 0.5740092396736145
    },
    {
      "classification_loss": 0.6490640044212341,
      "epoch": 5.632786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1718,
      "total_loss": 0.6490640044212341
    },
    {
      "classification_loss": 0.7030341029167175,
      "epoch": 5.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1719,
      "total_loss": 0.7030341029167175
    },
    {
      "classification_loss": 0.6349772810935974,
      "epoch": 5.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1720,
      "total_loss": 0.6349772810935974
    },
    {
      "classification_loss": 0.5856825709342957,
      "epoch": 5.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1721,
      "total_loss": 0.5856825709342957
    },
    {
      "classification_loss": 0.5687947869300842,
      "epoch": 5.645901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1722,
      "total_loss": 0.5687947869300842
    },
    {
      "classification_loss": 0.5528814196586609,
      "epoch": 5.649180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1723,
      "total_loss": 0.5528814196586609
    },
    {
      "classification_loss": 0.5379759073257446,
      "epoch": 5.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1724,
      "total_loss": 0.5379759073257446
    },
    {
      "classification_loss": 0.5430747270584106,
      "epoch": 5.655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1725,
      "total_loss": 0.5430747270584106
    },
    {
      "classification_loss": 0.6067262887954712,
      "epoch": 5.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1726,
      "total_loss": 0.6067262887954712
    },
    {
      "classification_loss": 0.6796644926071167,
      "epoch": 5.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1727,
      "total_loss": 0.6796644926071167
    },
    {
      "classification_loss": 0.7075309753417969,
      "epoch": 5.665573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1728,
      "total_loss": 0.7075309753417969
    },
    {
      "classification_loss": 0.5711971521377563,
      "epoch": 5.668852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1729,
      "total_loss": 0.5711971521377563
    },
    {
      "classification_loss": 0.5904287099838257,
      "epoch": 5.672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1730,
      "total_loss": 0.5904287099838257
    },
    {
      "classification_loss": 0.599726140499115,
      "epoch": 5.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1731,
      "total_loss": 0.599726140499115
    },
    {
      "classification_loss": 0.5482950210571289,
      "epoch": 5.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1732,
      "total_loss": 0.5482950210571289
    },
    {
      "classification_loss": 0.5941221117973328,
      "epoch": 5.6819672131147545,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1733,
      "total_loss": 0.5941221117973328
    },
    {
      "classification_loss": 0.5605884790420532,
      "epoch": 5.685245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1734,
      "total_loss": 0.5605884790420532
    },
    {
      "classification_loss": 0.5846195816993713,
      "epoch": 5.688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1735,
      "total_loss": 0.5846195816993713
    },
    {
      "classification_loss": 0.6859638094902039,
      "epoch": 5.691803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1736,
      "total_loss": 0.6859638094902039
    },
    {
      "classification_loss": 0.612999677658081,
      "epoch": 5.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1737,
      "total_loss": 0.612999677658081
    },
    {
      "classification_loss": 0.5501219630241394,
      "epoch": 5.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1738,
      "total_loss": 0.5501219630241394
    },
    {
      "classification_loss": 0.6329747438430786,
      "epoch": 5.701639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1739,
      "total_loss": 0.6329747438430786
    },
    {
      "classification_loss": 0.5631535649299622,
      "epoch": 5.704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1740,
      "total_loss": 0.5631535649299622
    },
    {
      "classification_loss": 0.6147648096084595,
      "epoch": 5.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1741,
      "total_loss": 0.6147648096084595
    },
    {
      "classification_loss": 0.5803424119949341,
      "epoch": 5.711475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1742,
      "total_loss": 0.5803424119949341
    },
    {
      "classification_loss": 0.6038102507591248,
      "epoch": 5.714754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1743,
      "total_loss": 0.6038102507591248
    },
    {
      "classification_loss": 0.5899420976638794,
      "epoch": 5.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1744,
      "total_loss": 0.5899420976638794
    },
    {
      "classification_loss": 0.5981983542442322,
      "epoch": 5.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1745,
      "total_loss": 0.5981983542442322
    },
    {
      "classification_loss": 0.5904481410980225,
      "epoch": 5.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1746,
      "total_loss": 0.5904481410980225
    },
    {
      "classification_loss": 0.5451491475105286,
      "epoch": 5.727868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1747,
      "total_loss": 0.5451491475105286
    },
    {
      "classification_loss": 0.6547409892082214,
      "epoch": 5.731147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1748,
      "total_loss": 0.6547409892082214
    },
    {
      "classification_loss": 0.6354094743728638,
      "epoch": 5.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1749,
      "total_loss": 0.6354094743728638
    },
    {
      "classification_loss": 0.5885980129241943,
      "epoch": 5.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1750,
      "total_loss": 0.5885980129241943
    },
    {
      "classification_loss": 0.6005251407623291,
      "epoch": 5.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1751,
      "total_loss": 0.6005251407623291
    },
    {
      "classification_loss": 0.5781897902488708,
      "epoch": 5.744262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1752,
      "total_loss": 0.5781897902488708
    },
    {
      "classification_loss": 0.5871270895004272,
      "epoch": 5.747540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1753,
      "total_loss": 0.5871270895004272
    },
    {
      "classification_loss": 0.5617706775665283,
      "epoch": 5.750819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1754,
      "total_loss": 0.5617706775665283
    },
    {
      "classification_loss": 0.6762306094169617,
      "epoch": 5.754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1755,
      "total_loss": 0.6762306094169617
    },
    {
      "classification_loss": 0.6594526767730713,
      "epoch": 5.757377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1756,
      "total_loss": 0.6594526767730713
    },
    {
      "classification_loss": 0.541719377040863,
      "epoch": 5.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1757,
      "total_loss": 0.541719377040863
    },
    {
      "classification_loss": 0.5071882605552673,
      "epoch": 5.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1758,
      "total_loss": 0.5071882605552673
    },
    {
      "classification_loss": 0.6398782730102539,
      "epoch": 5.767213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1759,
      "total_loss": 0.6398782730102539
    },
    {
      "classification_loss": 0.630033552646637,
      "epoch": 5.770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1760,
      "total_loss": 0.630033552646637
    },
    {
      "classification_loss": 0.5538896918296814,
      "epoch": 5.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1761,
      "total_loss": 0.5538896918296814
    },
    {
      "classification_loss": 0.6129533052444458,
      "epoch": 5.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1762,
      "total_loss": 0.6129533052444458
    },
    {
      "classification_loss": 0.5386292338371277,
      "epoch": 5.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1763,
      "total_loss": 0.5386292338371277
    },
    {
      "classification_loss": 0.5751042366027832,
      "epoch": 5.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1764,
      "total_loss": 0.5751042366027832
    },
    {
      "classification_loss": 0.584256112575531,
      "epoch": 5.786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1765,
      "total_loss": 0.584256112575531
    },
    {
      "classification_loss": 0.6217102408409119,
      "epoch": 5.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1766,
      "total_loss": 0.6217102408409119
    },
    {
      "classification_loss": 0.6146299839019775,
      "epoch": 5.79344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1767,
      "total_loss": 0.6146299839019775
    },
    {
      "classification_loss": 0.6316826343536377,
      "epoch": 5.79672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1768,
      "total_loss": 0.6316826343536377
    },
    {
      "classification_loss": 0.539649248123169,
      "epoch": 5.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1769,
      "total_loss": 0.539649248123169
    },
    {
      "classification_loss": 0.589898407459259,
      "epoch": 5.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1770,
      "total_loss": 0.589898407459259
    },
    {
      "classification_loss": 0.5364210605621338,
      "epoch": 5.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1771,
      "total_loss": 0.5364210605621338
    },
    {
      "classification_loss": 0.6198354363441467,
      "epoch": 5.80983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1772,
      "total_loss": 0.6198354363441467
    },
    {
      "classification_loss": 0.5677297711372375,
      "epoch": 5.81311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1773,
      "total_loss": 0.5677297711372375
    },
    {
      "classification_loss": 0.6901080012321472,
      "epoch": 5.81639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1774,
      "total_loss": 0.6901080012321472
    },
    {
      "classification_loss": 0.6357269883155823,
      "epoch": 5.8196721311475414,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1775,
      "total_loss": 0.6357269883155823
    },
    {
      "classification_loss": 0.6435679793357849,
      "epoch": 5.822950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1776,
      "total_loss": 0.6435679793357849
    },
    {
      "classification_loss": 0.5804790258407593,
      "epoch": 5.826229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1777,
      "total_loss": 0.5804790258407593
    },
    {
      "classification_loss": 0.5719375014305115,
      "epoch": 5.829508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1778,
      "total_loss": 0.5719375014305115
    },
    {
      "classification_loss": 0.5505544543266296,
      "epoch": 5.832786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1779,
      "total_loss": 0.5505544543266296
    },
    {
      "classification_loss": 0.567344069480896,
      "epoch": 5.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1780,
      "total_loss": 0.567344069480896
    },
    {
      "classification_loss": 0.5311126112937927,
      "epoch": 5.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1781,
      "total_loss": 0.5311126112937927
    },
    {
      "classification_loss": 0.6182644963264465,
      "epoch": 5.842622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1782,
      "total_loss": 0.6182644963264465
    },
    {
      "classification_loss": 0.5775355100631714,
      "epoch": 5.845901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1783,
      "total_loss": 0.5775355100631714
    },
    {
      "classification_loss": 0.6243841648101807,
      "epoch": 5.849180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1784,
      "total_loss": 0.6243841648101807
    },
    {
      "classification_loss": 0.5559186935424805,
      "epoch": 5.852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1785,
      "total_loss": 0.5559186935424805
    },
    {
      "classification_loss": 0.5799004435539246,
      "epoch": 5.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1786,
      "total_loss": 0.5799004435539246
    },
    {
      "classification_loss": 0.5299907326698303,
      "epoch": 5.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1787,
      "total_loss": 0.5299907326698303
    },
    {
      "classification_loss": 0.6376186609268188,
      "epoch": 5.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1788,
      "total_loss": 0.6376186609268188
    },
    {
      "classification_loss": 0.7200932502746582,
      "epoch": 5.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1789,
      "total_loss": 0.7200932502746582
    },
    {
      "classification_loss": 0.6536538600921631,
      "epoch": 5.868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1790,
      "total_loss": 0.6536538600921631
    },
    {
      "classification_loss": 0.5474568605422974,
      "epoch": 5.872131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1791,
      "total_loss": 0.5474568605422974
    },
    {
      "classification_loss": 0.5269936919212341,
      "epoch": 5.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1792,
      "total_loss": 0.5269936919212341
    },
    {
      "classification_loss": 0.6184963583946228,
      "epoch": 5.878688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1793,
      "total_loss": 0.6184963583946228
    },
    {
      "classification_loss": 0.5621777772903442,
      "epoch": 5.881967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1794,
      "total_loss": 0.5621777772903442
    },
    {
      "classification_loss": 0.6894485950469971,
      "epoch": 5.885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1795,
      "total_loss": 0.6894485950469971
    },
    {
      "classification_loss": 0.5489555597305298,
      "epoch": 5.888524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1796,
      "total_loss": 0.5489555597305298
    },
    {
      "classification_loss": 0.6226036548614502,
      "epoch": 5.891803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1797,
      "total_loss": 0.6226036548614502
    },
    {
      "classification_loss": 0.6058671474456787,
      "epoch": 5.895081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1798,
      "total_loss": 0.6058671474456787
    },
    {
      "classification_loss": 0.6273713707923889,
      "epoch": 5.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1799,
      "total_loss": 0.6273713707923889
    },
    {
      "epoch": 5.901639344262295,
      "grad_norm": 3.3945908546447754,
      "learning_rate": 0.00014336666666666666,
      "loss": 0.5998,
      "step": 1800
    },
    {
      "classification_loss": 0.6965091228485107,
      "epoch": 5.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1800,
      "total_loss": 0.6965091228485107
    },
    {
      "classification_loss": 0.6683136224746704,
      "epoch": 5.9049180327868855,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1801,
      "total_loss": 0.6683136224746704
    },
    {
      "classification_loss": 0.7167962789535522,
      "epoch": 5.908196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1802,
      "total_loss": 0.7167962789535522
    },
    {
      "classification_loss": 0.652079701423645,
      "epoch": 5.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1803,
      "total_loss": 0.652079701423645
    },
    {
      "classification_loss": 0.6228280067443848,
      "epoch": 5.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1804,
      "total_loss": 0.6228280067443848
    },
    {
      "classification_loss": 0.6171314716339111,
      "epoch": 5.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1805,
      "total_loss": 0.6171314716339111
    },
    {
      "classification_loss": 0.5892067551612854,
      "epoch": 5.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1806,
      "total_loss": 0.5892067551612854
    },
    {
      "classification_loss": 0.5876856446266174,
      "epoch": 5.924590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1807,
      "total_loss": 0.5876856446266174
    },
    {
      "classification_loss": 0.5734573602676392,
      "epoch": 5.927868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1808,
      "total_loss": 0.5734573602676392
    },
    {
      "classification_loss": 0.6094684600830078,
      "epoch": 5.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1809,
      "total_loss": 0.6094684600830078
    },
    {
      "classification_loss": 0.5346488356590271,
      "epoch": 5.934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1810,
      "total_loss": 0.5346488356590271
    },
    {
      "classification_loss": 0.6426770091056824,
      "epoch": 5.937704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1811,
      "total_loss": 0.6426770091056824
    },
    {
      "classification_loss": 0.680649995803833,
      "epoch": 5.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1812,
      "total_loss": 0.680649995803833
    },
    {
      "classification_loss": 0.5894228219985962,
      "epoch": 5.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1813,
      "total_loss": 0.5894228219985962
    },
    {
      "classification_loss": 0.61036616563797,
      "epoch": 5.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1814,
      "total_loss": 0.61036616563797
    },
    {
      "classification_loss": 0.5727936625480652,
      "epoch": 5.950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1815,
      "total_loss": 0.5727936625480652
    },
    {
      "classification_loss": 0.6465556025505066,
      "epoch": 5.954098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1816,
      "total_loss": 0.6465556025505066
    },
    {
      "classification_loss": 0.6269751191139221,
      "epoch": 5.9573770491803275,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1817,
      "total_loss": 0.6269751191139221
    },
    {
      "classification_loss": 0.6000234484672546,
      "epoch": 5.9606557377049185,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1818,
      "total_loss": 0.6000234484672546
    },
    {
      "classification_loss": 0.6318621039390564,
      "epoch": 5.963934426229509,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1819,
      "total_loss": 0.6318621039390564
    },
    {
      "classification_loss": 0.5729424357414246,
      "epoch": 5.967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1820,
      "total_loss": 0.5729424357414246
    },
    {
      "classification_loss": 0.5367099046707153,
      "epoch": 5.970491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1821,
      "total_loss": 0.5367099046707153
    },
    {
      "classification_loss": 0.5751940011978149,
      "epoch": 5.973770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1822,
      "total_loss": 0.5751940011978149
    },
    {
      "classification_loss": 0.6209040880203247,
      "epoch": 5.977049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1823,
      "total_loss": 0.6209040880203247
    },
    {
      "classification_loss": 0.5242931842803955,
      "epoch": 5.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1824,
      "total_loss": 0.5242931842803955
    },
    {
      "classification_loss": 0.6472184658050537,
      "epoch": 5.983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1825,
      "total_loss": 0.6472184658050537
    },
    {
      "classification_loss": 0.6001814603805542,
      "epoch": 5.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1826,
      "total_loss": 0.6001814603805542
    },
    {
      "classification_loss": 0.6469470262527466,
      "epoch": 5.99016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1827,
      "total_loss": 0.6469470262527466
    },
    {
      "classification_loss": 0.6065701246261597,
      "epoch": 5.99344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1828,
      "total_loss": 0.6065701246261597
    },
    {
      "classification_loss": 0.540103018283844,
      "epoch": 5.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1829,
      "total_loss": 0.540103018283844
    },
    {
      "classification_loss": 0.8529797196388245,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8529797196388245
    },
    {
      "classification_loss": 0.8693519234657288,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8693519234657288
    },
    {
      "classification_loss": 0.8090246319770813,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8090246319770813
    },
    {
      "classification_loss": 0.8667639493942261,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8667639493942261
    },
    {
      "classification_loss": 0.9011285901069641,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.9011285901069641
    },
    {
      "classification_loss": 0.8292636275291443,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8292636275291443
    },
    {
      "classification_loss": 0.8729363679885864,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.8729363679885864
    },
    {
      "classification_loss": 0.9194307923316956,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.9194307923316956
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.385,
      "eval_f1": 0.022257551669316374,
      "eval_loss": 0.8638062477111816,
      "eval_precision": 0.5384615384615384,
      "eval_recall": 0.011363636363636364,
      "eval_runtime": 6.0044,
      "eval_samples_per_second": 166.545,
      "eval_steps_per_second": 1.332,
      "step": 1830
    },
    {
      "classification_loss": 0.5806073546409607,
      "epoch": 6.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1830,
      "total_loss": 0.5806073546409607
    },
    {
      "classification_loss": 0.5727645754814148,
      "epoch": 6.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1831,
      "total_loss": 0.5727645754814148
    },
    {
      "classification_loss": 0.6193816065788269,
      "epoch": 6.00655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1832,
      "total_loss": 0.6193816065788269
    },
    {
      "classification_loss": 0.5132774710655212,
      "epoch": 6.00983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1833,
      "total_loss": 0.5132774710655212
    },
    {
      "classification_loss": 0.5332643389701843,
      "epoch": 6.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1834,
      "total_loss": 0.5332643389701843
    },
    {
      "classification_loss": 0.5313779711723328,
      "epoch": 6.016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1835,
      "total_loss": 0.5313779711723328
    },
    {
      "classification_loss": 0.6494385600090027,
      "epoch": 6.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1836,
      "total_loss": 0.6494385600090027
    },
    {
      "classification_loss": 0.6109398603439331,
      "epoch": 6.022950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1837,
      "total_loss": 0.6109398603439331
    },
    {
      "classification_loss": 0.5384947657585144,
      "epoch": 6.026229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1838,
      "total_loss": 0.5384947657585144
    },
    {
      "classification_loss": 0.5542232990264893,
      "epoch": 6.029508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1839,
      "total_loss": 0.5542232990264893
    },
    {
      "classification_loss": 0.4974783957004547,
      "epoch": 6.032786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1840,
      "total_loss": 0.4974783957004547
    },
    {
      "classification_loss": 0.5605330467224121,
      "epoch": 6.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1841,
      "total_loss": 0.5605330467224121
    },
    {
      "classification_loss": 0.491926372051239,
      "epoch": 6.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1842,
      "total_loss": 0.491926372051239
    },
    {
      "classification_loss": 0.5700345039367676,
      "epoch": 6.0426229508196725,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1843,
      "total_loss": 0.5700345039367676
    },
    {
      "classification_loss": 0.5685234665870667,
      "epoch": 6.045901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1844,
      "total_loss": 0.5685234665870667
    },
    {
      "classification_loss": 0.6149701476097107,
      "epoch": 6.049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1845,
      "total_loss": 0.6149701476097107
    },
    {
      "classification_loss": 0.5446391701698303,
      "epoch": 6.052459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1846,
      "total_loss": 0.5446391701698303
    },
    {
      "classification_loss": 0.6101577281951904,
      "epoch": 6.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1847,
      "total_loss": 0.6101577281951904
    },
    {
      "classification_loss": 0.6116995215415955,
      "epoch": 6.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1848,
      "total_loss": 0.6116995215415955
    },
    {
      "classification_loss": 0.6348393559455872,
      "epoch": 6.062295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1849,
      "total_loss": 0.6348393559455872
    },
    {
      "classification_loss": 0.6396308541297913,
      "epoch": 6.065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1850,
      "total_loss": 0.6396308541297913
    },
    {
      "classification_loss": 0.602004885673523,
      "epoch": 6.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1851,
      "total_loss": 0.602004885673523
    },
    {
      "classification_loss": 0.6164206862449646,
      "epoch": 6.072131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1852,
      "total_loss": 0.6164206862449646
    },
    {
      "classification_loss": 0.6425511837005615,
      "epoch": 6.075409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1853,
      "total_loss": 0.6425511837005615
    },
    {
      "classification_loss": 0.5975652933120728,
      "epoch": 6.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1854,
      "total_loss": 0.5975652933120728
    },
    {
      "classification_loss": 0.6076561212539673,
      "epoch": 6.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1855,
      "total_loss": 0.6076561212539673
    },
    {
      "classification_loss": 0.5834212899208069,
      "epoch": 6.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1856,
      "total_loss": 0.5834212899208069
    },
    {
      "classification_loss": 0.6368065476417542,
      "epoch": 6.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1857,
      "total_loss": 0.6368065476417542
    },
    {
      "classification_loss": 0.583351731300354,
      "epoch": 6.091803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1858,
      "total_loss": 0.583351731300354
    },
    {
      "classification_loss": 0.6321721076965332,
      "epoch": 6.0950819672131145,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1859,
      "total_loss": 0.6321721076965332
    },
    {
      "classification_loss": 0.5170990228652954,
      "epoch": 6.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1860,
      "total_loss": 0.5170990228652954
    },
    {
      "classification_loss": 0.5834342837333679,
      "epoch": 6.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1861,
      "total_loss": 0.5834342837333679
    },
    {
      "classification_loss": 0.5538958311080933,
      "epoch": 6.104918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1862,
      "total_loss": 0.5538958311080933
    },
    {
      "classification_loss": 0.6229912638664246,
      "epoch": 6.108196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1863,
      "total_loss": 0.6229912638664246
    },
    {
      "classification_loss": 0.6181129813194275,
      "epoch": 6.111475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1864,
      "total_loss": 0.6181129813194275
    },
    {
      "classification_loss": 0.5402867794036865,
      "epoch": 6.114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1865,
      "total_loss": 0.5402867794036865
    },
    {
      "classification_loss": 0.5469902753829956,
      "epoch": 6.118032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1866,
      "total_loss": 0.5469902753829956
    },
    {
      "classification_loss": 0.5949777364730835,
      "epoch": 6.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1867,
      "total_loss": 0.5949777364730835
    },
    {
      "classification_loss": 0.5586543679237366,
      "epoch": 6.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1868,
      "total_loss": 0.5586543679237366
    },
    {
      "classification_loss": 0.4780302345752716,
      "epoch": 6.127868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1869,
      "total_loss": 0.4780302345752716
    },
    {
      "classification_loss": 0.500116765499115,
      "epoch": 6.131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1870,
      "total_loss": 0.500116765499115
    },
    {
      "classification_loss": 0.5443543791770935,
      "epoch": 6.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1871,
      "total_loss": 0.5443543791770935
    },
    {
      "classification_loss": 0.5768535733222961,
      "epoch": 6.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1872,
      "total_loss": 0.5768535733222961
    },
    {
      "classification_loss": 0.5914469361305237,
      "epoch": 6.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1873,
      "total_loss": 0.5914469361305237
    },
    {
      "classification_loss": 0.6304324269294739,
      "epoch": 6.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1874,
      "total_loss": 0.6304324269294739
    },
    {
      "classification_loss": 0.5553825497627258,
      "epoch": 6.147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1875,
      "total_loss": 0.5553825497627258
    },
    {
      "classification_loss": 0.5185739398002625,
      "epoch": 6.150819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1876,
      "total_loss": 0.5185739398002625
    },
    {
      "classification_loss": 0.5272107720375061,
      "epoch": 6.154098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1877,
      "total_loss": 0.5272107720375061
    },
    {
      "classification_loss": 0.6043519377708435,
      "epoch": 6.157377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1878,
      "total_loss": 0.6043519377708435
    },
    {
      "classification_loss": 0.5187537670135498,
      "epoch": 6.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1879,
      "total_loss": 0.5187537670135498
    },
    {
      "classification_loss": 0.5832049250602722,
      "epoch": 6.163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1880,
      "total_loss": 0.5832049250602722
    },
    {
      "classification_loss": 0.6544439792633057,
      "epoch": 6.167213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1881,
      "total_loss": 0.6544439792633057
    },
    {
      "classification_loss": 0.6024925708770752,
      "epoch": 6.170491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1882,
      "total_loss": 0.6024925708770752
    },
    {
      "classification_loss": 0.5679277777671814,
      "epoch": 6.173770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1883,
      "total_loss": 0.5679277777671814
    },
    {
      "classification_loss": 0.5478244423866272,
      "epoch": 6.177049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1884,
      "total_loss": 0.5478244423866272
    },
    {
      "classification_loss": 0.6260948777198792,
      "epoch": 6.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1885,
      "total_loss": 0.6260948777198792
    },
    {
      "classification_loss": 0.5369112491607666,
      "epoch": 6.18360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1886,
      "total_loss": 0.5369112491607666
    },
    {
      "classification_loss": 0.5367147326469421,
      "epoch": 6.18688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1887,
      "total_loss": 0.5367147326469421
    },
    {
      "classification_loss": 0.6226917505264282,
      "epoch": 6.19016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1888,
      "total_loss": 0.6226917505264282
    },
    {
      "classification_loss": 0.5993525981903076,
      "epoch": 6.19344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1889,
      "total_loss": 0.5993525981903076
    },
    {
      "classification_loss": 0.5488818883895874,
      "epoch": 6.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1890,
      "total_loss": 0.5488818883895874
    },
    {
      "classification_loss": 0.6540742516517639,
      "epoch": 6.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1891,
      "total_loss": 0.6540742516517639
    },
    {
      "classification_loss": 0.625725269317627,
      "epoch": 6.20327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1892,
      "total_loss": 0.625725269317627
    },
    {
      "classification_loss": 0.6317040920257568,
      "epoch": 6.20655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1893,
      "total_loss": 0.6317040920257568
    },
    {
      "classification_loss": 0.6296610832214355,
      "epoch": 6.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1894,
      "total_loss": 0.6296610832214355
    },
    {
      "classification_loss": 0.5539137125015259,
      "epoch": 6.213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1895,
      "total_loss": 0.5539137125015259
    },
    {
      "classification_loss": 0.5299625396728516,
      "epoch": 6.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1896,
      "total_loss": 0.5299625396728516
    },
    {
      "classification_loss": 0.5695436596870422,
      "epoch": 6.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1897,
      "total_loss": 0.5695436596870422
    },
    {
      "classification_loss": 0.5134169459342957,
      "epoch": 6.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1898,
      "total_loss": 0.5134169459342957
    },
    {
      "classification_loss": 0.5446102023124695,
      "epoch": 6.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1899,
      "total_loss": 0.5446102023124695
    },
    {
      "epoch": 6.229508196721311,
      "grad_norm": 1.3727295398712158,
      "learning_rate": 0.00014003333333333334,
      "loss": 0.5875,
      "step": 1900
    },
    {
      "classification_loss": 0.6240277886390686,
      "epoch": 6.229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1900,
      "total_loss": 0.6240277886390686
    },
    {
      "classification_loss": 0.5774996876716614,
      "epoch": 6.232786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1901,
      "total_loss": 0.5774996876716614
    },
    {
      "classification_loss": 0.646736741065979,
      "epoch": 6.2360655737704915,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1902,
      "total_loss": 0.646736741065979
    },
    {
      "classification_loss": 0.5251232981681824,
      "epoch": 6.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1903,
      "total_loss": 0.5251232981681824
    },
    {
      "classification_loss": 0.6265149712562561,
      "epoch": 6.242622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1904,
      "total_loss": 0.6265149712562561
    },
    {
      "classification_loss": 0.626054048538208,
      "epoch": 6.245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1905,
      "total_loss": 0.626054048538208
    },
    {
      "classification_loss": 0.5940521359443665,
      "epoch": 6.249180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1906,
      "total_loss": 0.5940521359443665
    },
    {
      "classification_loss": 0.4617633819580078,
      "epoch": 6.252459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1907,
      "total_loss": 0.4617633819580078
    },
    {
      "classification_loss": 0.5318429470062256,
      "epoch": 6.255737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1908,
      "total_loss": 0.5318429470062256
    },
    {
      "classification_loss": 0.5834463238716125,
      "epoch": 6.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1909,
      "total_loss": 0.5834463238716125
    },
    {
      "classification_loss": 0.6350163221359253,
      "epoch": 6.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1910,
      "total_loss": 0.6350163221359253
    },
    {
      "classification_loss": 0.6146199703216553,
      "epoch": 6.2655737704918035,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1911,
      "total_loss": 0.6146199703216553
    },
    {
      "classification_loss": 0.6650277376174927,
      "epoch": 6.268852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1912,
      "total_loss": 0.6650277376174927
    },
    {
      "classification_loss": 0.5551496148109436,
      "epoch": 6.272131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1913,
      "total_loss": 0.5551496148109436
    },
    {
      "classification_loss": 0.4558360278606415,
      "epoch": 6.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1914,
      "total_loss": 0.4558360278606415
    },
    {
      "classification_loss": 0.44717907905578613,
      "epoch": 6.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1915,
      "total_loss": 0.44717907905578613
    },
    {
      "classification_loss": 0.6407036781311035,
      "epoch": 6.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1916,
      "total_loss": 0.6407036781311035
    },
    {
      "classification_loss": 0.6548159718513489,
      "epoch": 6.285245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1917,
      "total_loss": 0.6548159718513489
    },
    {
      "classification_loss": 0.4670025110244751,
      "epoch": 6.288524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1918,
      "total_loss": 0.4670025110244751
    },
    {
      "classification_loss": 0.6161987781524658,
      "epoch": 6.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1919,
      "total_loss": 0.6161987781524658
    },
    {
      "classification_loss": 0.5905726552009583,
      "epoch": 6.295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1920,
      "total_loss": 0.5905726552009583
    },
    {
      "classification_loss": 0.5576467514038086,
      "epoch": 6.298360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1921,
      "total_loss": 0.5576467514038086
    },
    {
      "classification_loss": 0.5574254989624023,
      "epoch": 6.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1922,
      "total_loss": 0.5574254989624023
    },
    {
      "classification_loss": 0.5616351962089539,
      "epoch": 6.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1923,
      "total_loss": 0.5616351962089539
    },
    {
      "classification_loss": 0.6259167194366455,
      "epoch": 6.308196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1924,
      "total_loss": 0.6259167194366455
    },
    {
      "classification_loss": 0.5386627912521362,
      "epoch": 6.311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1925,
      "total_loss": 0.5386627912521362
    },
    {
      "classification_loss": 0.5435369610786438,
      "epoch": 6.314754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1926,
      "total_loss": 0.5435369610786438
    },
    {
      "classification_loss": 0.5857653021812439,
      "epoch": 6.3180327868852455,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1927,
      "total_loss": 0.5857653021812439
    },
    {
      "classification_loss": 0.5258879065513611,
      "epoch": 6.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1928,
      "total_loss": 0.5258879065513611
    },
    {
      "classification_loss": 0.5490608215332031,
      "epoch": 6.324590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1929,
      "total_loss": 0.5490608215332031
    },
    {
      "classification_loss": 0.6998987197875977,
      "epoch": 6.327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1930,
      "total_loss": 0.6998987197875977
    },
    {
      "classification_loss": 0.7557983994483948,
      "epoch": 6.331147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1931,
      "total_loss": 0.7557983994483948
    },
    {
      "classification_loss": 0.5007653832435608,
      "epoch": 6.334426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1932,
      "total_loss": 0.5007653832435608
    },
    {
      "classification_loss": 0.5121027827262878,
      "epoch": 6.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1933,
      "total_loss": 0.5121027827262878
    },
    {
      "classification_loss": 0.5641094446182251,
      "epoch": 6.340983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1934,
      "total_loss": 0.5641094446182251
    },
    {
      "classification_loss": 0.593713104724884,
      "epoch": 6.344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1935,
      "total_loss": 0.593713104724884
    },
    {
      "classification_loss": 0.6435331106185913,
      "epoch": 6.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1936,
      "total_loss": 0.6435331106185913
    },
    {
      "classification_loss": 0.703556478023529,
      "epoch": 6.350819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1937,
      "total_loss": 0.703556478023529
    },
    {
      "classification_loss": 0.5276748538017273,
      "epoch": 6.354098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1938,
      "total_loss": 0.5276748538017273
    },
    {
      "classification_loss": 0.6556780934333801,
      "epoch": 6.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1939,
      "total_loss": 0.6556780934333801
    },
    {
      "classification_loss": 0.49953287839889526,
      "epoch": 6.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1940,
      "total_loss": 0.49953287839889526
    },
    {
      "classification_loss": 0.6837586760520935,
      "epoch": 6.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1941,
      "total_loss": 0.6837586760520935
    },
    {
      "classification_loss": 0.6509231328964233,
      "epoch": 6.367213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1942,
      "total_loss": 0.6509231328964233
    },
    {
      "classification_loss": 0.5488735437393188,
      "epoch": 6.370491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1943,
      "total_loss": 0.5488735437393188
    },
    {
      "classification_loss": 0.639371931552887,
      "epoch": 6.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1944,
      "total_loss": 0.639371931552887
    },
    {
      "classification_loss": 0.5333353877067566,
      "epoch": 6.377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1945,
      "total_loss": 0.5333353877067566
    },
    {
      "classification_loss": 0.5831144452095032,
      "epoch": 6.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1946,
      "total_loss": 0.5831144452095032
    },
    {
      "classification_loss": 0.4963662028312683,
      "epoch": 6.383606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1947,
      "total_loss": 0.4963662028312683
    },
    {
      "classification_loss": 0.672589123249054,
      "epoch": 6.386885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1948,
      "total_loss": 0.672589123249054
    },
    {
      "classification_loss": 0.5263398885726929,
      "epoch": 6.390163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1949,
      "total_loss": 0.5263398885726929
    },
    {
      "classification_loss": 0.4639035761356354,
      "epoch": 6.39344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1950,
      "total_loss": 0.4639035761356354
    },
    {
      "classification_loss": 0.5975196361541748,
      "epoch": 6.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1951,
      "total_loss": 0.5975196361541748
    },
    {
      "classification_loss": 0.6428701281547546,
      "epoch": 6.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1952,
      "total_loss": 0.6428701281547546
    },
    {
      "classification_loss": 0.5944926142692566,
      "epoch": 6.4032786885245905,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1953,
      "total_loss": 0.5944926142692566
    },
    {
      "classification_loss": 0.7282991409301758,
      "epoch": 6.406557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1954,
      "total_loss": 0.7282991409301758
    },
    {
      "classification_loss": 0.6860831379890442,
      "epoch": 6.409836065573771,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1955,
      "total_loss": 0.6860831379890442
    },
    {
      "classification_loss": 0.5990201234817505,
      "epoch": 6.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1956,
      "total_loss": 0.5990201234817505
    },
    {
      "classification_loss": 0.5639192461967468,
      "epoch": 6.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1957,
      "total_loss": 0.5639192461967468
    },
    {
      "classification_loss": 0.6694509387016296,
      "epoch": 6.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1958,
      "total_loss": 0.6694509387016296
    },
    {
      "classification_loss": 0.6682469248771667,
      "epoch": 6.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1959,
      "total_loss": 0.6682469248771667
    },
    {
      "classification_loss": 0.5653190016746521,
      "epoch": 6.426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1960,
      "total_loss": 0.5653190016746521
    },
    {
      "classification_loss": 0.5196607708930969,
      "epoch": 6.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1961,
      "total_loss": 0.5196607708930969
    },
    {
      "classification_loss": 0.5413677096366882,
      "epoch": 6.432786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1962,
      "total_loss": 0.5413677096366882
    },
    {
      "classification_loss": 0.5406394600868225,
      "epoch": 6.436065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1963,
      "total_loss": 0.5406394600868225
    },
    {
      "classification_loss": 0.5935238599777222,
      "epoch": 6.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1964,
      "total_loss": 0.5935238599777222
    },
    {
      "classification_loss": 0.6249373555183411,
      "epoch": 6.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1965,
      "total_loss": 0.6249373555183411
    },
    {
      "classification_loss": 0.583785355091095,
      "epoch": 6.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1966,
      "total_loss": 0.583785355091095
    },
    {
      "classification_loss": 0.5291670560836792,
      "epoch": 6.449180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1967,
      "total_loss": 0.5291670560836792
    },
    {
      "classification_loss": 0.49045923352241516,
      "epoch": 6.452459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1968,
      "total_loss": 0.49045923352241516
    },
    {
      "classification_loss": 0.5749019384384155,
      "epoch": 6.4557377049180324,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1969,
      "total_loss": 0.5749019384384155
    },
    {
      "classification_loss": 0.6093562245368958,
      "epoch": 6.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1970,
      "total_loss": 0.6093562245368958
    },
    {
      "classification_loss": 0.5469800233840942,
      "epoch": 6.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1971,
      "total_loss": 0.5469800233840942
    },
    {
      "classification_loss": 0.5206718444824219,
      "epoch": 6.465573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1972,
      "total_loss": 0.5206718444824219
    },
    {
      "classification_loss": 0.5375683903694153,
      "epoch": 6.468852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1973,
      "total_loss": 0.5375683903694153
    },
    {
      "classification_loss": 0.579902708530426,
      "epoch": 6.472131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1974,
      "total_loss": 0.579902708530426
    },
    {
      "classification_loss": 0.5228362679481506,
      "epoch": 6.475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1975,
      "total_loss": 0.5228362679481506
    },
    {
      "classification_loss": 0.6066150069236755,
      "epoch": 6.478688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1976,
      "total_loss": 0.6066150069236755
    },
    {
      "classification_loss": 0.48448503017425537,
      "epoch": 6.481967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1977,
      "total_loss": 0.48448503017425537
    },
    {
      "classification_loss": 0.6770217418670654,
      "epoch": 6.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1978,
      "total_loss": 0.6770217418670654
    },
    {
      "classification_loss": 0.47574952244758606,
      "epoch": 6.488524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1979,
      "total_loss": 0.47574952244758606
    },
    {
      "classification_loss": 0.497102290391922,
      "epoch": 6.491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1980,
      "total_loss": 0.497102290391922
    },
    {
      "classification_loss": 0.6027023792266846,
      "epoch": 6.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1981,
      "total_loss": 0.6027023792266846
    },
    {
      "classification_loss": 0.5244691371917725,
      "epoch": 6.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1982,
      "total_loss": 0.5244691371917725
    },
    {
      "classification_loss": 0.6051304340362549,
      "epoch": 6.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1983,
      "total_loss": 0.6051304340362549
    },
    {
      "classification_loss": 0.6525186896324158,
      "epoch": 6.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1984,
      "total_loss": 0.6525186896324158
    },
    {
      "classification_loss": 0.4799674451351166,
      "epoch": 6.508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1985,
      "total_loss": 0.4799674451351166
    },
    {
      "classification_loss": 0.6086431741714478,
      "epoch": 6.511475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1986,
      "total_loss": 0.6086431741714478
    },
    {
      "classification_loss": 0.615989625453949,
      "epoch": 6.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1987,
      "total_loss": 0.615989625453949
    },
    {
      "classification_loss": 0.6696933507919312,
      "epoch": 6.518032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1988,
      "total_loss": 0.6696933507919312
    },
    {
      "classification_loss": 0.5169110298156738,
      "epoch": 6.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1989,
      "total_loss": 0.5169110298156738
    },
    {
      "classification_loss": 0.5121592879295349,
      "epoch": 6.524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1990,
      "total_loss": 0.5121592879295349
    },
    {
      "classification_loss": 0.48339328169822693,
      "epoch": 6.527868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1991,
      "total_loss": 0.48339328169822693
    },
    {
      "classification_loss": 0.4601117968559265,
      "epoch": 6.531147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1992,
      "total_loss": 0.4601117968559265
    },
    {
      "classification_loss": 0.6889253258705139,
      "epoch": 6.534426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1993,
      "total_loss": 0.6889253258705139
    },
    {
      "classification_loss": 0.6248493194580078,
      "epoch": 6.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1994,
      "total_loss": 0.6248493194580078
    },
    {
      "classification_loss": 0.48045387864112854,
      "epoch": 6.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1995,
      "total_loss": 0.48045387864112854
    },
    {
      "classification_loss": 0.5777323842048645,
      "epoch": 6.5442622950819676,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1996,
      "total_loss": 0.5777323842048645
    },
    {
      "classification_loss": 0.6006425023078918,
      "epoch": 6.547540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1997,
      "total_loss": 0.6006425023078918
    },
    {
      "classification_loss": 0.5898413062095642,
      "epoch": 6.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1998,
      "total_loss": 0.5898413062095642
    },
    {
      "classification_loss": 0.6662626266479492,
      "epoch": 6.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 1999,
      "total_loss": 0.6662626266479492
    },
    {
      "epoch": 6.557377049180328,
      "grad_norm": 2.9474213123321533,
      "learning_rate": 0.00013670000000000002,
      "loss": 0.5797,
      "step": 2000
    },
    {
      "classification_loss": 0.7436827421188354,
      "epoch": 6.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2000,
      "total_loss": 0.7436827421188354
    },
    {
      "classification_loss": 0.5597279667854309,
      "epoch": 6.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2001,
      "total_loss": 0.5597279667854309
    },
    {
      "classification_loss": 0.5750166773796082,
      "epoch": 6.563934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2002,
      "total_loss": 0.5750166773796082
    },
    {
      "classification_loss": 0.599388062953949,
      "epoch": 6.567213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2003,
      "total_loss": 0.599388062953949
    },
    {
      "classification_loss": 0.6171501874923706,
      "epoch": 6.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2004,
      "total_loss": 0.6171501874923706
    },
    {
      "classification_loss": 0.5993971228599548,
      "epoch": 6.573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2005,
      "total_loss": 0.5993971228599548
    },
    {
      "classification_loss": 0.5589185953140259,
      "epoch": 6.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2006,
      "total_loss": 0.5589185953140259
    },
    {
      "classification_loss": 0.5836004614830017,
      "epoch": 6.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2007,
      "total_loss": 0.5836004614830017
    },
    {
      "classification_loss": 0.6420932412147522,
      "epoch": 6.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2008,
      "total_loss": 0.6420932412147522
    },
    {
      "classification_loss": 0.5580072402954102,
      "epoch": 6.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2009,
      "total_loss": 0.5580072402954102
    },
    {
      "classification_loss": 0.6760755181312561,
      "epoch": 6.590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2010,
      "total_loss": 0.6760755181312561
    },
    {
      "classification_loss": 0.6618524193763733,
      "epoch": 6.593442622950819,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2011,
      "total_loss": 0.6618524193763733
    },
    {
      "classification_loss": 0.573767364025116,
      "epoch": 6.5967213114754095,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2012,
      "total_loss": 0.573767364025116
    },
    {
      "classification_loss": 0.5814428925514221,
      "epoch": 6.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2013,
      "total_loss": 0.5814428925514221
    },
    {
      "classification_loss": 0.613120973110199,
      "epoch": 6.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2014,
      "total_loss": 0.613120973110199
    },
    {
      "classification_loss": 0.6347377896308899,
      "epoch": 6.60655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2015,
      "total_loss": 0.6347377896308899
    },
    {
      "classification_loss": 0.6108342409133911,
      "epoch": 6.60983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2016,
      "total_loss": 0.6108342409133911
    },
    {
      "classification_loss": 0.5582910180091858,
      "epoch": 6.613114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2017,
      "total_loss": 0.5582910180091858
    },
    {
      "classification_loss": 0.48864954710006714,
      "epoch": 6.616393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2018,
      "total_loss": 0.48864954710006714
    },
    {
      "classification_loss": 0.6938738822937012,
      "epoch": 6.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2019,
      "total_loss": 0.6938738822937012
    },
    {
      "classification_loss": 0.5748821496963501,
      "epoch": 6.622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2020,
      "total_loss": 0.5748821496963501
    },
    {
      "classification_loss": 0.5322710275650024,
      "epoch": 6.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2021,
      "total_loss": 0.5322710275650024
    },
    {
      "classification_loss": 0.6518084406852722,
      "epoch": 6.629508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2022,
      "total_loss": 0.6518084406852722
    },
    {
      "classification_loss": 0.567948043346405,
      "epoch": 6.632786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2023,
      "total_loss": 0.567948043346405
    },
    {
      "classification_loss": 0.525667130947113,
      "epoch": 6.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2024,
      "total_loss": 0.525667130947113
    },
    {
      "classification_loss": 0.56888347864151,
      "epoch": 6.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2025,
      "total_loss": 0.56888347864151
    },
    {
      "classification_loss": 0.5018640756607056,
      "epoch": 6.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2026,
      "total_loss": 0.5018640756607056
    },
    {
      "classification_loss": 0.571433424949646,
      "epoch": 6.645901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2027,
      "total_loss": 0.571433424949646
    },
    {
      "classification_loss": 0.544545590877533,
      "epoch": 6.649180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2028,
      "total_loss": 0.544545590877533
    },
    {
      "classification_loss": 0.5788822174072266,
      "epoch": 6.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2029,
      "total_loss": 0.5788822174072266
    },
    {
      "classification_loss": 0.5623259544372559,
      "epoch": 6.655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2030,
      "total_loss": 0.5623259544372559
    },
    {
      "classification_loss": 0.5050141215324402,
      "epoch": 6.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2031,
      "total_loss": 0.5050141215324402
    },
    {
      "classification_loss": 0.5579201579093933,
      "epoch": 6.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2032,
      "total_loss": 0.5579201579093933
    },
    {
      "classification_loss": 0.5360819101333618,
      "epoch": 6.665573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2033,
      "total_loss": 0.5360819101333618
    },
    {
      "classification_loss": 0.6458388566970825,
      "epoch": 6.668852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2034,
      "total_loss": 0.6458388566970825
    },
    {
      "classification_loss": 0.586941659450531,
      "epoch": 6.672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2035,
      "total_loss": 0.586941659450531
    },
    {
      "classification_loss": 0.6860268115997314,
      "epoch": 6.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2036,
      "total_loss": 0.6860268115997314
    },
    {
      "classification_loss": 0.48096951842308044,
      "epoch": 6.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2037,
      "total_loss": 0.48096951842308044
    },
    {
      "classification_loss": 0.5633184909820557,
      "epoch": 6.6819672131147545,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2038,
      "total_loss": 0.5633184909820557
    },
    {
      "classification_loss": 0.5635315775871277,
      "epoch": 6.685245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2039,
      "total_loss": 0.5635315775871277
    },
    {
      "classification_loss": 0.5826411843299866,
      "epoch": 6.688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2040,
      "total_loss": 0.5826411843299866
    },
    {
      "classification_loss": 0.5847616195678711,
      "epoch": 6.691803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2041,
      "total_loss": 0.5847616195678711
    },
    {
      "classification_loss": 0.6057602763175964,
      "epoch": 6.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2042,
      "total_loss": 0.6057602763175964
    },
    {
      "classification_loss": 0.47946634888648987,
      "epoch": 6.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2043,
      "total_loss": 0.47946634888648987
    },
    {
      "classification_loss": 0.5664223432540894,
      "epoch": 6.701639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2044,
      "total_loss": 0.5664223432540894
    },
    {
      "classification_loss": 0.6467429995536804,
      "epoch": 6.704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2045,
      "total_loss": 0.6467429995536804
    },
    {
      "classification_loss": 0.7101477384567261,
      "epoch": 6.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2046,
      "total_loss": 0.7101477384567261
    },
    {
      "classification_loss": 0.6228910684585571,
      "epoch": 6.711475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2047,
      "total_loss": 0.6228910684585571
    },
    {
      "classification_loss": 0.6009197235107422,
      "epoch": 6.714754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2048,
      "total_loss": 0.6009197235107422
    },
    {
      "classification_loss": 0.5124306678771973,
      "epoch": 6.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2049,
      "total_loss": 0.5124306678771973
    },
    {
      "classification_loss": 0.43942391872406006,
      "epoch": 6.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2050,
      "total_loss": 0.43942391872406006
    },
    {
      "classification_loss": 0.6207420825958252,
      "epoch": 6.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2051,
      "total_loss": 0.6207420825958252
    },
    {
      "classification_loss": 0.5555984973907471,
      "epoch": 6.727868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2052,
      "total_loss": 0.5555984973907471
    },
    {
      "classification_loss": 0.5483368039131165,
      "epoch": 6.731147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2053,
      "total_loss": 0.5483368039131165
    },
    {
      "classification_loss": 0.5060427188873291,
      "epoch": 6.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2054,
      "total_loss": 0.5060427188873291
    },
    {
      "classification_loss": 0.5161533355712891,
      "epoch": 6.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2055,
      "total_loss": 0.5161533355712891
    },
    {
      "classification_loss": 0.5859631896018982,
      "epoch": 6.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2056,
      "total_loss": 0.5859631896018982
    },
    {
      "classification_loss": 0.5864253044128418,
      "epoch": 6.744262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2057,
      "total_loss": 0.5864253044128418
    },
    {
      "classification_loss": 0.5226268768310547,
      "epoch": 6.747540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2058,
      "total_loss": 0.5226268768310547
    },
    {
      "classification_loss": 0.6186783313751221,
      "epoch": 6.750819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2059,
      "total_loss": 0.6186783313751221
    },
    {
      "classification_loss": 0.5270388126373291,
      "epoch": 6.754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2060,
      "total_loss": 0.5270388126373291
    },
    {
      "classification_loss": 0.5138880610466003,
      "epoch": 6.757377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2061,
      "total_loss": 0.5138880610466003
    },
    {
      "classification_loss": 0.6065405607223511,
      "epoch": 6.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2062,
      "total_loss": 0.6065405607223511
    },
    {
      "classification_loss": 0.6352490782737732,
      "epoch": 6.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2063,
      "total_loss": 0.6352490782737732
    },
    {
      "classification_loss": 0.608647882938385,
      "epoch": 6.767213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2064,
      "total_loss": 0.608647882938385
    },
    {
      "classification_loss": 0.707066535949707,
      "epoch": 6.770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2065,
      "total_loss": 0.707066535949707
    },
    {
      "classification_loss": 0.6488556265830994,
      "epoch": 6.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2066,
      "total_loss": 0.6488556265830994
    },
    {
      "classification_loss": 0.5585483312606812,
      "epoch": 6.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2067,
      "total_loss": 0.5585483312606812
    },
    {
      "classification_loss": 0.578680157661438,
      "epoch": 6.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2068,
      "total_loss": 0.578680157661438
    },
    {
      "classification_loss": 0.5029004812240601,
      "epoch": 6.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2069,
      "total_loss": 0.5029004812240601
    },
    {
      "classification_loss": 0.6499390602111816,
      "epoch": 6.786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2070,
      "total_loss": 0.6499390602111816
    },
    {
      "classification_loss": 0.6298267841339111,
      "epoch": 6.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2071,
      "total_loss": 0.6298267841339111
    },
    {
      "classification_loss": 0.6127386689186096,
      "epoch": 6.79344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2072,
      "total_loss": 0.6127386689186096
    },
    {
      "classification_loss": 0.6436683535575867,
      "epoch": 6.79672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2073,
      "total_loss": 0.6436683535575867
    },
    {
      "classification_loss": 0.5444150567054749,
      "epoch": 6.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2074,
      "total_loss": 0.5444150567054749
    },
    {
      "classification_loss": 0.5528721809387207,
      "epoch": 6.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2075,
      "total_loss": 0.5528721809387207
    },
    {
      "classification_loss": 0.6114223599433899,
      "epoch": 6.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2076,
      "total_loss": 0.6114223599433899
    },
    {
      "classification_loss": 0.4814441502094269,
      "epoch": 6.80983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2077,
      "total_loss": 0.4814441502094269
    },
    {
      "classification_loss": 0.5243132710456848,
      "epoch": 6.81311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2078,
      "total_loss": 0.5243132710456848
    },
    {
      "classification_loss": 0.640124499797821,
      "epoch": 6.81639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2079,
      "total_loss": 0.640124499797821
    },
    {
      "classification_loss": 0.5728432536125183,
      "epoch": 6.8196721311475414,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2080,
      "total_loss": 0.5728432536125183
    },
    {
      "classification_loss": 0.5809564590454102,
      "epoch": 6.822950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2081,
      "total_loss": 0.5809564590454102
    },
    {
      "classification_loss": 0.5452730059623718,
      "epoch": 6.826229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2082,
      "total_loss": 0.5452730059623718
    },
    {
      "classification_loss": 0.49213486909866333,
      "epoch": 6.829508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2083,
      "total_loss": 0.49213486909866333
    },
    {
      "classification_loss": 0.5895675420761108,
      "epoch": 6.832786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2084,
      "total_loss": 0.5895675420761108
    },
    {
      "classification_loss": 0.625373125076294,
      "epoch": 6.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2085,
      "total_loss": 0.625373125076294
    },
    {
      "classification_loss": 0.6014280319213867,
      "epoch": 6.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2086,
      "total_loss": 0.6014280319213867
    },
    {
      "classification_loss": 0.5518819689750671,
      "epoch": 6.842622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2087,
      "total_loss": 0.5518819689750671
    },
    {
      "classification_loss": 0.6500796675682068,
      "epoch": 6.845901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2088,
      "total_loss": 0.6500796675682068
    },
    {
      "classification_loss": 0.6509298086166382,
      "epoch": 6.849180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2089,
      "total_loss": 0.6509298086166382
    },
    {
      "classification_loss": 0.5926067233085632,
      "epoch": 6.852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2090,
      "total_loss": 0.5926067233085632
    },
    {
      "classification_loss": 0.6556008458137512,
      "epoch": 6.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2091,
      "total_loss": 0.6556008458137512
    },
    {
      "classification_loss": 0.6229910850524902,
      "epoch": 6.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2092,
      "total_loss": 0.6229910850524902
    },
    {
      "classification_loss": 0.5674539804458618,
      "epoch": 6.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2093,
      "total_loss": 0.5674539804458618
    },
    {
      "classification_loss": 0.6323913335800171,
      "epoch": 6.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2094,
      "total_loss": 0.6323913335800171
    },
    {
      "classification_loss": 0.5893818736076355,
      "epoch": 6.868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2095,
      "total_loss": 0.5893818736076355
    },
    {
      "classification_loss": 0.5363665819168091,
      "epoch": 6.872131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2096,
      "total_loss": 0.5363665819168091
    },
    {
      "classification_loss": 0.5964040160179138,
      "epoch": 6.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2097,
      "total_loss": 0.5964040160179138
    },
    {
      "classification_loss": 0.5231974720954895,
      "epoch": 6.878688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2098,
      "total_loss": 0.5231974720954895
    },
    {
      "classification_loss": 0.5569753646850586,
      "epoch": 6.881967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2099,
      "total_loss": 0.5569753646850586
    },
    {
      "epoch": 6.885245901639344,
      "grad_norm": 6.427957057952881,
      "learning_rate": 0.00013336666666666666,
      "loss": 0.5836,
      "step": 2100
    },
    {
      "classification_loss": 0.5096527338027954,
      "epoch": 6.885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2100,
      "total_loss": 0.5096527338027954
    },
    {
      "classification_loss": 0.5969736576080322,
      "epoch": 6.888524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2101,
      "total_loss": 0.5969736576080322
    },
    {
      "classification_loss": 0.6810275316238403,
      "epoch": 6.891803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2102,
      "total_loss": 0.6810275316238403
    },
    {
      "classification_loss": 0.6406946778297424,
      "epoch": 6.895081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2103,
      "total_loss": 0.6406946778297424
    },
    {
      "classification_loss": 0.5494080781936646,
      "epoch": 6.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2104,
      "total_loss": 0.5494080781936646
    },
    {
      "classification_loss": 0.629777729511261,
      "epoch": 6.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2105,
      "total_loss": 0.629777729511261
    },
    {
      "classification_loss": 0.681940495967865,
      "epoch": 6.9049180327868855,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2106,
      "total_loss": 0.681940495967865
    },
    {
      "classification_loss": 0.6487393379211426,
      "epoch": 6.908196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2107,
      "total_loss": 0.6487393379211426
    },
    {
      "classification_loss": 0.626671552658081,
      "epoch": 6.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2108,
      "total_loss": 0.626671552658081
    },
    {
      "classification_loss": 0.5741936564445496,
      "epoch": 6.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2109,
      "total_loss": 0.5741936564445496
    },
    {
      "classification_loss": 0.5792255401611328,
      "epoch": 6.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2110,
      "total_loss": 0.5792255401611328
    },
    {
      "classification_loss": 0.6487634181976318,
      "epoch": 6.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2111,
      "total_loss": 0.6487634181976318
    },
    {
      "classification_loss": 0.6320961713790894,
      "epoch": 6.924590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2112,
      "total_loss": 0.6320961713790894
    },
    {
      "classification_loss": 0.5962474346160889,
      "epoch": 6.927868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2113,
      "total_loss": 0.5962474346160889
    },
    {
      "classification_loss": 0.5486433506011963,
      "epoch": 6.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2114,
      "total_loss": 0.5486433506011963
    },
    {
      "classification_loss": 0.5775031447410583,
      "epoch": 6.934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2115,
      "total_loss": 0.5775031447410583
    },
    {
      "classification_loss": 0.5805951952934265,
      "epoch": 6.937704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2116,
      "total_loss": 0.5805951952934265
    },
    {
      "classification_loss": 0.5411761403083801,
      "epoch": 6.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2117,
      "total_loss": 0.5411761403083801
    },
    {
      "classification_loss": 0.6263130307197571,
      "epoch": 6.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2118,
      "total_loss": 0.6263130307197571
    },
    {
      "classification_loss": 0.683358371257782,
      "epoch": 6.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2119,
      "total_loss": 0.683358371257782
    },
    {
      "classification_loss": 0.6976550221443176,
      "epoch": 6.950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2120,
      "total_loss": 0.6976550221443176
    },
    {
      "classification_loss": 0.5971542596817017,
      "epoch": 6.954098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2121,
      "total_loss": 0.5971542596817017
    },
    {
      "classification_loss": 0.5799683332443237,
      "epoch": 6.9573770491803275,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2122,
      "total_loss": 0.5799683332443237
    },
    {
      "classification_loss": 0.572638988494873,
      "epoch": 6.9606557377049185,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2123,
      "total_loss": 0.572638988494873
    },
    {
      "classification_loss": 0.49484556913375854,
      "epoch": 6.963934426229509,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2124,
      "total_loss": 0.49484556913375854
    },
    {
      "classification_loss": 0.6596946716308594,
      "epoch": 6.967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2125,
      "total_loss": 0.6596946716308594
    },
    {
      "classification_loss": 0.6206018924713135,
      "epoch": 6.970491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2126,
      "total_loss": 0.6206018924713135
    },
    {
      "classification_loss": 0.6198456287384033,
      "epoch": 6.973770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2127,
      "total_loss": 0.6198456287384033
    },
    {
      "classification_loss": 0.5676701664924622,
      "epoch": 6.977049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2128,
      "total_loss": 0.5676701664924622
    },
    {
      "classification_loss": 0.5738524794578552,
      "epoch": 6.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2129,
      "total_loss": 0.5738524794578552
    },
    {
      "classification_loss": 0.5069800615310669,
      "epoch": 6.983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2130,
      "total_loss": 0.5069800615310669
    },
    {
      "classification_loss": 0.5972415804862976,
      "epoch": 6.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2131,
      "total_loss": 0.5972415804862976
    },
    {
      "classification_loss": 0.6393265724182129,
      "epoch": 6.99016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2132,
      "total_loss": 0.6393265724182129
    },
    {
      "classification_loss": 0.5486496090888977,
      "epoch": 6.99344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2133,
      "total_loss": 0.5486496090888977
    },
    {
      "classification_loss": 0.47165313363075256,
      "epoch": 6.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2134,
      "total_loss": 0.47165313363075256
    },
    {
      "classification_loss": 0.9511309862136841,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.9511309862136841
    },
    {
      "classification_loss": 0.972417414188385,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.972417414188385
    },
    {
      "classification_loss": 0.8920556306838989,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.8920556306838989
    },
    {
      "classification_loss": 0.9727398753166199,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.9727398753166199
    },
    {
      "classification_loss": 1.0241119861602783,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 1.0241119861602783
    },
    {
      "classification_loss": 0.9100872874259949,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.9100872874259949
    },
    {
      "classification_loss": 0.9981068968772888,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.9981068968772888
    },
    {
      "classification_loss": 1.035615086555481,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 1.035615086555481
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.383,
      "eval_f1": 0.009630818619582664,
      "eval_loss": 0.9679471254348755,
      "eval_precision": 0.42857142857142855,
      "eval_recall": 0.00487012987012987,
      "eval_runtime": 5.9805,
      "eval_samples_per_second": 167.209,
      "eval_steps_per_second": 1.338,
      "step": 2135
    },
    {
      "classification_loss": 0.4707390069961548,
      "epoch": 7.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2135,
      "total_loss": 0.4707390069961548
    },
    {
      "classification_loss": 0.5991479158401489,
      "epoch": 7.00327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2136,
      "total_loss": 0.5991479158401489
    },
    {
      "classification_loss": 0.6056131720542908,
      "epoch": 7.00655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2137,
      "total_loss": 0.6056131720542908
    },
    {
      "classification_loss": 0.5610402226448059,
      "epoch": 7.00983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2138,
      "total_loss": 0.5610402226448059
    },
    {
      "classification_loss": 0.4882742464542389,
      "epoch": 7.0131147540983605,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2139,
      "total_loss": 0.4882742464542389
    },
    {
      "classification_loss": 0.5914344191551208,
      "epoch": 7.016393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2140,
      "total_loss": 0.5914344191551208
    },
    {
      "classification_loss": 0.5904706120491028,
      "epoch": 7.019672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2141,
      "total_loss": 0.5904706120491028
    },
    {
      "classification_loss": 0.5244733691215515,
      "epoch": 7.022950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2142,
      "total_loss": 0.5244733691215515
    },
    {
      "classification_loss": 0.5827012658119202,
      "epoch": 7.026229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2143,
      "total_loss": 0.5827012658119202
    },
    {
      "classification_loss": 0.5759416818618774,
      "epoch": 7.029508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2144,
      "total_loss": 0.5759416818618774
    },
    {
      "classification_loss": 0.5376010537147522,
      "epoch": 7.032786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2145,
      "total_loss": 0.5376010537147522
    },
    {
      "classification_loss": 0.6185182929039001,
      "epoch": 7.036065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2146,
      "total_loss": 0.6185182929039001
    },
    {
      "classification_loss": 0.6511136889457703,
      "epoch": 7.039344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2147,
      "total_loss": 0.6511136889457703
    },
    {
      "classification_loss": 0.5457909107208252,
      "epoch": 7.0426229508196725,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2148,
      "total_loss": 0.5457909107208252
    },
    {
      "classification_loss": 0.6425703763961792,
      "epoch": 7.045901639344263,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2149,
      "total_loss": 0.6425703763961792
    },
    {
      "classification_loss": 0.5583280920982361,
      "epoch": 7.049180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2150,
      "total_loss": 0.5583280920982361
    },
    {
      "classification_loss": 0.587374210357666,
      "epoch": 7.052459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2151,
      "total_loss": 0.587374210357666
    },
    {
      "classification_loss": 0.5665491819381714,
      "epoch": 7.055737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2152,
      "total_loss": 0.5665491819381714
    },
    {
      "classification_loss": 0.5753453373908997,
      "epoch": 7.059016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2153,
      "total_loss": 0.5753453373908997
    },
    {
      "classification_loss": 0.5835581421852112,
      "epoch": 7.062295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2154,
      "total_loss": 0.5835581421852112
    },
    {
      "classification_loss": 0.5734714865684509,
      "epoch": 7.065573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2155,
      "total_loss": 0.5734714865684509
    },
    {
      "classification_loss": 0.5199705362319946,
      "epoch": 7.0688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2156,
      "total_loss": 0.5199705362319946
    },
    {
      "classification_loss": 0.7241769433021545,
      "epoch": 7.072131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2157,
      "total_loss": 0.7241769433021545
    },
    {
      "classification_loss": 0.6044909358024597,
      "epoch": 7.075409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2158,
      "total_loss": 0.6044909358024597
    },
    {
      "classification_loss": 0.6461160182952881,
      "epoch": 7.078688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2159,
      "total_loss": 0.6461160182952881
    },
    {
      "classification_loss": 0.5472989678382874,
      "epoch": 7.081967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2160,
      "total_loss": 0.5472989678382874
    },
    {
      "classification_loss": 0.5083603262901306,
      "epoch": 7.085245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2161,
      "total_loss": 0.5083603262901306
    },
    {
      "classification_loss": 0.5274423360824585,
      "epoch": 7.088524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2162,
      "total_loss": 0.5274423360824585
    },
    {
      "classification_loss": 0.6032254099845886,
      "epoch": 7.091803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2163,
      "total_loss": 0.6032254099845886
    },
    {
      "classification_loss": 0.5337405204772949,
      "epoch": 7.0950819672131145,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2164,
      "total_loss": 0.5337405204772949
    },
    {
      "classification_loss": 0.5576338171958923,
      "epoch": 7.098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2165,
      "total_loss": 0.5576338171958923
    },
    {
      "classification_loss": 0.6209521889686584,
      "epoch": 7.101639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2166,
      "total_loss": 0.6209521889686584
    },
    {
      "classification_loss": 0.4655783772468567,
      "epoch": 7.104918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2167,
      "total_loss": 0.4655783772468567
    },
    {
      "classification_loss": 0.6478050351142883,
      "epoch": 7.108196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2168,
      "total_loss": 0.6478050351142883
    },
    {
      "classification_loss": 0.5341127514839172,
      "epoch": 7.111475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2169,
      "total_loss": 0.5341127514839172
    },
    {
      "classification_loss": 0.5395431518554688,
      "epoch": 7.114754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2170,
      "total_loss": 0.5395431518554688
    },
    {
      "classification_loss": 0.5408276915550232,
      "epoch": 7.118032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2171,
      "total_loss": 0.5408276915550232
    },
    {
      "classification_loss": 0.5330021381378174,
      "epoch": 7.121311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2172,
      "total_loss": 0.5330021381378174
    },
    {
      "classification_loss": 0.5812336802482605,
      "epoch": 7.1245901639344265,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2173,
      "total_loss": 0.5812336802482605
    },
    {
      "classification_loss": 0.6621126532554626,
      "epoch": 7.127868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2174,
      "total_loss": 0.6621126532554626
    },
    {
      "classification_loss": 0.5687897205352783,
      "epoch": 7.131147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2175,
      "total_loss": 0.5687897205352783
    },
    {
      "classification_loss": 0.4927796423435211,
      "epoch": 7.134426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2176,
      "total_loss": 0.4927796423435211
    },
    {
      "classification_loss": 0.6075738072395325,
      "epoch": 7.137704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2177,
      "total_loss": 0.6075738072395325
    },
    {
      "classification_loss": 0.49745163321495056,
      "epoch": 7.140983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2178,
      "total_loss": 0.49745163321495056
    },
    {
      "classification_loss": 0.49895718693733215,
      "epoch": 7.144262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2179,
      "total_loss": 0.49895718693733215
    },
    {
      "classification_loss": 0.5805128216743469,
      "epoch": 7.147540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2180,
      "total_loss": 0.5805128216743469
    },
    {
      "classification_loss": 0.5606132745742798,
      "epoch": 7.150819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2181,
      "total_loss": 0.5606132745742798
    },
    {
      "classification_loss": 0.5280190110206604,
      "epoch": 7.154098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2182,
      "total_loss": 0.5280190110206604
    },
    {
      "classification_loss": 0.6365953683853149,
      "epoch": 7.157377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2183,
      "total_loss": 0.6365953683853149
    },
    {
      "classification_loss": 0.5111026167869568,
      "epoch": 7.160655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2184,
      "total_loss": 0.5111026167869568
    },
    {
      "classification_loss": 0.6415678858757019,
      "epoch": 7.163934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2185,
      "total_loss": 0.6415678858757019
    },
    {
      "classification_loss": 0.5382848978042603,
      "epoch": 7.167213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2186,
      "total_loss": 0.5382848978042603
    },
    {
      "classification_loss": 0.576820969581604,
      "epoch": 7.170491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2187,
      "total_loss": 0.576820969581604
    },
    {
      "classification_loss": 0.4970181882381439,
      "epoch": 7.173770491803278,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2188,
      "total_loss": 0.4970181882381439
    },
    {
      "classification_loss": 0.629335343837738,
      "epoch": 7.177049180327868,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2189,
      "total_loss": 0.629335343837738
    },
    {
      "classification_loss": 0.547424852848053,
      "epoch": 7.180327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2190,
      "total_loss": 0.547424852848053
    },
    {
      "classification_loss": 0.5409608483314514,
      "epoch": 7.18360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2191,
      "total_loss": 0.5409608483314514
    },
    {
      "classification_loss": 0.45569947361946106,
      "epoch": 7.18688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2192,
      "total_loss": 0.45569947361946106
    },
    {
      "classification_loss": 0.6152675151824951,
      "epoch": 7.19016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2193,
      "total_loss": 0.6152675151824951
    },
    {
      "classification_loss": 0.5844441652297974,
      "epoch": 7.19344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2194,
      "total_loss": 0.5844441652297974
    },
    {
      "classification_loss": 0.52587890625,
      "epoch": 7.19672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2195,
      "total_loss": 0.52587890625
    },
    {
      "classification_loss": 0.5617908835411072,
      "epoch": 7.2,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2196,
      "total_loss": 0.5617908835411072
    },
    {
      "classification_loss": 0.6851930022239685,
      "epoch": 7.20327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2197,
      "total_loss": 0.6851930022239685
    },
    {
      "classification_loss": 0.5567874908447266,
      "epoch": 7.20655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2198,
      "total_loss": 0.5567874908447266
    },
    {
      "classification_loss": 0.5328878164291382,
      "epoch": 7.2098360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2199,
      "total_loss": 0.5328878164291382
    },
    {
      "epoch": 7.213114754098361,
      "grad_norm": 6.9794816970825195,
      "learning_rate": 0.00013003333333333334,
      "loss": 0.578,
      "step": 2200
    },
    {
      "classification_loss": 0.5524949431419373,
      "epoch": 7.213114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2200,
      "total_loss": 0.5524949431419373
    },
    {
      "classification_loss": 0.6515294909477234,
      "epoch": 7.216393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2201,
      "total_loss": 0.6515294909477234
    },
    {
      "classification_loss": 0.5295615196228027,
      "epoch": 7.219672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2202,
      "total_loss": 0.5295615196228027
    },
    {
      "classification_loss": 0.5485080480575562,
      "epoch": 7.222950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2203,
      "total_loss": 0.5485080480575562
    },
    {
      "classification_loss": 0.49933579564094543,
      "epoch": 7.226229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2204,
      "total_loss": 0.49933579564094543
    },
    {
      "classification_loss": 0.6376144289970398,
      "epoch": 7.229508196721311,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2205,
      "total_loss": 0.6376144289970398
    },
    {
      "classification_loss": 0.5514587163925171,
      "epoch": 7.232786885245901,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2206,
      "total_loss": 0.5514587163925171
    },
    {
      "classification_loss": 0.5564182996749878,
      "epoch": 7.2360655737704915,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2207,
      "total_loss": 0.5564182996749878
    },
    {
      "classification_loss": 0.5019485354423523,
      "epoch": 7.239344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2208,
      "total_loss": 0.5019485354423523
    },
    {
      "classification_loss": 0.5780033469200134,
      "epoch": 7.242622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2209,
      "total_loss": 0.5780033469200134
    },
    {
      "classification_loss": 0.5423639416694641,
      "epoch": 7.245901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2210,
      "total_loss": 0.5423639416694641
    },
    {
      "classification_loss": 0.528286337852478,
      "epoch": 7.249180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2211,
      "total_loss": 0.528286337852478
    },
    {
      "classification_loss": 0.5892131328582764,
      "epoch": 7.252459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2212,
      "total_loss": 0.5892131328582764
    },
    {
      "classification_loss": 0.5853229761123657,
      "epoch": 7.255737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2213,
      "total_loss": 0.5853229761123657
    },
    {
      "classification_loss": 0.525056004524231,
      "epoch": 7.259016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2214,
      "total_loss": 0.525056004524231
    },
    {
      "classification_loss": 0.5357018113136292,
      "epoch": 7.262295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2215,
      "total_loss": 0.5357018113136292
    },
    {
      "classification_loss": 0.5258357524871826,
      "epoch": 7.2655737704918035,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2216,
      "total_loss": 0.5258357524871826
    },
    {
      "classification_loss": 0.48814067244529724,
      "epoch": 7.268852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2217,
      "total_loss": 0.48814067244529724
    },
    {
      "classification_loss": 0.5596319437026978,
      "epoch": 7.272131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2218,
      "total_loss": 0.5596319437026978
    },
    {
      "classification_loss": 0.47210779786109924,
      "epoch": 7.275409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2219,
      "total_loss": 0.47210779786109924
    },
    {
      "classification_loss": 0.5685082674026489,
      "epoch": 7.278688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2220,
      "total_loss": 0.5685082674026489
    },
    {
      "classification_loss": 0.51787930727005,
      "epoch": 7.281967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2221,
      "total_loss": 0.51787930727005
    },
    {
      "classification_loss": 0.5860518217086792,
      "epoch": 7.285245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2222,
      "total_loss": 0.5860518217086792
    },
    {
      "classification_loss": 0.6863916516304016,
      "epoch": 7.288524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2223,
      "total_loss": 0.6863916516304016
    },
    {
      "classification_loss": 0.5421153903007507,
      "epoch": 7.2918032786885245,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2224,
      "total_loss": 0.5421153903007507
    },
    {
      "classification_loss": 0.5348601341247559,
      "epoch": 7.295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2225,
      "total_loss": 0.5348601341247559
    },
    {
      "classification_loss": 0.46573564410209656,
      "epoch": 7.298360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2226,
      "total_loss": 0.46573564410209656
    },
    {
      "classification_loss": 0.5414952635765076,
      "epoch": 7.301639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2227,
      "total_loss": 0.5414952635765076
    },
    {
      "classification_loss": 0.6008152961730957,
      "epoch": 7.304918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2228,
      "total_loss": 0.6008152961730957
    },
    {
      "classification_loss": 0.6117206811904907,
      "epoch": 7.308196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2229,
      "total_loss": 0.6117206811904907
    },
    {
      "classification_loss": 0.6033264994621277,
      "epoch": 7.311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2230,
      "total_loss": 0.6033264994621277
    },
    {
      "classification_loss": 0.5582627058029175,
      "epoch": 7.314754098360655,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2231,
      "total_loss": 0.5582627058029175
    },
    {
      "classification_loss": 0.644048810005188,
      "epoch": 7.3180327868852455,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2232,
      "total_loss": 0.644048810005188
    },
    {
      "classification_loss": 0.5769757032394409,
      "epoch": 7.321311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2233,
      "total_loss": 0.5769757032394409
    },
    {
      "classification_loss": 0.5439237952232361,
      "epoch": 7.324590163934427,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2234,
      "total_loss": 0.5439237952232361
    },
    {
      "classification_loss": 0.5787501931190491,
      "epoch": 7.327868852459017,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2235,
      "total_loss": 0.5787501931190491
    },
    {
      "classification_loss": 0.5392086505889893,
      "epoch": 7.331147540983607,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2236,
      "total_loss": 0.5392086505889893
    },
    {
      "classification_loss": 0.5943610072135925,
      "epoch": 7.334426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2237,
      "total_loss": 0.5943610072135925
    },
    {
      "classification_loss": 0.5948960185050964,
      "epoch": 7.337704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2238,
      "total_loss": 0.5948960185050964
    },
    {
      "classification_loss": 0.6819850206375122,
      "epoch": 7.340983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2239,
      "total_loss": 0.6819850206375122
    },
    {
      "classification_loss": 0.44412434101104736,
      "epoch": 7.344262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2240,
      "total_loss": 0.44412434101104736
    },
    {
      "classification_loss": 0.5980879664421082,
      "epoch": 7.3475409836065575,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2241,
      "total_loss": 0.5980879664421082
    },
    {
      "classification_loss": 0.6502677798271179,
      "epoch": 7.350819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2242,
      "total_loss": 0.6502677798271179
    },
    {
      "classification_loss": 0.5705165863037109,
      "epoch": 7.354098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2243,
      "total_loss": 0.5705165863037109
    },
    {
      "classification_loss": 0.5198127627372742,
      "epoch": 7.357377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2244,
      "total_loss": 0.5198127627372742
    },
    {
      "classification_loss": 0.5154420137405396,
      "epoch": 7.360655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2245,
      "total_loss": 0.5154420137405396
    },
    {
      "classification_loss": 0.5489954352378845,
      "epoch": 7.363934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2246,
      "total_loss": 0.5489954352378845
    },
    {
      "classification_loss": 0.5655993819236755,
      "epoch": 7.367213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2247,
      "total_loss": 0.5655993819236755
    },
    {
      "classification_loss": 0.6460086107254028,
      "epoch": 7.370491803278688,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2248,
      "total_loss": 0.6460086107254028
    },
    {
      "classification_loss": 0.5605905055999756,
      "epoch": 7.3737704918032785,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2249,
      "total_loss": 0.5605905055999756
    },
    {
      "classification_loss": 0.49057555198669434,
      "epoch": 7.377049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2250,
      "total_loss": 0.49057555198669434
    },
    {
      "classification_loss": 0.5831191539764404,
      "epoch": 7.380327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2251,
      "total_loss": 0.5831191539764404
    },
    {
      "classification_loss": 0.5738532543182373,
      "epoch": 7.383606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2252,
      "total_loss": 0.5738532543182373
    },
    {
      "classification_loss": 0.5404742956161499,
      "epoch": 7.386885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2253,
      "total_loss": 0.5404742956161499
    },
    {
      "classification_loss": 0.7213970422744751,
      "epoch": 7.390163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2254,
      "total_loss": 0.7213970422744751
    },
    {
      "classification_loss": 0.6502588391304016,
      "epoch": 7.39344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2255,
      "total_loss": 0.6502588391304016
    },
    {
      "classification_loss": 0.5779232382774353,
      "epoch": 7.39672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2256,
      "total_loss": 0.5779232382774353
    },
    {
      "classification_loss": 0.5479434132575989,
      "epoch": 7.4,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2257,
      "total_loss": 0.5479434132575989
    },
    {
      "classification_loss": 0.5781263113021851,
      "epoch": 7.4032786885245905,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2258,
      "total_loss": 0.5781263113021851
    },
    {
      "classification_loss": 0.6134321689605713,
      "epoch": 7.406557377049181,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2259,
      "total_loss": 0.6134321689605713
    },
    {
      "classification_loss": 0.6116001605987549,
      "epoch": 7.409836065573771,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2260,
      "total_loss": 0.6116001605987549
    },
    {
      "classification_loss": 0.5729891061782837,
      "epoch": 7.413114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2261,
      "total_loss": 0.5729891061782837
    },
    {
      "classification_loss": 0.5627657175064087,
      "epoch": 7.416393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2262,
      "total_loss": 0.5627657175064087
    },
    {
      "classification_loss": 0.601779580116272,
      "epoch": 7.419672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2263,
      "total_loss": 0.601779580116272
    },
    {
      "classification_loss": 0.6282982230186462,
      "epoch": 7.422950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2264,
      "total_loss": 0.6282982230186462
    },
    {
      "classification_loss": 0.6351989507675171,
      "epoch": 7.426229508196721,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2265,
      "total_loss": 0.6351989507675171
    },
    {
      "classification_loss": 0.5781704187393188,
      "epoch": 7.4295081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2266,
      "total_loss": 0.5781704187393188
    },
    {
      "classification_loss": 0.6696004271507263,
      "epoch": 7.432786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2267,
      "total_loss": 0.6696004271507263
    },
    {
      "classification_loss": 0.5621824264526367,
      "epoch": 7.436065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2268,
      "total_loss": 0.5621824264526367
    },
    {
      "classification_loss": 0.5609009861946106,
      "epoch": 7.439344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2269,
      "total_loss": 0.5609009861946106
    },
    {
      "classification_loss": 0.5890737771987915,
      "epoch": 7.442622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2270,
      "total_loss": 0.5890737771987915
    },
    {
      "classification_loss": 0.5718053579330444,
      "epoch": 7.445901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2271,
      "total_loss": 0.5718053579330444
    },
    {
      "classification_loss": 0.49598175287246704,
      "epoch": 7.449180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2272,
      "total_loss": 0.49598175287246704
    },
    {
      "classification_loss": 0.5271434187889099,
      "epoch": 7.452459016393442,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2273,
      "total_loss": 0.5271434187889099
    },
    {
      "classification_loss": 0.4816433787345886,
      "epoch": 7.4557377049180324,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2274,
      "total_loss": 0.4816433787345886
    },
    {
      "classification_loss": 0.660227358341217,
      "epoch": 7.459016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2275,
      "total_loss": 0.660227358341217
    },
    {
      "classification_loss": 0.498272567987442,
      "epoch": 7.462295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2276,
      "total_loss": 0.498272567987442
    },
    {
      "classification_loss": 0.49814528226852417,
      "epoch": 7.465573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2277,
      "total_loss": 0.49814528226852417
    },
    {
      "classification_loss": 0.6038525700569153,
      "epoch": 7.468852459016394,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2278,
      "total_loss": 0.6038525700569153
    },
    {
      "classification_loss": 0.7057598233222961,
      "epoch": 7.472131147540984,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2279,
      "total_loss": 0.7057598233222961
    },
    {
      "classification_loss": 0.5535172820091248,
      "epoch": 7.475409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2280,
      "total_loss": 0.5535172820091248
    },
    {
      "classification_loss": 0.5167403221130371,
      "epoch": 7.478688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2281,
      "total_loss": 0.5167403221130371
    },
    {
      "classification_loss": 0.5380799174308777,
      "epoch": 7.481967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2282,
      "total_loss": 0.5380799174308777
    },
    {
      "classification_loss": 0.4951641261577606,
      "epoch": 7.4852459016393444,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2283,
      "total_loss": 0.4951641261577606
    },
    {
      "classification_loss": 0.48081108927726746,
      "epoch": 7.488524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2284,
      "total_loss": 0.48081108927726746
    },
    {
      "classification_loss": 0.6443471312522888,
      "epoch": 7.491803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2285,
      "total_loss": 0.6443471312522888
    },
    {
      "classification_loss": 0.568490743637085,
      "epoch": 7.495081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2286,
      "total_loss": 0.568490743637085
    },
    {
      "classification_loss": 0.6145883202552795,
      "epoch": 7.498360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2287,
      "total_loss": 0.6145883202552795
    },
    {
      "classification_loss": 0.5329365134239197,
      "epoch": 7.501639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2288,
      "total_loss": 0.5329365134239197
    },
    {
      "classification_loss": 0.5401083827018738,
      "epoch": 7.504918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2289,
      "total_loss": 0.5401083827018738
    },
    {
      "classification_loss": 0.5305216312408447,
      "epoch": 7.508196721311475,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2290,
      "total_loss": 0.5305216312408447
    },
    {
      "classification_loss": 0.5198214054107666,
      "epoch": 7.511475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2291,
      "total_loss": 0.5198214054107666
    },
    {
      "classification_loss": 0.5961601734161377,
      "epoch": 7.5147540983606556,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2292,
      "total_loss": 0.5961601734161377
    },
    {
      "classification_loss": 0.5292035937309265,
      "epoch": 7.518032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2293,
      "total_loss": 0.5292035937309265
    },
    {
      "classification_loss": 0.5479442477226257,
      "epoch": 7.521311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2294,
      "total_loss": 0.5479442477226257
    },
    {
      "classification_loss": 0.6112058758735657,
      "epoch": 7.524590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2295,
      "total_loss": 0.6112058758735657
    },
    {
      "classification_loss": 0.4884969890117645,
      "epoch": 7.527868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2296,
      "total_loss": 0.4884969890117645
    },
    {
      "classification_loss": 0.6506357192993164,
      "epoch": 7.531147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2297,
      "total_loss": 0.6506357192993164
    },
    {
      "classification_loss": 0.4600687623023987,
      "epoch": 7.534426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2298,
      "total_loss": 0.4600687623023987
    },
    {
      "classification_loss": 0.5504701137542725,
      "epoch": 7.537704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2299,
      "total_loss": 0.5504701137542725
    },
    {
      "epoch": 7.540983606557377,
      "grad_norm": 6.208115100860596,
      "learning_rate": 0.0001267,
      "loss": 0.5661,
      "step": 2300
    },
    {
      "classification_loss": 0.6858718991279602,
      "epoch": 7.540983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2300,
      "total_loss": 0.6858718991279602
    },
    {
      "classification_loss": 0.5898471474647522,
      "epoch": 7.5442622950819676,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2301,
      "total_loss": 0.5898471474647522
    },
    {
      "classification_loss": 0.5724513530731201,
      "epoch": 7.547540983606558,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2302,
      "total_loss": 0.5724513530731201
    },
    {
      "classification_loss": 0.48747649788856506,
      "epoch": 7.550819672131148,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2303,
      "total_loss": 0.48747649788856506
    },
    {
      "classification_loss": 0.5737395882606506,
      "epoch": 7.554098360655738,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2304,
      "total_loss": 0.5737395882606506
    },
    {
      "classification_loss": 0.5396084785461426,
      "epoch": 7.557377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2305,
      "total_loss": 0.5396084785461426
    },
    {
      "classification_loss": 0.6353602409362793,
      "epoch": 7.560655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2306,
      "total_loss": 0.6353602409362793
    },
    {
      "classification_loss": 0.5507383942604065,
      "epoch": 7.563934426229508,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2307,
      "total_loss": 0.5507383942604065
    },
    {
      "classification_loss": 0.7028002142906189,
      "epoch": 7.567213114754098,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2308,
      "total_loss": 0.7028002142906189
    },
    {
      "classification_loss": 0.6868891716003418,
      "epoch": 7.5704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2309,
      "total_loss": 0.6868891716003418
    },
    {
      "classification_loss": 0.7001339197158813,
      "epoch": 7.573770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2310,
      "total_loss": 0.7001339197158813
    },
    {
      "classification_loss": 0.6240736842155457,
      "epoch": 7.577049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2311,
      "total_loss": 0.6240736842155457
    },
    {
      "classification_loss": 0.598909854888916,
      "epoch": 7.580327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2312,
      "total_loss": 0.598909854888916
    },
    {
      "classification_loss": 0.5275033712387085,
      "epoch": 7.583606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2313,
      "total_loss": 0.5275033712387085
    },
    {
      "classification_loss": 0.5113428831100464,
      "epoch": 7.586885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2314,
      "total_loss": 0.5113428831100464
    },
    {
      "classification_loss": 0.5316034555435181,
      "epoch": 7.590163934426229,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2315,
      "total_loss": 0.5316034555435181
    },
    {
      "classification_loss": 0.5069210529327393,
      "epoch": 7.593442622950819,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2316,
      "total_loss": 0.5069210529327393
    },
    {
      "classification_loss": 0.5570452809333801,
      "epoch": 7.5967213114754095,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2317,
      "total_loss": 0.5570452809333801
    },
    {
      "classification_loss": 0.6254701018333435,
      "epoch": 7.6,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2318,
      "total_loss": 0.6254701018333435
    },
    {
      "classification_loss": 0.5270934104919434,
      "epoch": 7.60327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2319,
      "total_loss": 0.5270934104919434
    },
    {
      "classification_loss": 0.4844239354133606,
      "epoch": 7.60655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2320,
      "total_loss": 0.4844239354133606
    },
    {
      "classification_loss": 0.6216107606887817,
      "epoch": 7.60983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2321,
      "total_loss": 0.6216107606887817
    },
    {
      "classification_loss": 0.5160002708435059,
      "epoch": 7.613114754098361,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2322,
      "total_loss": 0.5160002708435059
    },
    {
      "classification_loss": 0.584312379360199,
      "epoch": 7.616393442622951,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2323,
      "total_loss": 0.584312379360199
    },
    {
      "classification_loss": 0.5811940431594849,
      "epoch": 7.619672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2324,
      "total_loss": 0.5811940431594849
    },
    {
      "classification_loss": 0.6101306676864624,
      "epoch": 7.622950819672131,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2325,
      "total_loss": 0.6101306676864624
    },
    {
      "classification_loss": 0.5355810523033142,
      "epoch": 7.6262295081967215,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2326,
      "total_loss": 0.5355810523033142
    },
    {
      "classification_loss": 0.6276939511299133,
      "epoch": 7.629508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2327,
      "total_loss": 0.6276939511299133
    },
    {
      "classification_loss": 0.5846216678619385,
      "epoch": 7.632786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2328,
      "total_loss": 0.5846216678619385
    },
    {
      "classification_loss": 0.4954984188079834,
      "epoch": 7.636065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2329,
      "total_loss": 0.4954984188079834
    },
    {
      "classification_loss": 0.5104235410690308,
      "epoch": 7.639344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2330,
      "total_loss": 0.5104235410690308
    },
    {
      "classification_loss": 0.5491158962249756,
      "epoch": 7.642622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2331,
      "total_loss": 0.5491158962249756
    },
    {
      "classification_loss": 0.4625127911567688,
      "epoch": 7.645901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2332,
      "total_loss": 0.4625127911567688
    },
    {
      "classification_loss": 0.5557581782341003,
      "epoch": 7.649180327868852,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2333,
      "total_loss": 0.5557581782341003
    },
    {
      "classification_loss": 0.5384508371353149,
      "epoch": 7.6524590163934425,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2334,
      "total_loss": 0.5384508371353149
    },
    {
      "classification_loss": 0.4919852614402771,
      "epoch": 7.655737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2335,
      "total_loss": 0.4919852614402771
    },
    {
      "classification_loss": 0.6782453060150146,
      "epoch": 7.659016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2336,
      "total_loss": 0.6782453060150146
    },
    {
      "classification_loss": 0.5256725549697876,
      "epoch": 7.662295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2337,
      "total_loss": 0.5256725549697876
    },
    {
      "classification_loss": 0.6136186718940735,
      "epoch": 7.665573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2338,
      "total_loss": 0.6136186718940735
    },
    {
      "classification_loss": 0.521780252456665,
      "epoch": 7.668852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2339,
      "total_loss": 0.521780252456665
    },
    {
      "classification_loss": 0.46130579710006714,
      "epoch": 7.672131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2340,
      "total_loss": 0.46130579710006714
    },
    {
      "classification_loss": 0.6295310854911804,
      "epoch": 7.675409836065574,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2341,
      "total_loss": 0.6295310854911804
    },
    {
      "classification_loss": 0.5995913743972778,
      "epoch": 7.678688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2342,
      "total_loss": 0.5995913743972778
    },
    {
      "classification_loss": 0.5408451557159424,
      "epoch": 7.6819672131147545,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2343,
      "total_loss": 0.5408451557159424
    },
    {
      "classification_loss": 0.7051999568939209,
      "epoch": 7.685245901639345,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2344,
      "total_loss": 0.7051999568939209
    },
    {
      "classification_loss": 0.5327130556106567,
      "epoch": 7.688524590163935,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2345,
      "total_loss": 0.5327130556106567
    },
    {
      "classification_loss": 0.5327492356300354,
      "epoch": 7.691803278688525,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2346,
      "total_loss": 0.5327492356300354
    },
    {
      "classification_loss": 0.5523941516876221,
      "epoch": 7.695081967213115,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2347,
      "total_loss": 0.5523941516876221
    },
    {
      "classification_loss": 0.560577392578125,
      "epoch": 7.698360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2348,
      "total_loss": 0.560577392578125
    },
    {
      "classification_loss": 0.6008943319320679,
      "epoch": 7.701639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2349,
      "total_loss": 0.6008943319320679
    },
    {
      "classification_loss": 0.5893581509590149,
      "epoch": 7.704918032786885,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2350,
      "total_loss": 0.5893581509590149
    },
    {
      "classification_loss": 0.5166263580322266,
      "epoch": 7.7081967213114755,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2351,
      "total_loss": 0.5166263580322266
    },
    {
      "classification_loss": 0.6325685381889343,
      "epoch": 7.711475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2352,
      "total_loss": 0.6325685381889343
    },
    {
      "classification_loss": 0.6248944997787476,
      "epoch": 7.714754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2353,
      "total_loss": 0.6248944997787476
    },
    {
      "classification_loss": 0.6213352680206299,
      "epoch": 7.718032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2354,
      "total_loss": 0.6213352680206299
    },
    {
      "classification_loss": 0.6722404360771179,
      "epoch": 7.721311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2355,
      "total_loss": 0.6722404360771179
    },
    {
      "classification_loss": 0.45331719517707825,
      "epoch": 7.724590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2356,
      "total_loss": 0.45331719517707825
    },
    {
      "classification_loss": 0.6431810855865479,
      "epoch": 7.727868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2357,
      "total_loss": 0.6431810855865479
    },
    {
      "classification_loss": 0.5451889634132385,
      "epoch": 7.731147540983606,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2358,
      "total_loss": 0.5451889634132385
    },
    {
      "classification_loss": 0.5269446969032288,
      "epoch": 7.7344262295081965,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2359,
      "total_loss": 0.5269446969032288
    },
    {
      "classification_loss": 0.5178192853927612,
      "epoch": 7.737704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2360,
      "total_loss": 0.5178192853927612
    },
    {
      "classification_loss": 0.6579251289367676,
      "epoch": 7.740983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2361,
      "total_loss": 0.6579251289367676
    },
    {
      "classification_loss": 0.5794812440872192,
      "epoch": 7.744262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2362,
      "total_loss": 0.5794812440872192
    },
    {
      "classification_loss": 0.6233168840408325,
      "epoch": 7.747540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2363,
      "total_loss": 0.6233168840408325
    },
    {
      "classification_loss": 0.6624091267585754,
      "epoch": 7.750819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2364,
      "total_loss": 0.6624091267585754
    },
    {
      "classification_loss": 0.5549938678741455,
      "epoch": 7.754098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2365,
      "total_loss": 0.5549938678741455
    },
    {
      "classification_loss": 0.5458521842956543,
      "epoch": 7.757377049180328,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2366,
      "total_loss": 0.5458521842956543
    },
    {
      "classification_loss": 0.6124333143234253,
      "epoch": 7.760655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2367,
      "total_loss": 0.6124333143234253
    },
    {
      "classification_loss": 0.5890825986862183,
      "epoch": 7.7639344262295085,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2368,
      "total_loss": 0.5890825986862183
    },
    {
      "classification_loss": 0.49716317653656006,
      "epoch": 7.767213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2369,
      "total_loss": 0.49716317653656006
    },
    {
      "classification_loss": 0.5425688624382019,
      "epoch": 7.770491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2370,
      "total_loss": 0.5425688624382019
    },
    {
      "classification_loss": 0.5102477073669434,
      "epoch": 7.773770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2371,
      "total_loss": 0.5102477073669434
    },
    {
      "classification_loss": 0.5512252449989319,
      "epoch": 7.777049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2372,
      "total_loss": 0.5512252449989319
    },
    {
      "classification_loss": 0.6157832741737366,
      "epoch": 7.780327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2373,
      "total_loss": 0.6157832741737366
    },
    {
      "classification_loss": 0.5494519472122192,
      "epoch": 7.783606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2374,
      "total_loss": 0.5494519472122192
    },
    {
      "classification_loss": 0.47852298617362976,
      "epoch": 7.786885245901639,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2375,
      "total_loss": 0.47852298617362976
    },
    {
      "classification_loss": 0.6101050972938538,
      "epoch": 7.7901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2376,
      "total_loss": 0.6101050972938538
    },
    {
      "classification_loss": 0.515971839427948,
      "epoch": 7.79344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2377,
      "total_loss": 0.515971839427948
    },
    {
      "classification_loss": 0.5914016962051392,
      "epoch": 7.79672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2378,
      "total_loss": 0.5914016962051392
    },
    {
      "classification_loss": 0.620405912399292,
      "epoch": 7.8,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2379,
      "total_loss": 0.620405912399292
    },
    {
      "classification_loss": 0.6113471388816833,
      "epoch": 7.80327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2380,
      "total_loss": 0.6113471388816833
    },
    {
      "classification_loss": 0.6214811205863953,
      "epoch": 7.80655737704918,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2381,
      "total_loss": 0.6214811205863953
    },
    {
      "classification_loss": 0.5689966678619385,
      "epoch": 7.80983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2382,
      "total_loss": 0.5689966678619385
    },
    {
      "classification_loss": 0.6812896728515625,
      "epoch": 7.81311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2383,
      "total_loss": 0.6812896728515625
    },
    {
      "classification_loss": 0.501908540725708,
      "epoch": 7.81639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2384,
      "total_loss": 0.501908540725708
    },
    {
      "classification_loss": 0.6184002757072449,
      "epoch": 7.8196721311475414,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2385,
      "total_loss": 0.6184002757072449
    },
    {
      "classification_loss": 0.544994056224823,
      "epoch": 7.822950819672132,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2386,
      "total_loss": 0.544994056224823
    },
    {
      "classification_loss": 0.5120457410812378,
      "epoch": 7.826229508196722,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2387,
      "total_loss": 0.5120457410812378
    },
    {
      "classification_loss": 0.5580939054489136,
      "epoch": 7.829508196721312,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2388,
      "total_loss": 0.5580939054489136
    },
    {
      "classification_loss": 0.5847702622413635,
      "epoch": 7.832786885245902,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2389,
      "total_loss": 0.5847702622413635
    },
    {
      "classification_loss": 0.5289652347564697,
      "epoch": 7.836065573770492,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2390,
      "total_loss": 0.5289652347564697
    },
    {
      "classification_loss": 0.5734578967094421,
      "epoch": 7.839344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2391,
      "total_loss": 0.5734578967094421
    },
    {
      "classification_loss": 0.5484541058540344,
      "epoch": 7.842622950819672,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2392,
      "total_loss": 0.5484541058540344
    },
    {
      "classification_loss": 0.6352186799049377,
      "epoch": 7.845901639344262,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2393,
      "total_loss": 0.6352186799049377
    },
    {
      "classification_loss": 0.6385919451713562,
      "epoch": 7.849180327868853,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2394,
      "total_loss": 0.6385919451713562
    },
    {
      "classification_loss": 0.5308370590209961,
      "epoch": 7.852459016393443,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2395,
      "total_loss": 0.5308370590209961
    },
    {
      "classification_loss": 0.5314186215400696,
      "epoch": 7.855737704918033,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2396,
      "total_loss": 0.5314186215400696
    },
    {
      "classification_loss": 0.5933178067207336,
      "epoch": 7.859016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2397,
      "total_loss": 0.5933178067207336
    },
    {
      "classification_loss": 0.5505536198616028,
      "epoch": 7.862295081967213,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2398,
      "total_loss": 0.5505536198616028
    },
    {
      "classification_loss": 0.5348497629165649,
      "epoch": 7.865573770491803,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2399,
      "total_loss": 0.5348497629165649
    },
    {
      "epoch": 7.868852459016393,
      "grad_norm": 8.305474281311035,
      "learning_rate": 0.00012336666666666667,
      "loss": 0.5726,
      "step": 2400
    },
    {
      "classification_loss": 0.5232796669006348,
      "epoch": 7.868852459016393,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2400,
      "total_loss": 0.5232796669006348
    },
    {
      "classification_loss": 0.546960711479187,
      "epoch": 7.872131147540983,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2401,
      "total_loss": 0.546960711479187
    },
    {
      "classification_loss": 0.6505337953567505,
      "epoch": 7.8754098360655735,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2402,
      "total_loss": 0.6505337953567505
    },
    {
      "classification_loss": 0.5075053572654724,
      "epoch": 7.878688524590164,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2403,
      "total_loss": 0.5075053572654724
    },
    {
      "classification_loss": 0.572057843208313,
      "epoch": 7.881967213114754,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2404,
      "total_loss": 0.572057843208313
    },
    {
      "classification_loss": 0.6156010031700134,
      "epoch": 7.885245901639344,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2405,
      "total_loss": 0.6156010031700134
    },
    {
      "classification_loss": 0.6448472738265991,
      "epoch": 7.888524590163934,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2406,
      "total_loss": 0.6448472738265991
    },
    {
      "classification_loss": 0.5809255838394165,
      "epoch": 7.891803278688524,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2407,
      "total_loss": 0.5809255838394165
    },
    {
      "classification_loss": 0.6329923868179321,
      "epoch": 7.895081967213114,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2408,
      "total_loss": 0.6329923868179321
    },
    {
      "classification_loss": 0.5024585723876953,
      "epoch": 7.898360655737705,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2409,
      "total_loss": 0.5024585723876953
    },
    {
      "classification_loss": 0.5823358297348022,
      "epoch": 7.901639344262295,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2410,
      "total_loss": 0.5823358297348022
    },
    {
      "classification_loss": 0.5991610288619995,
      "epoch": 7.9049180327868855,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2411,
      "total_loss": 0.5991610288619995
    },
    {
      "classification_loss": 0.6443155407905579,
      "epoch": 7.908196721311476,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2412,
      "total_loss": 0.6443155407905579
    },
    {
      "classification_loss": 0.5438965559005737,
      "epoch": 7.911475409836066,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2413,
      "total_loss": 0.5438965559005737
    },
    {
      "classification_loss": 0.4911649227142334,
      "epoch": 7.914754098360656,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2414,
      "total_loss": 0.4911649227142334
    },
    {
      "classification_loss": 0.4709755480289459,
      "epoch": 7.918032786885246,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2415,
      "total_loss": 0.4709755480289459
    },
    {
      "classification_loss": 0.6250196099281311,
      "epoch": 7.921311475409836,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2416,
      "total_loss": 0.6250196099281311
    },
    {
      "classification_loss": 0.5630298852920532,
      "epoch": 7.924590163934426,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2417,
      "total_loss": 0.5630298852920532
    },
    {
      "classification_loss": 0.6950063109397888,
      "epoch": 7.927868852459016,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2418,
      "total_loss": 0.6950063109397888
    },
    {
      "classification_loss": 0.5232666730880737,
      "epoch": 7.9311475409836065,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2419,
      "total_loss": 0.5232666730880737
    },
    {
      "classification_loss": 0.5885245203971863,
      "epoch": 7.934426229508197,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2420,
      "total_loss": 0.5885245203971863
    },
    {
      "classification_loss": 0.5985798835754395,
      "epoch": 7.937704918032787,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2421,
      "total_loss": 0.5985798835754395
    },
    {
      "classification_loss": 0.5374253392219543,
      "epoch": 7.940983606557377,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2422,
      "total_loss": 0.5374253392219543
    },
    {
      "classification_loss": 0.4955742657184601,
      "epoch": 7.944262295081967,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2423,
      "total_loss": 0.4955742657184601
    },
    {
      "classification_loss": 0.5285370349884033,
      "epoch": 7.947540983606557,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2424,
      "total_loss": 0.5285370349884033
    },
    {
      "classification_loss": 0.5201652646064758,
      "epoch": 7.950819672131147,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2425,
      "total_loss": 0.5201652646064758
    },
    {
      "classification_loss": 0.5915061235427856,
      "epoch": 7.954098360655737,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2426,
      "total_loss": 0.5915061235427856
    },
    {
      "classification_loss": 0.5560798048973083,
      "epoch": 7.9573770491803275,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2427,
      "total_loss": 0.5560798048973083
    },
    {
      "classification_loss": 0.6253147125244141,
      "epoch": 7.9606557377049185,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2428,
      "total_loss": 0.6253147125244141
    },
    {
      "classification_loss": 0.5793600082397461,
      "epoch": 7.963934426229509,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2429,
      "total_loss": 0.5793600082397461
    },
    {
      "classification_loss": 0.5487241744995117,
      "epoch": 7.967213114754099,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2430,
      "total_loss": 0.5487241744995117
    },
    {
      "classification_loss": 0.5573931932449341,
      "epoch": 7.970491803278689,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2431,
      "total_loss": 0.5573931932449341
    },
    {
      "classification_loss": 0.5380932688713074,
      "epoch": 7.973770491803279,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2432,
      "total_loss": 0.5380932688713074
    },
    {
      "classification_loss": 0.5733785629272461,
      "epoch": 7.977049180327869,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2433,
      "total_loss": 0.5733785629272461
    },
    {
      "classification_loss": 0.6513859629631042,
      "epoch": 7.980327868852459,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2434,
      "total_loss": 0.6513859629631042
    },
    {
      "classification_loss": 0.5484557747840881,
      "epoch": 7.983606557377049,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2435,
      "total_loss": 0.5484557747840881
    },
    {
      "classification_loss": 0.6530421376228333,
      "epoch": 7.9868852459016395,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2436,
      "total_loss": 0.6530421376228333
    },
    {
      "classification_loss": 0.5303828120231628,
      "epoch": 7.99016393442623,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2437,
      "total_loss": 0.5303828120231628
    },
    {
      "classification_loss": 0.5408068299293518,
      "epoch": 7.99344262295082,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2438,
      "total_loss": 0.5408068299293518
    },
    {
      "classification_loss": 0.6691458225250244,
      "epoch": 7.99672131147541,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2439,
      "total_loss": 0.6691458225250244
    },
    {
      "classification_loss": 0.9733321070671082,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 0.9733321070671082
    },
    {
      "classification_loss": 1.001143217086792,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 1.001143217086792
    },
    {
      "classification_loss": 0.9153680801391602,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 0.9153680801391602
    },
    {
      "classification_loss": 1.0090981721878052,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 1.0090981721878052
    },
    {
      "classification_loss": 1.0635582208633423,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 1.0635582208633423
    },
    {
      "classification_loss": 0.9462628364562988,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 0.9462628364562988
    },
    {
      "classification_loss": 1.0130788087844849,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 1.0130788087844849
    },
    {
      "classification_loss": 1.073581337928772,
      "epoch": 8.0,
      "mode": "LoRA",
      "orthogonal_loss": 0.0,
      "step": 2440,
      "total_loss": 1.073581337928772
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.384,
      "eval_f1": 0.022222222222222223,
      "eval_loss": 0.9976481795310974,
      "eval_precision": 0.5,
      "eval_recall": 0.011363636363636364,
      "eval_runtime": 6.0549,
      "eval_samples_per_second": 165.155,
      "eval_steps_per_second": 1.321,
      "step": 2440
    }
  ],
  "logging_steps": 100,
  "max_steps": 6100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6262912989020160.0,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
