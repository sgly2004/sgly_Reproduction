"""
CLoRAæ¨¡å‹è¯„ä¼°è„šæœ¬ - è¯„ä¼°è®­ç»ƒå¥½çš„CLoRAæ¨¡å‹æ€§èƒ½
é›†æˆäº†æ—¥å¿—ç³»ç»Ÿ
"""

import torch
import numpy as np
from torch.utils.data import DataLoader
from transformers import DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
from peft import PeftModel
import os
import sys
import argparse
from tqdm import tqdm
import random
from typing import Dict, List, Tuple, Optional
import warnings
import json
from datetime import datetime

# å¯¼å…¥æ—¥å¿—ç³»ç»Ÿ
from logger_utils import setup_logger, redirect_print_to_logger, restore_print

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from data_processor import get_eval_datasets, load_tokenizer
from model_setup import setup_lora_model, load_base_model


def load_trained_model(checkpoint_path, base_model=None, logger=None):
    """
    åŠ è½½è®­ç»ƒå¥½çš„CLoRAæ¨¡å‹
    
    Args:
        checkpoint_path: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„
        base_model: åŸºç¡€æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        logger: æ—¥å¿—å™¨
    
    Returns:
        tuple: (åŠ è½½çš„æ¨¡å‹, åˆ†è¯å™¨)
    """
    
    if logger:
        logger.info(f"æ­£åœ¨ä» {checkpoint_path} åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    else:
        print(f"æ­£åœ¨ä» {checkpoint_path} åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    
    # å¦‚æœæ²¡æœ‰æä¾›åŸºç¡€æ¨¡å‹ï¼Œåˆ™åŠ è½½ä¸€ä¸ªæ–°çš„
    if base_model is None:
        if logger:
            logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        else:
            print("åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = load_base_model("gpt2", num_labels=2)
    
    # åŠ è½½PEFTæ¨¡å‹
    model = PeftModel.from_pretrained(base_model, checkpoint_path)
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = load_tokenizer()
    
    if logger:
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ!")
    else:
        print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    return model, tokenizer


def evaluate_model(model, tokenizer, eval_dataset, batch_size=16, device=None, logger=None):
    """
    è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        eval_dataset: è¯„ä¼°æ•°æ®é›†
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¾å¤‡
        logger: æ—¥å¿—å™¨
    
    Returns:
        dict: è¯„ä¼°ç»“æœå­—å…¸
    """
    
    # è®¾ç½®è®¾å¤‡
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if logger:
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    model.to(device)
    model.eval()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    data_collator = DataCollatorWithPadding(
        tokenizer=tokenizer,
        padding=True,
        return_tensors="pt"
    )
    
    dataloader = DataLoader(
        eval_dataset,
        batch_size=batch_size,
        collate_fn=data_collator,
        shuffle=False
    )
    
    # å­˜å‚¨é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾
    all_predictions = []
    all_labels = []
    total_loss = 0.0
    num_batches = 0
    
    if logger:
        logger.info("å¼€å§‹è¯„ä¼°...")
        logger.info(f"è¯„ä¼°æ•°æ®é›†å¤§å°: {len(eval_dataset)}")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    else:
        print("å¼€å§‹è¯„ä¼°...")
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦"):
            # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(device) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(**batch)
            
            # è·å–é¢„æµ‹ç»“æœ
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1)
            
            # æ”¶é›†ç»“æœ
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(batch['labels'].cpu().numpy())
            
            # ç´¯è®¡æŸå¤±
            if hasattr(outputs, 'loss'):
                total_loss += outputs.loss.item()
            num_batches += 1
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_predictions, average='binary'
    )
    
    # è®¡ç®—å¹³å‡æŸå¤±
    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    class_report = classification_report(
        all_labels, all_predictions,
        target_names=['False', 'True'],
        output_dict=True
    )
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'avg_loss': avg_loss,
        'classification_report': class_report,
        'predictions': all_predictions,
        'labels': all_labels,
        'num_samples': len(all_labels)
    }


def print_evaluation_results(results, logger=None):
    """
    æ‰“å°è¯„ä¼°ç»“æœ
    
    Args:
        results: è¯„ä¼°ç»“æœå­—å…¸
        logger: æ—¥å¿—å™¨
    """
    
    if logger:
        logger.log_section("è¯„ä¼°ç»“æœ")
    else:
        print("\n" + "=" * 50)
        print("è¯„ä¼°ç»“æœ")
        print("=" * 50)
    
    # ä¸»è¦æŒ‡æ ‡
    main_metrics = {
        "å‡†ç¡®ç‡ (Accuracy)": f"{results['accuracy']:.4f}",
        "ç²¾ç¡®ç‡ (Precision)": f"{results['precision']:.4f}",
        "å¬å›ç‡ (Recall)": f"{results['recall']:.4f}",
        "F1åˆ†æ•°": f"{results['f1']:.4f}",
        "å¹³å‡æŸå¤±": f"{results['avg_loss']:.4f}"
    }
    
    if logger:
        logger.log_metrics(main_metrics, "ä¸»è¦æŒ‡æ ‡")
    else:
        for metric, value in main_metrics.items():
            print(f"{metric}: {value}")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    class_report = results['classification_report']
    
    if logger:
        logger.log_section("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š", "-")
        
        # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        for class_name in ['False', 'True']:
            if class_name in class_report:
                metrics = class_report[class_name]
                class_metrics = {
                    "ç²¾ç¡®ç‡": f"{metrics['precision']:.4f}",
                    "å¬å›ç‡": f"{metrics['recall']:.4f}",
                    "F1åˆ†æ•°": f"{metrics['f1-score']:.4f}",
                    "æ”¯æŒæ•°": str(metrics['support'])
                }
                logger.log_metrics(class_metrics, f"{class_name}ç±»æŒ‡æ ‡")
    else:
        print("\n" + "=" * 30)
        print("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š")
        print("=" * 30)
        
        # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        for class_name in ['False', 'True']:
            if class_name in class_report:
                metrics = class_report[class_name]
                print(f"\n{class_name}ç±»:")
                print(f"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
                print(f"  å¬å›ç‡: {metrics['recall']:.4f}")
                print(f"  F1åˆ†æ•°: {metrics['f1-score']:.4f}")
                print(f"  æ”¯æŒæ•°: {metrics['support']}")
    
    # å®å¹³å‡å’ŒåŠ æƒå¹³å‡
    if logger:
        if 'macro avg' in class_report:
            macro_avg = class_report['macro avg']
            macro_metrics = {
                "ç²¾ç¡®ç‡": f"{macro_avg['precision']:.4f}",
                "å¬å›ç‡": f"{macro_avg['recall']:.4f}",
                "F1åˆ†æ•°": f"{macro_avg['f1-score']:.4f}"
            }
            logger.log_metrics(macro_metrics, "å®å¹³å‡")
        
        if 'weighted avg' in class_report:
            weighted_avg = class_report['weighted avg']
            weighted_metrics = {
                "ç²¾ç¡®ç‡": f"{weighted_avg['precision']:.4f}",
                "å¬å›ç‡": f"{weighted_avg['recall']:.4f}",
                "F1åˆ†æ•°": f"{weighted_avg['f1-score']:.4f}"
            }
            logger.log_metrics(weighted_metrics, "åŠ æƒå¹³å‡")
    else:
        if 'macro avg' in class_report:
            macro_avg = class_report['macro avg']
            print(f"\nå®å¹³å‡:")
            print(f"  ç²¾ç¡®ç‡: {macro_avg['precision']:.4f}")
            print(f"  å¬å›ç‡: {macro_avg['recall']:.4f}")
            print(f"  F1åˆ†æ•°: {macro_avg['f1-score']:.4f}")
        
        if 'weighted avg' in class_report:
            weighted_avg = class_report['weighted avg']
            print(f"\nåŠ æƒå¹³å‡:")
            print(f"  ç²¾ç¡®ç‡: {weighted_avg['precision']:.4f}")
            print(f"  å¬å›ç‡: {weighted_avg['recall']:.4f}")
            print(f"  F1åˆ†æ•°: {weighted_avg['f1-score']:.4f}")


def compute_model_updating_capacity(model: torch.nn.Module, logger=None) -> float:
    """
    è®¡ç®—æ¨¡å‹æ›´æ–°å®¹é‡
    
    æ ¹æ®è®ºæ–‡å…¬å¼ï¼šå¯¹æ¯ä¸ªLoRAå±‚è®¡ç®— Î”W = B @ Aï¼Œç„¶åè®¡ç®— ||Î”W||_2
    è¿”å›æ‰€æœ‰LoRAå±‚çš„å¹³å‡æ›´æ–°å®¹é‡
    
    Args:
        model: PEFTæ¨¡å‹æˆ–åŸºçº¿æ¨¡å‹
        logger: æ—¥å¿—å™¨
    
    Returns:
        float: å¹³å‡æ¨¡å‹æ›´æ–°å®¹é‡ï¼ˆåŸºçº¿æ¨¡å‹è¿”å›0.0ï¼‰
    """
    
    updating_capacities = []
    
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºåŸºçº¿æ¨¡å‹ï¼ˆæ²¡æœ‰LoRAå±‚ï¼‰
        has_lora = False
        
        # éå†æ‰€æœ‰æ¨¡å—å¯»æ‰¾LoRAå±‚
        for name, module in model.named_modules():
            # æŸ¥æ‰¾LoRAçš„Aå’ŒBçŸ©é˜µ
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                has_lora = True
                for adapter_name in module.lora_A.keys():
                    # è·å–Aå’ŒBçŸ©é˜µ
                    lora_a = module.lora_A[adapter_name].weight  # (r, input_dim)
                    lora_b = module.lora_B[adapter_name].weight  # (output_dim, r)
                    
                    # è®¡ç®— Î”W = B @ A
                    delta_w = torch.mm(lora_b, lora_a)  # (output_dim, input_dim)
                    
                    # è®¡ç®—L2èŒƒæ•°ï¼ˆæœ€å¤§å¥‡å¼‚å€¼ï¼‰
                    l2_norm = torch.norm(delta_w, p=2).item()
                    updating_capacities.append(l2_norm)
        
        if not has_lora:
            msg = "åŸºçº¿æ¨¡å‹ï¼ˆæ— LoRAå±‚ï¼‰ï¼Œæ›´æ–°å®¹é‡ä¸º0"
            if logger:
                logger.info(msg)
            else:
                print(msg)
            return 0.0
                    
        if not updating_capacities:
            msg = "è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•LoRAå±‚"
            if logger:
                logger.warning(msg)
            else:
                print(msg)
            return 0.0
            
        # è¿”å›å¹³å‡æ›´æ–°å®¹é‡
        avg_capacity = np.mean(updating_capacities)
        
        capacity_info = {
            "LoRAå±‚æ•°é‡": len(updating_capacities),
            "æ›´æ–°å®¹é‡èŒƒå›´": f"{min(updating_capacities):.6f} - {max(updating_capacities):.6f}",
            "å¹³å‡æ›´æ–°å®¹é‡": f"{avg_capacity:.6f}"
        }
        
        if logger:
            logger.log_metrics(capacity_info, "æ¨¡å‹æ›´æ–°å®¹é‡")
        else:
            print(f"æ‰¾åˆ° {len(updating_capacities)} ä¸ªLoRAå±‚")
            print(f"æ›´æ–°å®¹é‡èŒƒå›´: {min(updating_capacities):.6f} - {max(updating_capacities):.6f}")
            print(f"å¹³å‡æ›´æ–°å®¹é‡: {avg_capacity:.6f}")
        
        return avg_capacity
        
    except Exception as e:
        error_msg = f"è®¡ç®—æ¨¡å‹æ›´æ–°å®¹é‡æ—¶å‡ºé”™: {e}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        return 0.0


def compute_relative_output_change(model: torch.nn.Module, 
                                 sample_inputs: List[torch.Tensor],
                                 device: Optional[torch.device] = None,
                                 logger=None) -> float:
    """
    è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–
    
    å®ç°è®ºæ–‡ä¸­çš„å…¬å¼ï¼šF_Î”(Î”W, x) = ||Î”W @ x|| / ||x||
    
    Args:
        model: PEFTæ¨¡å‹æˆ–åŸºçº¿æ¨¡å‹
        sample_inputs: æ ·æœ¬è¾“å…¥å¼ é‡åˆ—è¡¨
        device: è®¡ç®—è®¾å¤‡
        logger: æ—¥å¿—å™¨
    
    Returns:
        float: å¹³å‡ç›¸å¯¹è¾“å‡ºå˜åŒ–ï¼ˆåŸºçº¿æ¨¡å‹è¿”å›0.0ï¼‰
    """
    
    if device is None:
        device = next(model.parameters()).device
    
    relative_changes = []
    
    try:
        model.eval()
        
        # æ”¶é›†æ‰€æœ‰LoRAå±‚çš„Î”WçŸ©é˜µ
        delta_w_matrices = []
        has_lora = False
        
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                has_lora = True
                for adapter_name in module.lora_A.keys():
                    lora_a = module.lora_A[adapter_name].weight
                    lora_b = module.lora_B[adapter_name].weight
                    delta_w = torch.mm(lora_b, lora_a)
                    delta_w_matrices.append(delta_w)
        
        if not has_lora:
            msg = "åŸºçº¿æ¨¡å‹ï¼ˆæ— LoRAå±‚ï¼‰ï¼Œç›¸å¯¹è¾“å‡ºå˜åŒ–ä¸º0"
            if logger:
                logger.info(msg)
            else:
                print(msg)
            return 0.0
        
        if not delta_w_matrices:
            msg = "è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•LoRAå±‚ç”¨äºè®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–"
            if logger:
                logger.warning(msg)
            else:
                print(msg)
            return 0.0
        
        # å¯¹æ¯ä¸ªæ ·æœ¬è¾“å…¥è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–
        with torch.no_grad():
            for input_tensor in sample_inputs[:100]:  # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥æé«˜æ•ˆç‡
                input_tensor = input_tensor.to(device)
                
                # ç¡®ä¿è¾“å…¥æ˜¯2Då¼ é‡
                if input_tensor.dim() == 1:
                    input_tensor = input_tensor.unsqueeze(0)
                
                input_norm = torch.norm(input_tensor, p=2)
                
                if input_norm > 1e-8:  # é¿å…é™¤é›¶
                    layer_changes = []
                    
                    for delta_w in delta_w_matrices:
                        # è°ƒæ•´ç»´åº¦ä»¥ç¡®ä¿çŸ©é˜µä¹˜æ³•å¯è¡Œ
                        if delta_w.shape[1] == input_tensor.shape[-1]:
                            output_change = torch.mm(delta_w, input_tensor.T)
                            output_change_norm = torch.norm(output_change, p=2)
                            relative_change = (output_change_norm / input_norm).item()
                            layer_changes.append(relative_change)
                    
                    if layer_changes:
                        # å¯¹æ‰€æœ‰å±‚çš„ç›¸å¯¹å˜åŒ–å–å¹³å‡
                        avg_change = np.mean(layer_changes)
                        relative_changes.append(avg_change)
        
        if not relative_changes:
            msg = "è­¦å‘Š: æ— æ³•è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–"
            if logger:
                logger.warning(msg)
            else:
                print(msg)
            return 0.0
        
        avg_relative_change = np.mean(relative_changes)
        
        change_info = {
            "æ ·æœ¬æ•°é‡": len(relative_changes),
            "å¹³å‡ç›¸å¯¹è¾“å‡ºå˜åŒ–": f"{avg_relative_change:.6f}"
        }
        
        if logger:
            logger.log_metrics(change_info, "ç›¸å¯¹è¾“å‡ºå˜åŒ–")
        else:
            print(f"è®¡ç®—äº† {len(relative_changes)} ä¸ªæ ·æœ¬çš„ç›¸å¯¹è¾“å‡ºå˜åŒ–")
            print(f"å¹³å‡ç›¸å¯¹è¾“å‡ºå˜åŒ–: {avg_relative_change:.6f}")
        
        return avg_relative_change
        
    except Exception as e:
        error_msg = f"è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–æ—¶å‡ºé”™: {e}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        return 0.0


def extract_sample_inputs(eval_dataset, tokenizer, num_samples: int = 50) -> List[torch.Tensor]:
    """
    ä»è¯„ä¼°æ•°æ®é›†ä¸­æå–æ ·æœ¬è¾“å…¥
    
    Args:
        eval_dataset: è¯„ä¼°æ•°æ®é›†
        tokenizer: åˆ†è¯å™¨
        num_samples: æå–çš„æ ·æœ¬æ•°é‡
    
    Returns:
        List[torch.Tensor]: æ ·æœ¬è¾“å…¥å¼ é‡åˆ—è¡¨
    """
    
    sample_inputs = []
    
    try:
        # éšæœºé€‰æ‹©æ ·æœ¬
        indices = random.sample(range(len(eval_dataset)), min(num_samples, len(eval_dataset)))
        
        for idx in indices:
            sample = eval_dataset[idx]
            input_ids = torch.tensor(sample['input_ids'], dtype=torch.long)
            sample_inputs.append(input_ids)
            
    except Exception as e:
        print(f"æå–æ ·æœ¬è¾“å…¥æ—¶å‡ºé”™: {e}")
        
    return sample_inputs


def calculate_task_accuracies(all_results: Dict[str, Dict]) -> Tuple[float, float, Dict[str, float]]:
    """
    è®¡ç®—ä»»åŠ¡å‡†ç¡®ç‡
    
    Args:
        all_results: æ‰€æœ‰æ•°æ®é›†çš„è¯„ä¼°ç»“æœ
    
    Returns:
        tuple: (åŸŸå†…å¹³å‡å‡†ç¡®ç‡, åŸŸå¤–å¹³å‡å‡†ç¡®ç‡, è¯¦ç»†å‡†ç¡®ç‡å­—å…¸)
    """
    
    # å®šä¹‰åŸŸå†…å’ŒåŸŸå¤–æ•°æ®é›†
    # åŸŸå†…ï¼šCommonsenseQA (è®­ç»ƒæ•°æ®é›†åŒç±»å‹)
    in_domain_datasets = ['commonsense_qa']
    # åŸŸå¤–ï¼šå…¶ä»–æ‰€æœ‰æ•°æ®é›†
    out_domain_datasets = ['boolq', 'piqa', 'winogrande', 'hellaswag']
    
    in_domain_accuracies = []
    out_domain_accuracies = []
    detailed_accuracies = {}
    
    for dataset_name, results in all_results.items():
        accuracy = results['accuracy']
        detailed_accuracies[dataset_name] = accuracy
        
        if dataset_name in in_domain_datasets:
            in_domain_accuracies.append(accuracy)
        elif dataset_name in out_domain_datasets:
            out_domain_accuracies.append(accuracy)
        # å…¶ä»–æœªå®šä¹‰çš„æ•°æ®é›†é»˜è®¤ä¸ºåŸŸå¤–
        else:
            out_domain_accuracies.append(accuracy)
    
    # è®¡ç®—å¹³å‡å€¼
    avg_in_domain = np.mean(in_domain_accuracies) if in_domain_accuracies else 0.0
    avg_out_domain = np.mean(out_domain_accuracies) if out_domain_accuracies else 0.0
    
    return avg_in_domain, avg_out_domain, detailed_accuracies


def generate_evaluation_report(model: torch.nn.Module, 
                             tokenizer, 
                             eval_datasets: Dict[str, any],
                             batch_size: int = 16,
                             logger=None) -> Dict[str, any]:
    """
    ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š
    
    Args:
        model: CLoRAæ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        eval_datasets: è¯„ä¼°æ•°æ®é›†å­—å…¸
        batch_size: è¯„ä¼°æ‰¹æ¬¡å¤§å°
        logger: æ—¥å¿—å™¨
    
    Returns:
        dict: ç»¼åˆè¯„ä¼°æŠ¥å‘Š
    """
    
    if logger:
        logger.log_section("ç»¼åˆè¯„ä¼°æŠ¥å‘Šç”Ÿæˆ")
    else:
        print("\n" + "=" * 40)
        print("ç»¼åˆè¯„ä¼°æŠ¥å‘Šç”Ÿæˆ")
        print("=" * 40)
    
    # è¯„ä¼°æ‰€æœ‰æ•°æ®é›†
    all_results = {}
    for dataset_name, dataset in eval_datasets.items():
        if logger:
            logger.info(f"è¯„ä¼°æ•°æ®é›†: {dataset_name}")
        else:
            print(f"\nè¯„ä¼°æ•°æ®é›†: {dataset_name}")
        
        results = evaluate_model(model, tokenizer, dataset, batch_size=batch_size, logger=logger)
        all_results[dataset_name] = results
        
        if logger:
            metrics = {
                "å‡†ç¡®ç‡": f"{results['accuracy']:.4f}",
                "F1åˆ†æ•°": f"{results['f1']:.4f}",
                "æ ·æœ¬æ•°": results['num_samples']
            }
            logger.log_metrics(metrics, f"{dataset_name} ç»“æœ")
    
    # è®¡ç®—ä»»åŠ¡å‡†ç¡®ç‡
    avg_in_domain, avg_out_domain, detailed_accuracies = calculate_task_accuracies(all_results)
    
    # è®¡ç®—æ¨¡å‹æ›´æ–°å®¹é‡
    if logger:
        logger.info("è®¡ç®—æ¨¡å‹æ›´æ–°å®¹é‡...")
    model_updating_capacity = compute_model_updating_capacity(model, logger=logger)
    
    # è·å–æ ·æœ¬è¾“å…¥ç”¨äºç›¸å¯¹è¾“å‡ºå˜åŒ–è®¡ç®—
    sample_inputs = []
    for dataset_name, dataset in eval_datasets.items():
        inputs = extract_sample_inputs(dataset, tokenizer, num_samples=10)
        sample_inputs.extend(inputs)
        if len(sample_inputs) >= 50:  # é™åˆ¶æ€»æ•°
            break
    
    # è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–
    if logger:
        logger.info("è®¡ç®—ç›¸å¯¹è¾“å‡ºå˜åŒ–...")
    relative_output_change = compute_relative_output_change(model, sample_inputs, logger=logger)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluation_report = {
        'task_accuracies': detailed_accuracies,
        'avg_in_domain_accuracy': avg_in_domain,
        'avg_out_domain_accuracy': avg_out_domain,
        'model_updating_capacity': model_updating_capacity,
        'relative_output_change': relative_output_change,
        'detailed_results': all_results
    }
    
    # è®°å½•ç»¼åˆç»“æœ
    if logger:
        summary_metrics = {
            "åŸŸå†…å¹³å‡å‡†ç¡®ç‡": f"{avg_in_domain:.4f}",
            "åŸŸå¤–å¹³å‡å‡†ç¡®ç‡": f"{avg_out_domain:.4f}",
            "æ¨¡å‹æ›´æ–°å®¹é‡": f"{model_updating_capacity:.6f}",
            "ç›¸å¯¹è¾“å‡ºå˜åŒ–": f"{relative_output_change:.6f}"
        }
        logger.log_results(summary_metrics, "ç»¼åˆè¯„ä¼°ç»“æœ")
    
    return evaluation_report


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="CLoRAæ¨¡å‹è¯„ä¼°è„šæœ¬")
    
    parser.add_argument(
        "--model-path",
        type=str,
        default=None,
        help="æŒ‡å®šè®­ç»ƒå¥½çš„CLoRAæ¨¡å‹è·¯å¾„ï¼ˆPEFTé€‚é…å™¨ç›®å½•ï¼‰"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=16,
        help="è¯„ä¼°æ—¶çš„æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16)"
    )
    
    parser.add_argument(
        "--max-samples",
        type=int,
        default=2000,
        help="æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•° (é»˜è®¤: 2000)"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./",
        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: ./)"
    )
    
    parser.add_argument(
        "--baseline",
        action="store_true",
        help="ä½¿ç”¨åŸå§‹GPT2æ¨¡å‹è¿›è¡ŒåŸºçº¿æµ‹è¯•ï¼ˆä¸ä½¿ç”¨LoRAå¾®è°ƒï¼‰"
    )
    
    return parser.parse_args()


def main():
    """ä¸»è¯„ä¼°å‡½æ•°"""
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()
    
    # è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    log_dir = "./logs"
    logger = setup_logger(
        name="CLoRA_evaluation",
        log_dir=log_dir,
        console_output=True,
        command_args=sys.argv,
        script_name="evaluate_clora.py"
    )
    
    # é‡å®šå‘printè¾“å‡ºåˆ°æ—¥å¿—ï¼ˆä¿æŒæ§åˆ¶å°è¾“å‡ºï¼‰
    original_stdout = redirect_print_to_logger(logger, also_print=False)
    
    try:
        logger.log_section("CLoRA æ¨¡å‹è¯„ä¼°")
        
        # 1. å¤„ç†æ•°æ®é›†
        logger.log_step(1, "åŠ è½½è¯„ä¼°æ•°æ®é›†")
        
        # åŠ è½½åˆ†è¯å™¨
        from data_processor import load_tokenizer, get_in_domain_eval_dataset
        tokenizer = load_tokenizer()
        
        # è·å–åŸŸå†…è¯„ä¼°æ•°æ®é›†ï¼ˆCommonsenseQAéªŒè¯é›†ï¼‰
        in_domain_dataset = get_in_domain_eval_dataset(tokenizer, max_samples=args.max_samples)
        
        # è·å–åŸŸå¤–è¯„ä¼°æ•°æ®é›†
        out_domain_datasets = get_eval_datasets(tokenizer, max_samples_per_dataset=args.max_samples)
        
        # åˆå¹¶æ‰€æœ‰è¯„ä¼°æ•°æ®é›†
        eval_datasets = {"commonsense_qa": in_domain_dataset}
        eval_datasets.update(out_domain_datasets)
        
        dataset_info = {
            "æ•°æ®é›†æ•°é‡": len(eval_datasets)
        }
        for name, dataset in eval_datasets.items():
            dataset_info[f"{name}æ•°æ®é›†"] = f"{len(dataset)} æ ·æœ¬"
        
        logger.log_config(dataset_info, "è¯„ä¼°æ•°æ®é›†ä¿¡æ¯")
        
        # 2. åŠ è½½æ¨¡å‹
        if args.baseline:
            logger.log_step(2, "åŠ è½½åŸºçº¿æ¨¡å‹ï¼ˆåŸå§‹GPT2ï¼‰")
            
            # ä½¿ç”¨åŸå§‹GPT2æ¨¡å‹è¿›è¡ŒåŸºçº¿æµ‹è¯•
            from model_setup import load_base_model
            model = load_base_model("gpt2", num_labels=2)
            logger.info("å·²åŠ è½½åŸå§‹GPT2æ¨¡å‹ä½œä¸ºåŸºçº¿")
            checkpoint_path = "baseline_gpt2"
            
        else:
            logger.log_step(2, "ç¡®å®šCLoRAæ¨¡å‹è·¯å¾„")
            
            checkpoint_path = None
            
            # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹è·¯å¾„
            if args.model_path:
                if os.path.exists(args.model_path):
                    # éªŒè¯è·¯å¾„æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„PEFTæ¨¡å‹æ–‡ä»¶
                    if os.path.isdir(args.model_path):
                        required_files = ['adapter_config.json']
                        model_files = [f for f in os.listdir(args.model_path) 
                                     if f.endswith('.bin') or f.endswith('.safetensors')]
                        
                        if any(f in os.listdir(args.model_path) for f in required_files) and model_files:
                            checkpoint_path = args.model_path
                            logger.info(f"ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹è·¯å¾„: {checkpoint_path}")
                        else:
                            logger.error(f"æŒ‡å®šè·¯å¾„ {args.model_path} ä¸åŒ…å«æœ‰æ•ˆçš„PEFTæ¨¡å‹æ–‡ä»¶")
                            logger.info("éœ€è¦åŒ…å« adapter_config.json å’Œæ¨¡å‹æƒé‡æ–‡ä»¶ (.bin æˆ– .safetensors)")
                            return
                    else:
                        logger.error(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾„ä¸æ˜¯ç›®å½•: {args.model_path}")
                        return
                else:
                    logger.error(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
                    return
        
            # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šè·¯å¾„ï¼Œåˆ™æœç´¢é»˜è®¤ä½ç½®
            if checkpoint_path is None:
                logger.info("æœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œæœç´¢é»˜è®¤ä½ç½®...")
                
                # å¯èƒ½çš„æ¨¡å‹è·¯å¾„
                possible_paths = [
                    "./clora_final_model",
                    "./clora_results", 
                    "./results",
                    "./final_model",
                    "./"
                ]
                
                # æŸ¥æ‰¾æ£€æŸ¥ç‚¹ç›®å½•
                checkpoint_dirs = []
                for path in possible_paths:
                    if os.path.exists(path):
                        if os.path.isdir(path):
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                            if any(f.endswith('.bin') or f.endswith('.safetensors') or f == 'adapter_config.json' 
                                   for f in os.listdir(path)):
                                checkpoint_dirs.append(path)
                            else:
                                # æŸ¥æ‰¾å­ç›®å½•ä¸­çš„æ£€æŸ¥ç‚¹
                                for subdir in os.listdir(path):
                                    subpath = os.path.join(path, subdir)
                                    if os.path.isdir(subpath) and 'checkpoint' in subdir:
                                        checkpoint_dirs.append(subpath)
                
                if not checkpoint_dirs:
                    error_msg = "æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹"
                    logger.error(error_msg)
                    logger.info("è¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€:")
                    logger.info("1. è¿è¡Œ train_clora.py è®­ç»ƒæ¨¡å‹")
                    logger.info("2. ä½¿ç”¨ --model-path å‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„")
                    logger.info("3. ä½¿ç”¨ --baseline å‚æ•°è¿›è¡ŒåŸºçº¿æµ‹è¯•")
                    logger.info("ç¤ºä¾‹: python evaluate_clora.py --model-path ./path/to/your/model")
                    return
                
                # ä½¿ç”¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªæ£€æŸ¥ç‚¹
                checkpoint_path = checkpoint_dirs[0]
                logger.info(f"æ‰¾åˆ°æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
            
            # 3. åŠ è½½è®­ç»ƒå¥½çš„CLoRAæ¨¡å‹
            logger.log_step(3, "åŠ è½½è®­ç»ƒå¥½çš„CLoRAæ¨¡å‹")
            
            try:
                model, tokenizer = load_trained_model(checkpoint_path, logger=logger)
            except Exception as e:
                logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
                logger.info("å°è¯•è®¾ç½®åŸºç¡€æ¨¡å‹å¹¶é‡æ–°åŠ è½½...")
                from model_setup import setup_lora_model
                base_model = setup_lora_model()
                model, tokenizer = load_trained_model(checkpoint_path, base_model.base_model, logger=logger)
        
        # 4. ä½¿ç”¨ç»¼åˆè¯„ä¼°åŠŸèƒ½
        logger.log_step(4, "å¼€å§‹CLoRAç»¼åˆè¯„ä¼°")
        
        # è°ƒç”¨æ–°çš„ç»¼åˆè¯„ä¼°æŠ¥å‘Šç”Ÿæˆå‡½æ•°
        evaluation_report = generate_evaluation_report(model, tokenizer, eval_datasets, batch_size=args.batch_size, logger=logger)
        
        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        logger.log_step(5, "ä¿å­˜è¯„ä¼°ç»“æœ")
        
        
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æŠ¥å‘Šæ–‡ä»¶å
        if args.baseline:
            # report_file = os.path.join(args.model_path, args.output_dir, "baseline_evaluation_report.json")
            report_file = os.path.normpath(os.path.join(args.model_path, args.output_dir, "baseline_evaluation_report.json"))
            report_file = f"./{report_file}"
        else:
            report_file = os.path.normpath(os.path.join(args.model_path, args.output_dir, "clora_evaluation_report.json"))
            report_file = f"./{report_file}"
            # report_file = os.path.join(args.model_path, args.output_dir, "clora_evaluation_report.json")
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(report_file, exist_ok=True)
            # å°†æŠ¥å‘Šä¿å­˜ä¸ºJSONæ–‡ä»¶
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'task_accuracies': evaluation_report['task_accuracies'],
                    'avg_in_domain_accuracy': evaluation_report['avg_in_domain_accuracy'],
                    'avg_out_domain_accuracy': evaluation_report['avg_out_domain_accuracy'],
                    'model_updating_capacity': evaluation_report['model_updating_capacity'],
                    'relative_output_change': evaluation_report['relative_output_change'],
                    'timestamp': datetime.now().isoformat(),
                    'checkpoint_path': checkpoint_path
                }, f, indent=2, ensure_ascii=False)
            logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
        
        # è®°å½•è¯„ä¼°å®Œæˆ
        logger.log_section("è¯„ä¼°å®Œæˆ", "ğŸ‰")
        
        completion_info = {
            "è¯„ä¼°æŠ¥å‘Šæ–‡ä»¶": report_file,
            "æ—¥å¿—æ–‡ä»¶": logger.get_log_file_path(),
            "æ¨¡å‹æ£€æŸ¥ç‚¹": checkpoint_path
        }
        logger.log_config(completion_info, "è¯„ä¼°å®Œæˆæ€»ç»“")
        
        return evaluation_report
        
    except KeyboardInterrupt:
        logger.error("è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
        raise
    except Exception as e:
        logger.log_exception(e, "è¯„ä¼°è¿‡ç¨‹ä¸­")
        raise
    finally:
        # æ¢å¤åŸå§‹stdoutå¹¶å…³é—­æ—¥å¿—å™¨
        restore_print(original_stdout)
        logger.close()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nè¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise